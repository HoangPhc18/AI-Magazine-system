#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ƒê√¢y l√† module ch√≠nh cho scraper b√†i vi·∫øt tin t·ª©c
Quy tr√¨nh: 
1. T√¨m ki·∫øm URL b√†i vi·∫øt t·ª´ google_news.py 
2. Tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL b·∫±ng scrape_articles_selenium.py
3. L∆∞u k·∫øt qu·∫£ v√†o file JSON trong th∆∞ m·ª•c output
"""

import os
import sys
import json
import uuid
import time
import shutil
import logging
import argparse
import requests
from pathlib import Path
from datetime import datetime, timedelta
from urllib.parse import urlparse
import glob
import traceback
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import random
from dotenv import load_dotenv
import re
import mysql.connector
from mysql.connector import Error
from unidecode import unidecode

# Import c·∫•u h√¨nh t·ª´ module config
from config import get_config, reload_config

# Import c√°c module n·ªôi b·ªô
from google_news import (
    fetch_categories_from_backend,
    fetch_subcategories_by_category,
    search_with_category,
    process_all_categories,
    save_article_to_json,
    import_article_to_backend,
    get_category_by_id,
    get_subcategory_by_id,
    search_google_news
)
from scrape_articles_selenium import extract_article_content

# T·∫£i c·∫•u h√¨nh t·ª´ file .env th√¥ng qua module config
config = get_config()

# Th√¥ng tin k·∫øt n·ªëi database t·ª´ c·∫•u h√¨nh
DB_HOST = config['DB_HOST']
DB_USER = config['DB_USER']
DB_PASSWORD = config['DB_PASSWORD']
DB_NAME = config['DB_NAME']
DB_PORT = config['DB_PORT']

# Config k·∫øt n·ªëi database
DB_CONFIG = {
    "host": DB_HOST,
    "user": DB_USER,
    "password": DB_PASSWORD,
    "database": DB_NAME,
    "port": DB_PORT
}

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(f"scraper_{datetime.now().strftime('%Y%m%d')}.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# üîπ C√°c th√¥ng s·ªë m·∫∑c ƒë·ªãnh t·ª´ c·∫•u h√¨nh
SCRAPER_DIR = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.path.join(SCRAPER_DIR, "temp")
OUTPUT_DIR = os.path.join(SCRAPER_DIR, "output")
DEFAULT_BATCH_SIZE = config.get("DEFAULT_BATCH_SIZE", 5)
# S·ªë ng√†y ƒë·ªÉ gi·ªØ l·∫°i log files v√† output files
RETENTION_DAYS = config.get("RETENTION_DAYS", 7)
# S·ªë b√†i vi·∫øt t·ªëi ƒëa cho m·ªói danh m·ª•c
MAX_ARTICLES_PER_CATEGORY = config.get("MAX_ARTICLES_PER_CATEGORY", 3)
MAX_ARTICLES_PER_SUBCATEGORY = config.get("MAX_ARTICLES_PER_SUBCATEGORY", 2)

# üîπ Laravel Backend API URLs t·ª´ c·∫•u h√¨nh
BACKEND_URL = config['BACKEND_URL']
BACKEND_PORT = config['BACKEND_PORT']
BASE_API_URL = config['BASE_API_URL']
CATEGORIES_API_URL = config['CATEGORIES_API_URL']
ARTICLES_API_URL = config['ARTICLES_API_URL']
ARTICLES_BATCH_API_URL = config['ARTICLES_BATCH_API_URL']
ARTICLES_IMPORT_API_URL = config['ARTICLES_IMPORT_API_URL']
ARTICLES_CHECK_API_URL = config['ARTICLES_CHECK_API_URL']

# C√°c bi·∫øn to√†n c·ª•c
# Cache l∆∞u c√°c URL ƒë√£ x·ª≠ l√Ω ƒë·ªÉ tr√°nh tr√πng l·∫∑p
processed_urls = set()

def check_api_keys():
    """
    Ki·ªÉm tra c√°c API keys t·ª´ file .env
    """
    config = get_config()  # T·∫£i l·∫°i c·∫•u h√¨nh ƒë·ªÉ c√≥ th√¥ng tin m·ªõi nh·∫•t
    api_keys_status = {}
    
    # Ki·ªÉm tra WorldNewsAPI
    worldnews_api_key = config.get('WORLDNEWS_API_KEY', '')
    if worldnews_api_key:
        api_keys_status['WorldNewsAPI'] = True
    else:
        api_keys_status['WorldNewsAPI'] = False
        
    # Ki·ªÉm tra Currents API
    currents_api_key = config.get('CURRENTS_API_KEY', '')
    if currents_api_key:
        api_keys_status['CurrentsAPI'] = True
    else:
        api_keys_status['CurrentsAPI'] = False
    
    # In th√¥ng tin
    logger.info("=== Tr·∫°ng th√°i API keys ===")
    for api_name, status in api_keys_status.items():
        if status:
            logger.info(f"‚úÖ {api_name}: OK")
        else:
            logger.warning(f"‚ö†Ô∏è {api_name}: Kh√¥ng t√¨m th·∫•y API key")
    
    return api_keys_status

def save_articles_to_file(articles, output_file=None):
    """
    L∆∞u danh s√°ch b√†i vi·∫øt v√†o file JSON
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt
        output_file (str): ƒê∆∞·ªùng d·∫´n file ƒë·∫ßu ra
        
    Returns:
        str: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ƒë√£ l∆∞u
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ l∆∞u!")
        return None
    
    # T·∫°o th∆∞ m·ª•c ƒë·∫ßu ra n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # T·∫°o t√™n file m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
    if not output_file:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(OUTPUT_DIR, f"enriched_articles_{timestamp}.json")
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u
    normalized_articles = []
    for article in articles:
        normalized = article.copy()
        
        # ƒê·∫£m b·∫£o source_name l√† chu·ªói
        if isinstance(normalized.get("source_name"), dict):
            normalized["source_name"] = normalized["source_name"].get("name", "Unknown Source")
        
        # ƒê·∫£m b·∫£o meta_data l√† chu·ªói JSON
        if isinstance(normalized.get("meta_data"), dict):
            normalized["meta_data"] = json.dumps(normalized["meta_data"])
        
        # ƒê·∫£m b·∫£o summary kh√¥ng bao gi·ªù l√† None
        if normalized.get("summary") is None:
            normalized["summary"] = ""
        
        # ƒê·∫£m b·∫£o content kh√¥ng bao gi·ªù l√† None
        if normalized.get("content") is None:
            normalized["content"] = ""
        
        # ƒê·∫£m b·∫£o published_at c√≥ ƒë·ªãnh d·∫°ng chu·∫©n
        if normalized.get("published_at"):
            try:
                # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y th√°ng
                date_obj = datetime.fromisoformat(normalized["published_at"].replace('Z', '+00:00'))
                normalized["published_at"] = date_obj.isoformat()
            except (ValueError, TypeError):
                # N·∫øu kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        
        normalized_articles.append(normalized)
    
    # L∆∞u b√†i vi·∫øt v√†o file JSON
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(normalized_articles, f, ensure_ascii=False, indent=4)
    
    logger.info(f"[OK] ƒê√£ l∆∞u {len(normalized_articles)} b√†i vi·∫øt v√†o {output_file}")
    return output_file

def send_to_backend(articles, batch_size=DEFAULT_BATCH_SIZE, auto_send=True):
    """
    G·ª≠i b√†i vi·∫øt t·ªõi backend API
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt
        batch_size (int): S·ªë l∆∞·ª£ng b√†i vi·∫øt g·ª≠i trong m·ªói request
        auto_send (bool): T·ª± ƒë·ªông g·ª≠i kh√¥ng c·∫ßn x√°c nh·∫≠n (lu√¥n True)
        
    Returns:
        bool: Tr·∫°ng th√°i th√†nh c√¥ng
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ g·ª≠i!")
        return False
    
    # Ghi log t·ªïng s·ªë b√†i vi·∫øt c·∫ßn g·ª≠i
    logger.info(f"Chu·∫©n b·ªã g·ª≠i {len(articles)} b√†i vi·∫øt t·ªõi backend")
    
    # Debug: Ki·ªÉm tra API URL
    api_url = ARTICLES_IMPORT_API_URL
    logger.info(f"API URL: {api_url}")
    
    # Debug ti√™u ƒë·ªÅ v√† URL c·ªßa b√†i vi·∫øt ƒë·∫ßu ti√™n
    if len(articles) > 0:
        logger.info(f"B√†i vi·∫øt ƒë·∫ßu ti√™n: {articles[0].get('title', 'N/A')}")
        logger.info(f"Source URL: {articles[0].get('source_url', 'N/A')}")
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi g·ª≠i
    normalized_articles = []
    for article in articles:
        normalized = article.copy()
        
        # N·∫øu ch·ªâ c√≥ JSON filepath m√† kh√¥ng c√≥ n·ªôi dung, ƒë·ªçc n·ªôi dung t·ª´ file
        if "content" not in normalized and "json_filepath" in normalized:
            try:
                json_file_content = read_json_file(normalized["json_filepath"])
                if json_file_content and isinstance(json_file_content, dict):
                    # C·∫≠p nh·∫≠t c√°c tr∆∞·ªùng t·ª´ file JSON
                    for key, value in json_file_content.items():
                        if key not in normalized or normalized[key] is None:
                            normalized[key] = value
                    logger.info(f"ƒê√£ ƒë·ªçc n·ªôi dung t·ª´ file: {normalized['json_filepath']}")
            except Exception as e:
                logger.error(f"L·ªói khi ƒë·ªçc file JSON {normalized.get('json_filepath')}: {str(e)}")
        
        # X√°c ƒë·ªãnh URL ngu·ªìn
        source_url = normalized.get("source_url", normalized.get("url", ""))
        
        # ƒê·∫£m b·∫£o c√°c tr∆∞·ªùng b·∫Øt bu·ªôc t·ªìn t·∫°i
        if "title" not in normalized or not normalized["title"]:
            normalized["title"] = "Kh√¥ng c√≥ ti√™u ƒë·ªÅ"
            
        if "content" not in normalized or not normalized["content"]:
            normalized["content"] = "Kh√¥ng c√≥ n·ªôi dung"
            
        if "source_name" not in normalized or not normalized["source_name"]:
            # C·ªë g·∫Øng tr√≠ch xu·∫•t t√™n ngu·ªìn t·ª´ URL
            try:
                parsed_url = urlparse(source_url)
                normalized["source_name"] = parsed_url.netloc
            except:
                normalized["source_name"] = "Kh√¥ng r√µ ngu·ªìn"
        
        # ƒê·∫£m b·∫£o source_name l√† chu·ªói
        if isinstance(normalized.get("source_name"), dict):
            normalized["source_name"] = normalized["source_name"].get("name", "Unknown Source")
        
        # ƒê·∫£m b·∫£o c√≥ ƒë·ªß tr∆∞·ªùng source_url
        if "source_url" not in normalized or not normalized["source_url"]:
            normalized["source_url"] = source_url
            
        # ƒê·∫£m b·∫£o c√≥ ƒë·ªß tr∆∞·ªùng url n·∫øu ch·ªâ c√≥ source_url
        if "url" not in normalized or not normalized["url"]:
            normalized["url"] = normalized.get("source_url", "")
        
        # T·∫°o slug n·∫øu ch∆∞a c√≥
        if "slug" not in normalized or not normalized["slug"]:
            normalized["slug"] = generate_slug(normalized["title"], add_uuid=True)
            
        # ƒê·∫£m b·∫£o c√≥ summary
        if "summary" not in normalized or not normalized["summary"]:
            content = normalized.get("content", "")
            if content:
                sentences = re.split(r'[.!?]+', content)
                if len(sentences) >= 2:
                    normalized["summary"] = '. '.join(s.strip() for s in sentences[:2] if s.strip()) + '.'
                else:
                    normalized["summary"] = sentences[0].strip() if sentences else ""
            else:
                normalized["summary"] = "Kh√¥ng c√≥ t√≥m t·∫Øt"
                
        # ƒê·∫£m b·∫£o summary kh√¥ng qu√° d√†i
        if len(normalized.get("summary", "")) > 300:
            normalized["summary"] = normalized["summary"][:297] + "..."
        
        # ƒê·∫£m b·∫£o published_at c√≥ ƒë·ªãnh d·∫°ng chu·∫©n
        if normalized.get("published_at"):
            try:
                # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y th√°ng
                date_obj = datetime.fromisoformat(normalized["published_at"].replace('Z', '+00:00'))
                normalized["published_at"] = date_obj.isoformat()
            except (ValueError, TypeError):
                # N·∫øu kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        else:
            # N·∫øu kh√¥ng c√≥ published_at, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
            normalized["published_at"] = datetime.now().isoformat()

        # ƒê·∫£m b·∫£o c√≥ category v√† category_id
        if "category" not in normalized:
            if "category_id" in normalized:
                normalized["category"] = normalized["category_id"]
            else:
                normalized["category"] = 1  # Default category ID
        
        if "category_id" not in normalized:
            if "category" in normalized:
                normalized["category_id"] = normalized["category"]
            else:
                normalized["category_id"] = 1  # Default category ID
                
        # ƒê·∫£m b·∫£o category_id v√† category l√† s·ªë nguy√™n
        try:
            normalized["category_id"] = int(normalized["category_id"])
            normalized["category"] = int(normalized["category"])
        except (ValueError, TypeError):
            logger.warning(f"Invalid category_id/category for article {normalized.get('title')}, using default")
            normalized["category_id"] = 1
            normalized["category"] = 1
        
        # ƒê·∫£m b·∫£o c√°c flag c·∫ßn thi·∫øt ƒë∆∞·ª£c ƒë·∫∑t
        normalized["is_published"] = 1
        normalized["is_imported"] = 1
                
        normalized_articles.append(normalized)
    
    # Chia th√†nh c√°c batch ƒë·ªÉ g·ª≠i
    batches = [normalized_articles[i:i + batch_size] for i in range(0, len(normalized_articles), batch_size)]
    logger.info(f"G·ª≠i {len(normalized_articles)} b√†i vi·∫øt t·ªõi backend trong {len(batches)} batches...")
    
    # In chi ti·∫øt b√†i vi·∫øt ƒë·∫ßu ti√™n ƒë·ªÉ debug
    if normalized_articles:
        first = normalized_articles[0]
        logger.info(f"Chi ti·∫øt b√†i ƒë·∫ßu: Title='{first.get('title')}', Content length={len(first.get('content', ''))}, Source='{first.get('source_name')}'")
    
    total_success = True
    total_imported = 0
    total_skipped = 0
    
    for i, batch in enumerate(batches, 1):
        logger.info(f"G·ª≠i batch {i}/{len(batches)} ({len(batch)} b√†i vi·∫øt)")
        
        # Chu·∫©n b·ªã payload theo ƒë√∫ng ƒë·ªãnh d·∫°ng API y√™u c·∫ßu
        payload = {
            "articles": batch
        }
        
        # Thi·∫øt l·∫≠p header
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        
        # G·ª≠i request, v·ªõi c∆° ch·∫ø th·ª≠ l·∫°i n·∫øu l·ªói
        max_retries = 3
        retry_count = 0
        retry_delay = 2
        
        while retry_count < max_retries:
            try:
                # S·ª≠ d·ª•ng endpoint import t·ª´ c·∫•u h√¨nh
                api_url = ARTICLES_IMPORT_API_URL
                logger.info(f"G·ª≠i d·ªØ li·ªáu ƒë·∫øn API: {api_url}")
                
                response = requests.post(
                    api_url,
                    json=payload,
                    headers=headers,
                    timeout=60  # TƒÉng timeout ƒë·ªÉ x·ª≠ l√Ω b√†i vi·∫øt l·ªõn
                )
                
                # In ra response status v√† ph·∫ßn ƒë·∫ßu c·ªßa response body ƒë·ªÉ debug
                logger.info(f"API Response Status: {response.status_code}")
                response_text = response.text[:200] + "..." if len(response.text) > 200 else response.text
                logger.info(f"API Response Text: {response_text}")
                
                if response.status_code == 200 or response.status_code == 201:
                    # Ph√¢n t√≠ch ph·∫£n h·ªìi ƒë·ªÉ t√≠nh s·ªë b√†i vi·∫øt ƒë√£ import
                    try:
                        result = response.json()
                        status = result.get('status')
                        
                        if status == 'success' or status == 'warning':
                            # C·∫£ success v√† warning ƒë·ªÅu ƒë∆∞·ª£c xem l√† th√†nh c√¥ng
                            success_count = result.get('success', 0)
                            skipped_count = result.get('skipped', 0)
                            
                            total_imported += int(success_count) if isinstance(success_count, (int, str)) else 0
                            total_skipped += int(skipped_count) if isinstance(skipped_count, (int, str)) else 0
                            
                            logger.info(f"[OK] Batch {i}: {success_count} b√†i vi·∫øt ƒë√£ import, {skipped_count} b√†i vi·∫øt b·ªã b·ªè qua")
                            
                            # Log chi ti·∫øt l·ªói n·∫øu c√≥
                            if status == 'warning' and 'errors' in result:
                                for error in result.get('errors', []):
                                    logger.warning(f"Warning: {error}")
                        else:
                            logger.error(f"[ERROR] Batch {i}: Kh√¥ng mong ƒë·ª£i status '{status}' t·ª´ API")
                            total_success = False
                    except Exception as e:
                        logger.warning(f"Kh√¥ng th·ªÉ ph√¢n t√≠ch JSON response: {str(e)}")
                    break
                else:
                    error_msg = response.text[:100] + "..." if len(response.text) > 100 else response.text
                    logger.error(f"[ERROR] Batch {i}: L·ªói {response.status_code} - {error_msg}")
                    
                    # Th·ª≠ l·∫°i sau th·ªùi gian delay
                    retry_count += 1
                    if retry_count < max_retries:
                        logger.info(f"Th·ª≠ l·∫°i sau {retry_delay} gi√¢y...")
                        time.sleep(retry_delay)
                        # TƒÉng th·ªùi gian delay cho l·∫ßn th·ª≠ ti·∫øp theo
                        retry_delay *= 2
                    else:
                        logger.error(f"[ERROR] ƒê√£ th·ª≠ l·∫°i {max_retries} l·∫ßn kh√¥ng th√†nh c√¥ng cho batch {i}")
                        total_success = False
                        
            except Exception as e:
                logger.error(f"[ERROR] Batch {i}: Exception - {str(e)}")
                # Log stack trace
                import traceback
                logger.error(f"Stack trace: {traceback.format_exc()}")
                
                retry_count += 1
                if retry_count < max_retries:
                    logger.info(f"Th·ª≠ l·∫°i sau {retry_delay} gi√¢y...")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    logger.error(f"[ERROR] ƒê√£ th·ª≠ l·∫°i {max_retries} l·∫ßn kh√¥ng th√†nh c√¥ng cho batch {i}")
                    total_success = False
    
    if total_success:
        logger.info(f"[OK] T·ªïng c·ªông: {total_imported} b√†i vi·∫øt ƒë√£ ƒë∆∞·ª£c import th√†nh c√¥ng, {total_skipped} b√†i vi·∫øt b·ªã b·ªè qua")
    else:
        logger.warning(f"[WARN] C√≥ l·ªói x·∫£y ra khi g·ª≠i b√†i vi·∫øt t·ªõi backend. ƒê√£ import: {total_imported}, ƒê√£ b·ªè qua: {total_skipped}")
        
    return total_success

def cleanup_temp_files():
    """
    D·ªçn d·∫πp c√°c file t·∫°m th·ªùi
    """
    if os.path.exists(TEMP_DIR):
        try:
            shutil.rmtree(TEMP_DIR)
            logger.info(f"[OK] ƒê√£ d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {TEMP_DIR}")
        except Exception as e:
            logger.error(f"[ERROR] L·ªói khi d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {str(e)}")

def cleanup_old_files(retention_days=RETENTION_DAYS):
    """
    D·ªçn d·∫πp c√°c file c≈© trong th∆∞ m·ª•c output
    
    Args:
        retention_days (int): S·ªë ng√†y gi·ªØ l·∫°i files (m·∫∑c ƒë·ªãnh: 2)
    """
    if not os.path.exists(OUTPUT_DIR):
        return
    
    cutoff_date = time.time() - (retention_days * 24 * 60 * 60)
    logger.info(f"D·ªçn d·∫πp c√°c file c≈© h∆°n {retention_days} ng√†y trong th∆∞ m·ª•c {OUTPUT_DIR}")
    
    count = 0
    for filename in os.listdir(OUTPUT_DIR):
        filepath = os.path.join(OUTPUT_DIR, filename)
        if os.path.isfile(filepath):
            file_time = os.path.getmtime(filepath)
            if file_time < cutoff_date:
                try:
                    os.remove(filepath)
                    count += 1
                    logger.debug(f"ƒê√£ x√≥a file c≈©: {filename}")
                except Exception as e:
                    logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {filename}: {str(e)}")
    
    if count > 0:
        logger.info(f"ƒê√£ x√≥a {count} file c≈©")
    else:
        logger.info("Kh√¥ng c√≥ file n√†o c·∫ßn x√≥a")

def process_category(category_id, category_name, max_articles=2):
    """
    X·ª≠ l√Ω m·ªôt danh m·ª•c c·ª• th·ªÉ v√† tr√≠ch xu·∫•t b√†i vi·∫øt
    
    Args:
        category_id: ID c·ªßa danh m·ª•c
        category_name: T√™n c·ªßa danh m·ª•c
        max_articles: S·ªë b√†i vi·∫øt t·ªëi ƒëa c·∫ßn l·∫•y
        
    Returns:
        dict: Danh s√°ch c√°c b√†i vi·∫øt ƒë√£ t√¨m th·∫•y cho danh m·ª•c
    """
    articles = []
    
    for i in range(1, max_articles + 1):
        logger.info(f"T√¨m ki·∫øm b√†i vi·∫øt th·ª© {i}/{max_articles} cho danh m·ª•c: {category_name}")
        
        try:
            article_data = search_with_category(category_id)
            
            if article_data and "url" in article_data:
                article_url = article_data["url"]
                
                # Ki·ªÉm tra xem URL n√†y ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ch∆∞a
                if article_url in processed_urls:
                    logger.warning(f"URL ƒë√£ x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥, b·ªè qua: {article_url}")
                    continue
                
                # ƒê√°nh d·∫•u URL n√†y ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
                processed_urls.add(article_url)
                
                articles.append(article_data)
                
                logger.info(f"ƒê√£ t√¨m th·∫•y b√†i vi·∫øt: {article_data.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')}")
                logger.info(f"URL: {article_url}")
                logger.info(f"ƒê√£ l∆∞u v√†o: {article_data.get('file_path', 'Kh√¥ng l∆∞u file')}")
            else:
                logger.warning(f"Kh√¥ng t√¨m th·∫•y th√™m b√†i vi·∫øt n√†o cho danh m·ª•c: {category_name}")
                break
            
            # Th√™m kho·∫£ng th·ªùi gian ngh·ªâ gi·ªØa c√°c l·∫ßn t√¨m ki·∫øm ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
            if i < max_articles:
                time.sleep(2)
                
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω b√†i vi·∫øt th·ª© {i} cho danh m·ª•c {category_name}: {str(e)}")
            logger.error(traceback.format_exc())
    
    return articles

def find_and_process_all_categories(max_articles_per_category=2, use_subcategories=False):
    """
    T√¨m ki·∫øm v√† x·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c t·ª´ backend.
    
    Args:
        max_articles_per_category (int): S·ªë b√†i vi·∫øt t·ªëi ƒëa cho m·ªói danh m·ª•c
        use_subcategories (bool): S·ª≠ d·ª•ng danh m·ª•c con khi x·ª≠ l√Ω danh m·ª•c
        
    Returns:
        dict: K·∫øt qu·∫£ x·ª≠ l√Ω b√†i vi·∫øt theo danh m·ª•c
    """
    logger.info(f"B·∫Øt ƒë·∫ßu t√¨m ki·∫øm v√† x·ª≠ l√Ω b√†i vi·∫øt cho c√°c danh m·ª•c (t·ªëi ƒëa {max_articles_per_category} b√†i/danh m·ª•c)")
    logger.info(f"Ch·∫ø ƒë·ªô s·ª≠ d·ª•ng danh m·ª•c con: {use_subcategories}")
    
    # L·∫•y danh s√°ch danh m·ª•c t·ª´ backend
    categories = fetch_categories_from_backend()
    
    if not categories:
        logger.error("Kh√¥ng th·ªÉ l·∫•y danh s√°ch danh m·ª•c t·ª´ backend")
        return {"success": [], "failed": [], "articles_by_category": {}, "all_articles": [], "total_articles": 0}
    
    logger.info(f"ƒê√£ t√¨m th·∫•y {len(categories)} danh m·ª•c ƒë·ªÉ x·ª≠ l√Ω")
    
    success = []
    failed = []
    articles_by_category = {}
    all_articles = []
    
    # X·ª≠ l√Ω t·ª´ng danh m·ª•c
    for category in categories:
        category_id = category.get('id')
        category_name = category.get('name')
        
        logger.info(f"X·ª≠ l√Ω danh m·ª•c: ID={category_id}, T√™n={category_name}")
        
        # Ki·ªÉm tra n·∫øu s·ª≠ d·ª•ng danh m·ª•c con
        if use_subcategories:
            logger.info(f"ƒêang t√¨m ki·∫øm danh m·ª•c con cho danh m·ª•c: {category_name}")
            subcategories = fetch_subcategories_by_category(category_id)
            
            if subcategories and len(subcategories) > 0:
                logger.info(f"T√¨m th·∫•y {len(subcategories)} danh m·ª•c con cho danh m·ª•c {category_name}")
                
                # X·ª≠ l√Ω t·ª´ng danh m·ª•c con
                for subcategory in subcategories:
                    subcategory_id = subcategory.get('id')
                    subcategory_name = subcategory.get('name')
                    
                    logger.info(f"X·ª≠ l√Ω danh m·ª•c con: ID={subcategory_id}, T√™n={subcategory_name}")
                    
                    # T√¨m ki·∫øm b√†i vi·∫øt cho danh m·ª•c con n√†y
                    articles_found = []
                    for i in range(MAX_ARTICLES_PER_SUBCATEGORY):
                        if len(articles_found) >= MAX_ARTICLES_PER_SUBCATEGORY:
                            break
                            
                        logger.info(f"T√¨m ki·∫øm b√†i vi·∫øt th·ª© {i+1}/{MAX_ARTICLES_PER_SUBCATEGORY} cho danh m·ª•c con: {subcategory_name}")
                        article = search_with_category(category_id, subcategory_id)
                        
                        if article and article.get('title'):
                            logger.info(f"ƒê√£ t√¨m th·∫•y b√†i vi·∫øt: {article['title']}")
                            logger.info(f"URL: {article['url']}")
                            logger.info(f"ƒê√£ l∆∞u v√†o: {article.get('json_filepath', 'Kh√¥ng l∆∞u file')}")
                            
                            # ƒê·∫£m b·∫£o article c√≥ th√¥ng tin subcategory_id
                            if 'subcategory_id' not in article:
                                article['subcategory_id'] = subcategory_id
                                
                            articles_found.append(article)
                            all_articles.append(article)
                        else:
                            logger.warning(f"Kh√¥ng t√¨m th·∫•y th√™m b√†i vi·∫øt n√†o cho danh m·ª•c con: {subcategory_name}")
                            break
                            
                        # Th√™m kho·∫£ng th·ªùi gian ngh·ªâ gi·ªØa c√°c l·∫ßn t√¨m ki·∫øm ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
                        time.sleep(2)
                    
                    # C·∫≠p nh·∫≠t k·∫øt qu·∫£ cho danh m·ª•c n√†y
                    if articles_found:
                        if category_id not in articles_by_category:
                            articles_by_category[category_id] = []
                        articles_by_category[category_id].extend(articles_found)
                        
                        # C·∫≠p nh·∫≠t tr·∫°ng th√°i th√†nh c√¥ng n·∫øu ch∆∞a c√≥ trong danh s√°ch
                        if category_id not in success:
                            success.append(category_id)
            else:
                logger.info(f"Kh√¥ng t√¨m th·∫•y danh m·ª•c con n√†o cho danh m·ª•c {category_name}")
                if category_id not in failed:
                    failed.append(category_id)
        else:
            # Kh√¥ng s·ª≠ d·ª•ng danh m·ª•c con, x·ª≠ l√Ω tr·ª±c ti·∫øp danh m·ª•c ch√≠nh
            logger.info(f"T√¨m ki·∫øm b√†i vi·∫øt cho danh m·ª•c ch√≠nh: {category_name}")
            articles_found = []
            
            for i in range(max_articles_per_category):
                logger.info(f"T√¨m ki·∫øm b√†i vi·∫øt th·ª© {i+1}/{max_articles_per_category} cho danh m·ª•c: {category_name}")
                article = search_with_category(category_id)
                
                if article and article.get('title'):
                    logger.info(f"ƒê√£ t√¨m th·∫•y b√†i vi·∫øt: {article['title']}")
                    logger.info(f"URL: {article['url']}")
                    logger.info(f"ƒê√£ l∆∞u v√†o: {article.get('json_filepath', 'Kh√¥ng l∆∞u file')}")
                    articles_found.append(article)
                    all_articles.append(article)
                else:
                    logger.warning(f"Kh√¥ng t√¨m th·∫•y th√™m b√†i vi·∫øt n√†o cho danh m·ª•c: {category_name}")
                    break
                    
                # Th√™m kho·∫£ng th·ªùi gian ngh·ªâ gi·ªØa c√°c l·∫ßn t√¨m ki·∫øm
                time.sleep(2)
                
            # C·∫≠p nh·∫≠t k·∫øt qu·∫£
            if articles_found:
                articles_by_category[category_id] = articles_found
                success.append(category_id)
            else:
                failed.append(category_id)
    
    logger.info("")
    logger.info(f"Th√†nh c√¥ng: {len(success)} danh m·ª•c, Th·∫•t b·∫°i: {len(failed)} danh m·ª•c")
    logger.info(f"T·ªïng s·ªë b√†i vi·∫øt ƒë√£ t√¨m ƒë∆∞·ª£c: {len(all_articles)}")
    
    return {
        "success": success,
        "failed": failed,
        "articles_by_category": articles_by_category,
        "all_articles": all_articles,
        "total_articles": len(all_articles)
    }

def import_all_to_backend(directory=None):
    """
    Import t·∫•t c·∫£ c√°c file JSON t·ª´ th∆∞ m·ª•c output v√†o backend
    
    Args:
        directory (str): Th∆∞ m·ª•c ch·ª©a c√°c file JSON (m·∫∑c ƒë·ªãnh: OUTPUT_DIR)
        
    Returns:
        tuple: (success_count, failed_count) - S·ªë l∆∞·ª£ng file th√†nh c√¥ng v√† th·∫•t b·∫°i
    """
    if directory is None:
        directory = OUTPUT_DIR
    
    if not os.path.exists(directory):
        logger.error(f"Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {directory}")
        return 0, 0
    
    logger.info(f"B·∫Øt ƒë·∫ßu import t·∫•t c·∫£ c√°c file JSON t·ª´ th∆∞ m·ª•c: {directory}")
    
    # T√¨m t·∫•t c·∫£ c√°c file JSON trong th∆∞ m·ª•c
    json_files = [f for f in os.listdir(directory) if f.endswith('.json')]
    
    if not json_files:
        logger.warning(f"Kh√¥ng t√¨m th·∫•y file JSON n√†o trong th∆∞ m·ª•c: {directory}")
        return 0, 0
    
    logger.info(f"T√¨m th·∫•y {len(json_files)} file JSON")
    
    success_count = 0
    failed_count = 0
    
    for json_file in json_files:
        filepath = os.path.join(directory, json_file)
        logger.info(f"ƒêang x·ª≠ l√Ω file: {json_file}")
        
        try:
            # ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Ki·ªÉm tra xem d·ªØ li·ªáu c√≥ ph·∫£i l√† dictionary v·ªõi c√°c tr∆∞·ªùng c·∫ßn thi·∫øt kh√¥ng
            if isinstance(data, dict) and 'category_id' in data and 'title' in data and 'content' in data:
                # Import b√†i vi·∫øt v√†o backend
                result = import_article_to_backend(
                    category_id=data['category_id'],
                    article_url=data.get('url', ''),
                    title=data['title'],
                    content=data['content']
                )
                
                if result:
                    logger.info(f"Import th√†nh c√¥ng file: {json_file}")
                    success_count += 1
                    # X√≥a file JSON sau khi import th√†nh c√¥ng
                    try:
                        os.remove(filepath)
                        logger.info(f"ƒê√£ x√≥a file JSON sau khi import th√†nh c√¥ng: {json_file}")
                    except Exception as e:
                        logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {json_file}: {str(e)}")
                else:
                    logger.error(f"Import th·∫•t b·∫°i file: {json_file}")
                    failed_count += 1
            else:
                logger.warning(f"File kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng: {json_file}")
                failed_count += 1
        
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω file {json_file}: {str(e)}")
            failed_count += 1
    
    logger.info(f"K·∫øt qu·∫£ import: Th√†nh c√¥ng: {success_count}, Th·∫•t b·∫°i: {failed_count}")
    return success_count, failed_count

def save_all_articles_to_single_file(results):
    """
    L∆∞u t·∫•t c·∫£ b√†i vi·∫øt t·ª´ t·∫•t c·∫£ c√°c danh m·ª•c v√†o m·ªôt file JSON duy nh·∫•t
    :param results: K·∫øt qu·∫£ t·ª´ h√†m find_and_process_all_categories
    :return: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON ƒë√£ l∆∞u
    """
    logger.info(f"B·∫Øt ƒë·∫ßu l∆∞u t·∫•t c·∫£ b√†i vi·∫øt v√†o file JSON duy nh·∫•t, d·ªØ li·ªáu ƒë·∫ßu v√†o: {type(results)}")
    
    # Ki·ªÉm tra d·ªØ li·ªáu ƒë·∫ßu v√†o
    if not isinstance(results, dict):
        logger.error(f"D·ªØ li·ªáu ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá: {type(results)}")
        return None
    
    # Log c·∫•u tr√∫c c·ªßa results ƒë·ªÉ debug
    logger.info(f"C·∫•u tr√∫c results: {list(results.keys())}")
    
    # ∆Øu ti√™n s·ª≠ d·ª•ng all_articles n·∫øu c√≥
    all_articles = []
    
    if "all_articles" in results and isinstance(results["all_articles"], list) and results["all_articles"]:
        all_articles_raw = results["all_articles"]
        logger.info(f"S·ª≠ d·ª•ng {len(all_articles_raw)} b√†i vi·∫øt t·ª´ all_articles")
        
        # Thu th·∫≠p b√†i vi·∫øt v·ªõi n·ªôi dung ƒë·∫ßy ƒë·ªß
        for article in all_articles_raw:
            if not isinstance(article, dict):
                logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng h·ª£p l·ªá: {type(article)}")
                continue
                
            # ƒê·ªçc n·ªôi dung t·ª´ file JSON n·∫øu b√†i vi·∫øt kh√¥ng c√≥ tr∆∞·ªùng 'content'
            article_copy = article.copy()
            if "content" not in article_copy and "json_filepath" in article_copy:
                try:
                    json_content = read_json_file(article_copy["json_filepath"])
                    if json_content and "content" in json_content:
                        # C·∫≠p nh·∫≠t n·ªôi dung t·ª´ file JSON
                        article_copy.update(json_content)
                        logger.info(f"ƒê√£ l·∫•y n·ªôi dung t·ª´ file: {article_copy['json_filepath']}")
                except Exception as e:
                    logger.error(f"L·ªói khi ƒë·ªçc file {article_copy['json_filepath']}: {str(e)}")
            
            # Th√™m v√†o danh s√°ch n·∫øu c√≥ n·ªôi dung
            if article_copy.get("content"):
                all_articles.append(article_copy)
            else:
                logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng c√≥ n·ªôi dung: {article_copy.get('title', 'Kh√¥ng ti√™u ƒë·ªÅ')}")
    else:
        # Thu th·∫≠p b√†i vi·∫øt t·ª´ c·∫•u tr√∫c articles_by_category
        if "articles_by_category" in results and isinstance(results["articles_by_category"], dict):
            articles_by_category = results["articles_by_category"]
            
            # Thu th·∫≠p b√†i vi·∫øt t·ª´ m·ªói danh m·ª•c
            for category_name, articles in articles_by_category.items():
                if not isinstance(articles, list):
                    logger.warning(f"B·ªè qua danh m·ª•c {category_name} v√¨ d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá: {type(articles)}")
                    continue
                
                logger.info(f"X·ª≠ l√Ω {len(articles)} b√†i vi·∫øt t·ª´ danh m·ª•c {category_name}")
                
                for article in articles:
                    if not isinstance(article, dict):
                        logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng h·ª£p l·ªá: {type(article)}")
                        continue
                    
                    # T·∫°o b·∫£n sao v√† th√™m th√¥ng tin danh m·ª•c
                    article_copy = article.copy()
                    article_copy["category_name"] = category_name
                    
                    # ƒê·ªçc n·ªôi dung t·ª´ file JSON n·∫øu b√†i vi·∫øt kh√¥ng c√≥ tr∆∞·ªùng 'content'
                    if "content" not in article_copy and "json_filepath" in article_copy:
                        try:
                            json_content = read_json_file(article_copy["json_filepath"])
                            if json_content and "content" in json_content:
                                # C·∫≠p nh·∫≠t n·ªôi dung t·ª´ file JSON
                                article_copy.update(json_content)
                                logger.info(f"ƒê√£ l·∫•y n·ªôi dung t·ª´ file: {article_copy['json_filepath']}")
                        except Exception as e:
                            logger.error(f"L·ªói khi ƒë·ªçc file {article_copy['json_filepath']}: {str(e)}")
                    
                    # Th√™m v√†o danh s√°ch n·∫øu c√≥ n·ªôi dung
                    if article_copy.get("content"):
                        all_articles.append(article_copy)
                    else:
                        logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng c√≥ n·ªôi dung: {article_copy.get('title', 'Kh√¥ng ti√™u ƒë·ªÅ')}")
    
    # Ki·ªÉm tra s·ªë l∆∞·ª£ng b√†i vi·∫øt sau khi thu th·∫≠p
    if not all_articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt h·ª£p l·ªá ƒë·ªÉ l∆∞u v√†o file JSON duy nh·∫•t")
        return None
    
    # T·∫°o t√™n file duy nh·∫•t
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    unique_id = str(uuid.uuid4())[:8]  # L·∫•y 8 k√Ω t·ª± ƒë·∫ßu c·ªßa UUID
    
    # Ki·ªÉm tra xem ƒë∆∞·ªùng d·∫´n /app/output c√≥ t·ªìn t·∫°i kh√¥ng (Docker environment)
    docker_output = "/app/output"
    if os.path.exists(docker_output) and os.path.isdir(docker_output):
        output_file = os.path.join(docker_output, f"all_articles_{current_time}_{unique_id}.json")
        # B·∫£o ƒë·∫£m c≈©ng t·∫°o b·∫£n sao trong OUTPUT_DIR ƒë·ªÉ import_json_file_to_backend c√≥ th·ªÉ t√¨m th·∫•y
        local_output_file = os.path.join(OUTPUT_DIR, f"all_articles_{current_time}_{unique_id}.json")
    else:
        output_file = os.path.join(OUTPUT_DIR, f"all_articles_{current_time}_{unique_id}.json")
        local_output_file = output_file
    
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c output t·ªìn t·∫°i
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    if output_file != local_output_file:
        os.makedirs(os.path.dirname(local_output_file), exist_ok=True)
    
    # L∆∞u v√†o file JSON
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_articles, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ƒê√£ l∆∞u {len(all_articles)} b√†i vi·∫øt v√†o file JSON duy nh·∫•t: {output_file}")
        
        # N·∫øu ƒëang ch·∫°y trong Docker, t·∫°o b·∫£n sao ƒë·ªÉ ƒë·∫£m b·∫£o import
        if output_file != local_output_file:
            shutil.copy2(output_file, local_output_file)
            logger.info(f"ƒê√£ t·∫°o b·∫£n sao t·∫°i: {local_output_file} ƒë·ªÉ ƒë·∫£m b·∫£o import v√†o DB")
        
        # In th√¥ng tin b√†i vi·∫øt ƒë·∫ßu ti√™n ƒë·ªÉ debug
        if all_articles:
            first_article = all_articles[0]
            logger.info(f"B√†i vi·∫øt m·∫´u: Title={first_article.get('title', 'Kh√¥ng ti√™u ƒë·ªÅ')}, Content length={len(first_article.get('content', ''))}")
            
        return local_output_file  # Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n ƒë·∫øn file c√≥ th·ªÉ ƒë·ªçc ƒë∆∞·ª£c t·ª´ h√†m import
    except Exception as e:
        logger.error(f"L·ªói khi l∆∞u file JSON: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def read_json_file(file_path):
    """
    ƒê·ªçc n·ªôi dung t·ª´ file JSON
    :param file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON
    :return: D·ªØ li·ªáu t·ª´ file JSON ho·∫∑c None n·∫øu l·ªói
    """
    if not os.path.exists(file_path):
        logger.error(f"File kh√¥ng t·ªìn t·∫°i: {file_path}")
        return None
        
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        logger.error(f"L·ªói khi ƒë·ªçc file JSON: {str(e)}")
        return None

def import_json_file_to_backend(json_file_path):
    """
    Import t·∫•t c·∫£ b√†i vi·∫øt t·ª´ file JSON v√†o backend
    :param json_file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON
    :return: tuple (s·ªë b√†i vi·∫øt import th√†nh c√¥ng, s·ªë b√†i vi·∫øt th·∫•t b·∫°i)
    """
    logger.info(f"ƒêang import b√†i vi·∫øt t·ª´ file: {json_file_path}")
    
    # Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa file
    file_exists = os.path.exists(json_file_path)
    
    # Th·ª≠ t√¨m file ·ªü ƒë∆∞·ªùng d·∫´n /app/output/ n·∫øu kh√¥ng t√¨m th·∫•y ·ªü ƒë∆∞·ªùng d·∫´n g·ªëc
    if not file_exists and not json_file_path.startswith('/app/output/'):
        filename = os.path.basename(json_file_path)
        docker_path = f"/app/output/{filename}"
        if os.path.exists(docker_path):
            logger.info(f"Kh√¥ng t√¨m th·∫•y file t·∫°i {json_file_path}, nh∆∞ng t√¨m th·∫•y t·∫°i {docker_path}")
            json_file_path = docker_path
            file_exists = True
    
    # Th·ª≠ t√¨m file ·ªü OUTPUT_DIR n·∫øu kh√¥ng t√¨m th·∫•y ·ªü ƒë∆∞·ªùng d·∫´n /app/output/
    if not file_exists and json_file_path.startswith('/app/output/'):
        filename = os.path.basename(json_file_path)
        local_path = os.path.join(OUTPUT_DIR, filename)
        if os.path.exists(local_path):
            logger.info(f"Kh√¥ng t√¨m th·∫•y file t·∫°i {json_file_path}, nh∆∞ng t√¨m th·∫•y t·∫°i {local_path}")
            json_file_path = local_path
            file_exists = True
    
    if not file_exists:
        logger.error(f"File kh√¥ng t·ªìn t·∫°i: {json_file_path}")
        return 0, 0
    
    # ƒê·ªçc file JSON
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        logger.error(f"L·ªói khi ƒë·ªçc file JSON: {str(e)}")
        logger.error(traceback.format_exc())
        return 0, 0
    
    # Ki·ªÉm tra d·ªØ li·ªáu l√† list
    if not isinstance(data, list):
        logger.error(f"D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá, ph·∫£i l√† list: {type(data)}")
        return 0, 0
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ g·ª≠i l√™n API
    articles_to_import = []
    
    for article in data:
        if not isinstance(article, dict):
            logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng h·ª£p l·ªá, kh√¥ng ph·∫£i dict: {type(article)}")
            continue
            
        # ƒê·ªçc n·ªôi dung t·ª´ file l∆∞u tr·ªØ n·∫øu c·∫ßn
        article_content = article.get("content", "")
        if not article_content and "json_filepath" in article:
            json_data = read_json_file(article["json_filepath"])
            if json_data and isinstance(json_data, dict):
                article_content = json_data.get("content", "")
                # C·∫≠p nh·∫≠t th√¥ng tin kh√°c t·ª´ file JSON
                if not article.get("title") and json_data.get("title"):
                    article["title"] = json_data["title"]
                if not article.get("summary") and json_data.get("summary"):
                    article["summary"] = json_data["summary"]
                
        # Chu·∫©n b·ªã d·ªØ li·ªáu b√†i vi·∫øt
        article_data = {
            "title": article.get("title", ""),
            "slug": generate_slug(article.get("title", "Kh√¥ng ti√™u ƒë·ªÅ")),
            "summary": article.get("summary", ""),
            "content": article_content,
            "source_name": article.get("source_name", ""),
            "source_url": article.get("source_url", article.get("url", "")),
            "source_icon": article.get("source_icon", ""),
            "published_at": article.get("published_at", datetime.now().isoformat()),
            "meta_data": article.get("meta_data", {}),
            "category": article.get("category_id", 1)  # ƒê·∫£m b·∫£o s·ª≠ d·ª•ng category_id, kh√¥ng ph·∫£i category
        }
        
        # T·∫°o summary t·ª´ n·ªôi dung n·∫øu kh√¥ng c√≥
        if not article_data["summary"] and article_data["content"]:
            sentences = re.split(r'[.!?]+', article_data["content"])
            if len(sentences) >= 2:
                article_data["summary"] = '. '.join(s.strip() for s in sentences[:2] if s.strip()) + '.'
            else:
                article_data["summary"] = sentences[0] if sentences else ""
                
        # T·∫°o source_name t·ª´ URL n·∫øu kh√¥ng c√≥
        if not article_data["source_name"] and article_data["source_url"]:
            try:
                parsed_url = urlparse(article_data["source_url"])
                article_data["source_name"] = parsed_url.netloc
            except:
                article_data["source_name"] = "Kh√¥ng r√µ"
                
        # T·∫°o source_icon t·ª´ domain n·∫øu kh√¥ng c√≥
        if not article_data["source_icon"] and article_data["source_url"]:
            try:
                parsed_url = urlparse(article_data["source_url"])
                domain = parsed_url.netloc
                article_data["source_icon"] = f"https://www.google.com/s2/favicons?domain={domain}"
            except:
                article_data["source_icon"] = ""
        
        # Ki·ªÉm tra n·ªôi dung b√†i vi·∫øt
        if not article_data["content"]:
            logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng c√≥ n·ªôi dung: {article_data['title']}")
            continue
            
        articles_to_import.append(article_data)
    
    if not articles_to_import:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt h·ª£p l·ªá ƒë·ªÉ import")
        return 0, 0
    
    logger.info(f"ƒêang import {len(articles_to_import)} b√†i vi·∫øt")
    
    # In ph·∫ßn ƒë·∫ßu c·ªßa b√†i vi·∫øt ƒë·∫ßu ti√™n ƒë·ªÉ debug
    if articles_to_import:
        first_article = articles_to_import[0]
        logger.info(f"M·∫´u b√†i vi·∫øt ƒë·∫ßu ti√™n: Title={first_article['title']}, Content length={len(first_article['content'])}, Source={first_article['source_name']}")
    
    # Chu·∫©n b·ªã payload theo ƒë√∫ng ƒë·ªãnh d·∫°ng API y√™u c·∫ßu
    payload = {
        "articles": articles_to_import
    }
    
    # Th·ª≠ import v·ªõi endpoint import
    try:
        logger.info(f"Th·ª≠ import v·ªõi endpoint: {ARTICLES_IMPORT_API_URL}")
        
        # G·ªçi API import
        response = requests.post(
            ARTICLES_IMPORT_API_URL,
            json=payload,
            headers={"Content-Type": "application/json", "Accept": "application/json"}
        )
        
        logger.info(f"Ph·∫£n h·ªìi t·ª´ endpoint import: Status code: {response.status_code}")
        logger.info(f"Ph·∫£n h·ªìi n·ªôi dung: {response.text[:500]}")
        
        if response.status_code == 200:
            try:
                result = response.json()
                success = result.get("success", 0) or (len(articles_to_import) - result.get("skipped", 0))
                failed = result.get("failed", 0) or result.get("skipped", 0)
                logger.info(f"Import th√†nh c√¥ng v·ªõi endpoint import: {success} th√†nh c√¥ng, {failed} th·∫•t b·∫°i")
                
                # X√≥a file JSON sau khi import th√†nh c√¥ng n·∫øu c√≥ b√†i vi·∫øt ƒë∆∞·ª£c import
                if success > 0:
                    try:
                        os.remove(json_file_path)
                        logger.info(f"ƒê√£ x√≥a file JSON sau khi import th√†nh c√¥ng: {json_file_path}")
                    except Exception as e:
                        logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {json_file_path}: {str(e)}")
                return success, failed
            except Exception as e:
                logger.error(f"L·ªói khi x·ª≠ l√Ω ph·∫£n h·ªìi t·ª´ endpoint import: {str(e)}")
        else:
            logger.warning(f"Import v·ªõi endpoint import th·∫•t b·∫°i. Status code: {response.status_code}")
    except Exception as e:
        logger.error(f"L·ªói khi g·ªçi endpoint import: {str(e)}")
        logger.error(traceback.format_exc())
    
    # N·∫øu endpoint import th·∫•t b·∫°i, th·ª≠ import tr·ª±c ti·∫øp v√†o DB
    logger.info("Th·ª≠ import tr·ª±c ti·∫øp v√†o database...")
    success, failed = import_articles_to_database_directly(articles_to_import)
    
    # X√≥a file JSON sau khi import th√†nh c√¥ng n·∫øu c√≥ b√†i vi·∫øt ƒë∆∞·ª£c import
    if success > 0:
        try:
            os.remove(json_file_path)
            logger.info(f"ƒê√£ x√≥a file JSON sau khi import th√†nh c√¥ng: {json_file_path}")
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {json_file_path}: {str(e)}")
    
    return success, failed

def import_articles_to_database_directly(articles):
    """
    Import b√†i vi·∫øt tr·ª±c ti·∫øp v√†o database (fallback khi API th·∫•t b·∫°i)
    :param articles: Danh s√°ch b√†i vi·∫øt c·∫ßn import
    :return: tuple (s·ªë b√†i vi·∫øt import th√†nh c√¥ng, s·ªë b√†i vi·∫øt th·∫•t b·∫°i)
    """
    if not check_db_connection():
        logger.error("Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn database")
        return 0, 0
    
    logger.info(f"ƒêang import {len(articles)} b√†i vi·∫øt tr·ª±c ti·∫øp v√†o database")
    
    # K·∫øt n·ªëi ƒë·∫øn database
    conn = mysql.connector.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    success_count = 0
    failed_count = 0
    
    try:
        for article in articles:
            try:
                # Chu·∫©n b·ªã d·ªØ li·ªáu
                title = article.get('title', '')
                slug = article.get('slug', '')
                summary = article.get('summary', '')
                content = article.get('content', '')
                source_name = article.get('source_name', '')
                source_url = article.get('source_url', '')
                source_icon = article.get('source_icon', '')
                published_at = article.get('published_at', datetime.now().isoformat())
                category = article.get('category', 1)  # ƒê·∫£m b·∫£o s·ª≠ d·ª•ng ƒë√∫ng t√™n c·ªôt trong database
                meta_data = article.get('meta_data', {})
                
                # ƒê·∫£m b·∫£o meta_data l√† string JSON
                if isinstance(meta_data, dict):
                    meta_data = json.dumps(meta_data)
                
                # SQL query ƒë·ªÉ insert b√†i vi·∫øt
                query = """
                INSERT INTO articles 
                (title, slug, summary, content, source_name, source_url, source_icon, 
                published_at, category, meta_data, is_processed, created_at, updated_at) 
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
                """
                
                # T·∫°o tuple c√°c gi√° tr·ªã
                values = (
                    title, slug, summary, content, source_name, source_url, source_icon,
                    published_at, category, meta_data, True
                )
                
                # Th·ª±c hi·ªán query
                cursor.execute(query, values)
                
                # ƒê·∫øm s·ªë b√†i vi·∫øt th√†nh c√¥ng
                success_count += 1
                logger.info(f"ƒê√£ import th√†nh c√¥ng b√†i vi·∫øt: {title}")
                
            except mysql.connector.Error as e:
                # B·ªè qua l·ªói duplicate key
                if e.errno == 1062:  # M√£ l·ªói cho duplicate key
                    logger.warning(f"B√†i vi·∫øt ƒë√£ t·ªìn t·∫°i (slug tr√πng l·∫∑p): {article.get('title', '')}")
                else:
                    logger.error(f"L·ªói MySQL khi import b√†i vi·∫øt: {str(e)}")
                
                failed_count += 1
            except Exception as e:
                logger.error(f"L·ªói khi import b√†i vi·∫øt: {str(e)}")
                failed_count += 1
        
        # Commit c√°c thay ƒë·ªïi
        conn.commit()
        logger.info(f"ƒê√£ import tr·ª±c ti·∫øp v√†o database: {success_count} th√†nh c√¥ng, {failed_count} th·∫•t b·∫°i")
        
    except Exception as e:
        logger.error(f"L·ªói khi import v√†o database: {str(e)}")
        logger.error(traceback.format_exc())
        conn.rollback()
    finally:
        cursor.close()
        conn.close()
    
    return success_count, failed_count

def check_db_connection():
    """
    Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn c∆° s·ªü d·ªØ li·ªáu
    
    Returns:
        bool: True n·∫øu k·∫øt n·ªëi th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        if conn.is_connected():
            logger.info("K·∫øt n·ªëi CSDL th√†nh c√¥ng")
            conn.close()
            return True
        else:
            logger.error("Kh√¥ng th·ªÉ k·∫øt n·ªëi CSDL")
            return False
    except Exception as e:
        logger.error(f"L·ªói k·∫øt n·ªëi CSDL: {str(e)}")
        return False

def check_backend_api():
    """
    Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn backend API
    
    Returns:
        bool: True n·∫øu k·∫øt n·ªëi th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    try:
        # T·∫£i l·∫°i c·∫•u h√¨nh ƒë·ªÉ c√≥ th√¥ng tin m·ªõi nh·∫•t
        current_config = get_config()
        
        logger.info(f"Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn Backend API: {CATEGORIES_API_URL}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'
        }
        
        # Th√™m Host header n·∫øu c·∫•u h√¨nh y√™u c·∫ßu
        if current_config.get("USE_HOST_HEADER", False):
            headers['Host'] = 'magazine.test'
            logger.info("S·ª≠ d·ª•ng Host header: magazine.test")
            
        # Ki·ªÉm tra ph√°t hi·ªán m√¥i tr∆∞·ªùng
        if os.path.exists('/.dockerenv'):
            logger.info(f"ƒêang ch·∫°y trong Docker container. Backend URL: {CATEGORIES_API_URL}")
            if 'linux' in sys.platform.lower():
                logger.info("H·ªá ƒëi·ªÅu h√†nh: Linux")
            else:
                logger.info("H·ªá ƒëi·ªÅu h√†nh: Windows ho·∫∑c kh√°c")
        
        # Th·ª≠ k·∫øt n·ªëi ƒë·∫øn API
        logger.info(f"G·ª≠i request ƒë·∫øn: {CATEGORIES_API_URL} v·ªõi headers: {headers}")
        response = requests.get(CATEGORIES_API_URL, headers=headers, timeout=10)
        
        # Log ph·∫£n h·ªìi
        logger.info(f"Response status code: {response.status_code}")
        logger.info(f"Response headers: {response.headers}")
        
        if response.status_code == 200:
            categories = response.json()
            logger.info(f"K·∫øt n·ªëi API th√†nh c√¥ng! T√¨m th·∫•y {len(categories)} danh m·ª•c.")
            
            # In ra 3 danh m·ª•c ƒë·∫ßu ti√™n ƒë·ªÉ ki·ªÉm tra
            for i, category in enumerate(categories[:3]):
                logger.info(f"Danh m·ª•c {i+1}: ID={category.get('id')}, T√™n={category.get('name')}")
                
            return True
        else:
            # N·∫øu th·∫•t b·∫°i v√† ƒëang s·ª≠ d·ª•ng Host header, th·ª≠ k·∫øt n·ªëi tr·ª±c ti·∫øp ƒë·∫øn magazine.test
            if current_config.get("USE_HOST_HEADER", False):
                try:
                    direct_url = "http://magazine.test/api/categories"
                    logger.info(f"Th·ª≠ k·∫øt n·ªëi tr·ª±c ti·∫øp ƒë·∫øn domain: {direct_url}")
                    direct_response = requests.get(direct_url, timeout=10)
                    
                    if direct_response.status_code == 200:
                        categories = direct_response.json()
                        logger.info(f"K·∫øt n·ªëi th√†nh c√¥ng th√¥ng qua domain magazine.test! T√¨m th·∫•y {len(categories)} danh m·ª•c.")
                        # C·∫≠p nh·∫≠t URL s·ª≠ d·ª•ng domain cho l·∫ßn sau
                        config["BACKEND_URL"] = "http://magazine.test"
                        config["BASE_API_URL"] = "http://magazine.test/api"
                        config["CATEGORIES_API_URL"] = f"{config['BASE_API_URL']}/categories"
                        return True
                except Exception as domain_err:
                    logger.error(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi tr·ª±c ti·∫øp ƒë·∫øn domain magazine.test: {str(domain_err)}")
            
            logger.error(f"L·ªói API HTTP {response.status_code}: {response.text}")
            logger.error("Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn backend API. Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh v√† k·∫øt n·ªëi m·∫°ng.")
            return False
    except Exception as e:
        logger.error(f"L·ªói k·∫øt n·ªëi API: {str(e)}")
        return False

def generate_slug(title, add_uuid=True):
    """
    T·∫°o slug t·ª´ ti√™u ƒë·ªÅ b√†i vi·∫øt
    :param title: Ti√™u ƒë·ªÅ b√†i vi·∫øt
    :param add_uuid: Th√™m UUID v√†o slug ƒë·ªÉ ƒë·∫£m b·∫£o duy nh·∫•t
    :return: Slug ƒë√£ ƒë∆∞·ª£c t·∫°o
    """
    # Lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát
    slug = unidecode(title)
    
    # Chuy·ªÉn ƒë·ªïi th√†nh ch·ªØ th∆∞·ªùng v√† thay th·∫ø c√°c k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ c√°i, s·ªë b·∫±ng d·∫•u g·∫°ch ngang
    slug = re.sub(r'[^\w\s-]', '', slug.lower())
    slug = re.sub(r'[\s_-]+', '-', slug).strip('-')
    
    # Th√™m UUID ƒë·ªÉ ƒë·∫£m b·∫£o duy nh·∫•t
    if add_uuid:
        unique_id = str(uuid.uuid4())[:8]
        slug = f"{slug}-{unique_id}"
        
    return slug

def main():
    """
    H√†m ch√≠nh x·ª≠ l√Ω lu·ªìng l√†m vi·ªác c·ªßa script
    """
    import argparse
    
    # Thi·∫øt l·∫≠p argument parser
    parser = argparse.ArgumentParser(description='C√¥ng c·ª• t√¨m ki·∫øm v√† tr√≠ch xu·∫•t b√†i vi·∫øt t·ª´ Google News')
    
    # C√°c tham s·ªë c∆° b·∫£n
    parser.add_argument('--all', action='store_true', help='X·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c')
    parser.add_argument('--category', type=int, help='ID c·ªßa danh m·ª•c c·∫ßn x·ª≠ l√Ω')
    parser.add_argument('--subcategory', type=int, help='ID c·ªßa danh m·ª•c con c·∫ßn x·ª≠ l√Ω')
    parser.add_argument('--keyword', type=str, help='T·ª´ kh√≥a t√¨m ki·∫øm')
    parser.add_argument('--import-dir', type=str, help='Th∆∞ m·ª•c ch·ª©a file JSON c·∫ßn import')
    parser.add_argument('--import-file', type=str, help='File JSON c·∫ßn import')
    parser.add_argument('--auto-send', action='store_true', help='T·ª± ƒë·ªông g·ª≠i b√†i vi·∫øt v√†o backend', default=True)
    parser.add_argument('--use-subcategories', action='store_true', help='S·ª≠ d·ª•ng danh m·ª•c con khi t√¨m ki·∫øm')
    
    # Ph√¢n t√≠ch tham s·ªë ƒë·∫ßu v√†o
    args = parser.parse_args()
    
    # Log c√°c tham s·ªë ƒë·∫ßu v√†o
    logger.info(f"C√°c tham s·ªë: {args}")
    
    # ƒê·∫£m b·∫£o ch·ªâ c√≥ m·ªôt t√πy ch·ªçn ƒë∆∞·ª£c ch·ªçn
    options_count = sum([
        1 if args.all else 0,
        1 if args.category else 0,
        1 if args.keyword else 0,
        1 if args.import_dir else 0,
        1 if args.import_file else 0
    ])
    
    if options_count > 1 and not (args.category and args.subcategory):
        logger.error("Ch·ªâ ƒë∆∞·ª£c ph√©p ch·ªçn m·ªôt trong c√°c t√πy ch·ªçn: --all, --category, --keyword, --import-dir, --import-file")
        print("L·ªói: Ch·ªâ ƒë∆∞·ª£c ph√©p ch·ªçn m·ªôt trong c√°c t√πy ch·ªçn: --all, --category, --keyword, --import-dir, --import-file")
        return
    
    if options_count == 0:
        logger.error("B·∫°n c·∫ßn ch·ªçn √≠t nh·∫•t m·ªôt trong c√°c t√πy ch·ªçn: --all, --category, --subcategory, --keyword, --import-dir, --import-file")
        print("L·ªói: B·∫°n c·∫ßn ch·ªçn √≠t nh·∫•t m·ªôt trong c√°c t√πy ch·ªçn: --all, --category, --subcategory, --keyword, --import-dir, --import-file")
        return
    
    # N·∫øu ch·∫°y v·ªõi --all, t·ª± ƒë·ªông b·∫≠t ch·∫ø ƒë·ªô s·ª≠ d·ª•ng danh m·ª•c con n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if args.all and not args.use_subcategories:
        logger.info("T·ª± ƒë·ªông b·∫≠t ch·∫ø ƒë·ªô s·ª≠ d·ª•ng danh m·ª•c con khi ch·∫°y v·ªõi --all")
        args.use_subcategories = True
    
    # X·ª≠ l√Ω t√πy ch·ªçn import t·ª´ th∆∞ m·ª•c
    if args.import_dir:
        import_dir = args.import_dir
        logger.info(f"Import b√†i vi·∫øt t·ª´ th∆∞ m·ª•c: {import_dir}")
        
        success_count, failed_count = import_all_to_backend(import_dir)
        
        if success_count > 0:
            logger.info(f"Import t·ª´ th∆∞ m·ª•c th√†nh c√¥ng: {success_count} b√†i vi·∫øt, {failed_count} b√†i vi·∫øt th·∫•t b·∫°i")
        else:
            logger.error(f"Import t·ª´ th∆∞ m·ª•c th·∫•t b·∫°i. C√≥ {failed_count} l·ªói.")
        
        return
    
    # X·ª≠ l√Ω import file
    if args.import_file:
        import_file = args.import_file
        logger.info(f"Import b√†i vi·∫øt t·ª´ file: {import_file}")
        
        success = import_json_file_to_backend(import_file)
        
        if success > 0:
            logger.info(f"Import t·ª´ file th√†nh c√¥ng: {success} b√†i vi·∫øt")
        else:
            logger.error("Import t·ª´ file th·∫•t b·∫°i")
        
        return
    
    # X·ª≠ l√Ω subcategory c·ª• th·ªÉ (n·∫øu c·∫£ category v√† subcategory ƒë·ªÅu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh)
    if args.category and args.subcategory:
        logger.info(f"X·ª≠ l√Ω danh m·ª•c con c·ª• th·ªÉ: Category ID={args.category}, Subcategory ID={args.subcategory}")
        
        # L·∫•y th√¥ng tin danh m·ª•c
        category = get_category_by_id(args.category)
        if not category:
            logger.error(f"Kh√¥ng t√¨m th·∫•y danh m·ª•c v·ªõi ID: {args.category}")
            return
            
        category_name = category.get('name', 'Kh√¥ng c√≥ t√™n')
        
        # L·∫•y th√¥ng tin danh m·ª•c con
        subcategory = get_subcategory_by_id(args.subcategory)
        if not subcategory:
            logger.error(f"Kh√¥ng t√¨m th·∫•y danh m·ª•c con v·ªõi ID: {args.subcategory}")
            return
            
        subcategory_name = subcategory.get('name', 'Kh√¥ng c√≥ t√™n')
        
        logger.info(f"B·∫Øt ƒë·∫ßu t√¨m ki·∫øm b√†i vi·∫øt cho danh m·ª•c con: {subcategory_name} (thu·ªôc danh m·ª•c {category_name})")
        
        # T√¨m ki·∫øm b√†i vi·∫øt cho danh m·ª•c con c·ª• th·ªÉ n√†y
        all_articles = []
        for i in range(MAX_ARTICLES_PER_SUBCATEGORY):
            logger.info(f"T√¨m ki·∫øm b√†i vi·∫øt th·ª© {i+1}/{MAX_ARTICLES_PER_SUBCATEGORY} cho danh m·ª•c con: {subcategory_name}")
            article = search_with_category(args.category, args.subcategory)
            
            if article and article.get('title'):
                logger.info(f"ƒê√£ t√¨m th·∫•y b√†i vi·∫øt: {article['title']}")
                logger.info(f"URL: {article['url']}")
                logger.info(f"ƒê√£ l∆∞u v√†o: {article['json_filepath']}")
                all_articles.append(article)
            else:
                logger.warning(f"Kh√¥ng t√¨m th·∫•y th√™m b√†i vi·∫øt n√†o cho danh m·ª•c con: {subcategory_name}")
                break
                
            # Th√™m kho·∫£ng th·ªùi gian ngh·ªâ gi·ªØa c√°c l·∫ßn t√¨m ki·∫øm ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
            time.sleep(2)
        
        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        if all_articles:
            logger.info(f"ƒê√£ t√¨m th·∫•y {len(all_articles)} b√†i vi·∫øt cho danh m·ª•c con {subcategory_name}")
            
            # G·ª≠i b√†i vi·∫øt l√™n backend n·∫øu c√≥ tham s·ªë auto-send
            if args.auto_send:
                send_to_backend(all_articles, auto_send=True)
        else:
            logger.warning(f"Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o cho danh m·ª•c con: {subcategory_name}")
        
        return
    
    # X·ª≠ l√Ω m·ªôt danh m·ª•c c·ª• th·ªÉ
    if args.category:
        logger.info(f"X·ª≠ l√Ω danh m·ª•c c·ª• th·ªÉ: ID={args.category}")
        
        # L·∫•y th√¥ng tin danh m·ª•c
        category = get_category_by_id(args.category)
        if not category:
            logger.error(f"Kh√¥ng t√¨m th·∫•y danh m·ª•c v·ªõi ID: {args.category}")
            return
            
        category_name = category.get('name', 'Kh√¥ng c√≥ t√™n')
        
        # X·ª≠ l√Ω danh m·ª•c
        results = process_category(args.category, category_name, max_articles=MAX_ARTICLES_PER_CATEGORY)
        
        # G·ª≠i k·∫øt qu·∫£ l√™n backend n·∫øu c√≥ tham s·ªë auto-send
        if args.auto_send and results.get('articles'):
            send_to_backend(results.get('articles'), auto_send=True)
        
        return
    
    # X·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c
    if args.all:
        logger.info("X·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c t·ª´ backend")
        results = find_and_process_all_categories(max_articles_per_category=MAX_ARTICLES_PER_CATEGORY, use_subcategories=args.use_subcategories)
        
        # L∆∞u t·∫•t c·∫£ k·∫øt qu·∫£ v√†o m·ªôt file JSON duy nh·∫•t
        json_file = save_all_articles_to_single_file(results)
        
        # G·ª≠i k·∫øt qu·∫£ l√™n backend n·∫øu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if args.auto_send:
            logger.info("G·ª≠i k·∫øt qu·∫£ l√™n backend")
            # T√°ch th√†nh c√°c batch nh·ªè ƒë·ªÉ g·ª≠i
            send_to_backend(results['all_articles'], auto_send=True)
        
        # Hi·ªÉn th·ªã s·ªë li·ªáu th·ªëng k√™
        success_count = len([c for c in results['articles_by_category'].values() if c])
        total_categories = len(results['articles_by_category'])
        total_articles = results['total_articles']
        logger.info(f"T·ªïng s·ªë danh m·ª•c: {total_categories}")
        logger.info(f"S·ªë danh m·ª•c th√†nh c√¥ng: {success_count}")
        logger.info(f"S·ªë danh m·ª•c th·∫•t b·∫°i: {total_categories - success_count}")
        logger.info(f"T·ªïng s·ªë b√†i vi·∫øt: {total_articles}")
        
        return

if __name__ == "__main__":
    # Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn backend tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu
    if not check_backend_api():
        logger.error("Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn backend API. Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh v√† k·∫øt n·ªëi m·∫°ng.")
        sys.exit(1)
    
    # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        logger.info(f"ƒê√£ t·∫°o th∆∞ m·ª•c output: {OUTPUT_DIR}")
    
    # Kh·ªüi ch·∫°y h√†m main
    main() 