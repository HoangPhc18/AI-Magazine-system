#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ƒê√¢y l√† module ch√≠nh cho scraper b√†i vi·∫øt tin t·ª©c
"""

import os
import sys
import json
import uuid
import time
import shutil
import logging
import argparse
import requests
from pathlib import Path
from datetime import datetime, timedelta
from urllib.parse import urlparse

# Import c√°c module n·ªôi b·ªô
import google_news_serpapi
from google_news_serpapi import fetch_articles_by_category, get_categories
import scrape_articles_selenium

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(f"scraper_{datetime.now().strftime('%Y%m%d')}.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

# üîπ C√°c th√¥ng s·ªë m·∫∑c ƒë·ªãnh
SCRAPER_DIR = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.path.join(SCRAPER_DIR, "temp")
OUTPUT_DIR = os.path.join(SCRAPER_DIR, "output")
DEFAULT_BATCH_SIZE = 5
# S·ªë ng√†y ƒë·ªÉ gi·ªØ l·∫°i log files v√† output files
RETENTION_DAYS = 2

# üîπ Laravel Backend API URLs
BACKEND_API_URL = "http://localhost:8000/api/articles/import"
CATEGORIES_API_URL = "http://localhost:8000/api/categories"

def extract_content(articles, extraction_method="javascript"):
    """
    Tr√≠ch xu·∫•t n·ªôi dung b√†i vi·∫øt t·ª´ URLs s·ª≠ d·ª•ng Selenium
    
    Args:
        articles (list): Danh s√°ch c√°c b√†i vi·∫øt (dicts)
        extraction_method (str): Ph∆∞∆°ng th·ª©c tr√≠ch xu·∫•t (javascript/readability)
        
    Returns:
        list: Danh s√°ch b√†i vi·∫øt ƒë√£ ƒë∆∞·ª£c b·ªï sung n·ªôi dung
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ tr√≠ch xu·∫•t n·ªôi dung!")
        return []
    
    # Thi·∫øt l·∫≠p driver Selenium
    driver = scrape_articles_selenium.setup_driver()
    
    try:
        enriched_articles = []
        
        for i, article in enumerate(articles, 1):
            title = article.get("title", "")
            url = article.get("source_url", "")
            
            if not url:
                logger.warning(f"B√†i vi·∫øt {i} kh√¥ng c√≥ URL: {title}")
                continue
            
            # Ki·ªÉm tra URL ph√π h·ª£p
            if "vtv.vn/video/" in url:
                logger.info(f"[INFO] B·ªè qua b√†i vi·∫øt video: {title} - {url}")
                continue
            
            logger.info(f"[INFO] ƒêang tr√≠ch xu·∫•t n·ªôi dung cho b√†i vi·∫øt {i}/{len(articles)}: {title}")
            
            try:
                # S·ª≠ d·ª•ng Selenium ƒë·ªÉ tr√≠ch xu·∫•t n·ªôi dung
                content = scrape_articles_selenium.extract_content(driver, url, title)
                
                # C·∫≠p nh·∫≠t b√†i vi·∫øt v·ªõi n·ªôi dung ƒë√£ tr√≠ch xu·∫•t
                enriched_article = article.copy()
                enriched_article["content"] = content
                
                # C·∫≠p nh·∫≠t meta_data
                if isinstance(enriched_article.get("meta_data"), dict):
                    enriched_article["meta_data"]["extracted_at"] = datetime.now().isoformat()
                    enriched_article["meta_data"]["word_count"] = len(content.split())
                else:
                    # T·∫°o metadata n·∫øu ch∆∞a c√≥
                    enriched_article["meta_data"] = {
                        "extracted_at": datetime.now().isoformat(),
                        "word_count": len(content.split())
                    }
                
                # ƒê·∫£m b·∫£o source_name l√† chu·ªói
                if isinstance(enriched_article.get("source_name"), dict):
                    enriched_article["source_name"] = enriched_article["source_name"].get("name", "Unknown Source")
                
                enriched_articles.append(enriched_article)
                logger.info(f"[OK] ƒê√£ tr√≠ch xu·∫•t n·ªôi dung cho b√†i vi·∫øt {i}/{len(articles)}: {title}")
                
                # Th√™m ƒë·ªô tr·ªÖ gi·ªØa c√°c request ƒë·ªÉ tr√°nh t·∫£i qu√° m·ª©c
                if i < len(articles):
                    time.sleep(2)
                
            except Exception as e:
                logger.error(f"[ERROR] L·ªói khi tr√≠ch xu·∫•t {url}: {str(e)}")
                # V·∫´n th√™m b√†i vi·∫øt v√†o danh s√°ch k·ªÉ c·∫£ khi l·ªói, nh∆∞ng v·ªõi n·ªôi dung r·ªóng
                article["content"] = f"L·ªói tr√≠ch xu·∫•t: {str(e)}"
                enriched_articles.append(article)
    
    finally:
        # ƒê√≥ng driver khi ho√†n th√†nh
        driver.quit()
    
    return enriched_articles

def save_articles_to_file(articles, output_file=None):
    """
    L∆∞u danh s√°ch b√†i vi·∫øt v√†o file JSON
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt
        output_file (str): ƒê∆∞·ªùng d·∫´n file ƒë·∫ßu ra
        
    Returns:
        str: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ƒë√£ l∆∞u
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ l∆∞u!")
        return None
    
    # T·∫°o th∆∞ m·ª•c ƒë·∫ßu ra n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # T·∫°o t√™n file m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
    if not output_file:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(OUTPUT_DIR, f"enriched_articles_{timestamp}.json")
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u
    normalized_articles = []
    for article in articles:
        normalized = article.copy()
        
        # ƒê·∫£m b·∫£o source_name l√† chu·ªói
        if isinstance(normalized.get("source_name"), dict):
            normalized["source_name"] = normalized["source_name"].get("name", "Unknown Source")
        
        # ƒê·∫£m b·∫£o meta_data l√† chu·ªói JSON
        if isinstance(normalized.get("meta_data"), dict):
            normalized["meta_data"] = json.dumps(normalized["meta_data"])
        
        # ƒê·∫£m b·∫£o summary kh√¥ng bao gi·ªù l√† None
        if normalized.get("summary") is None:
            normalized["summary"] = ""
        
        # ƒê·∫£m b·∫£o content kh√¥ng bao gi·ªù l√† None
        if normalized.get("content") is None:
            normalized["content"] = ""
        
        # ƒê·∫£m b·∫£o published_at c√≥ ƒë·ªãnh d·∫°ng chu·∫©n
        if normalized.get("published_at"):
            try:
                # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y th√°ng
                date_obj = datetime.fromisoformat(normalized["published_at"].replace('Z', '+00:00'))
                normalized["published_at"] = date_obj.isoformat()
            except (ValueError, TypeError):
                # N·∫øu kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        
        normalized_articles.append(normalized)
    
    # L∆∞u b√†i vi·∫øt v√†o file JSON
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(normalized_articles, f, ensure_ascii=False, indent=4)
    
    logger.info(f"[OK] ƒê√£ l∆∞u {len(normalized_articles)} b√†i vi·∫øt v√†o {output_file}")
    return output_file

def send_to_backend(articles, batch_size=DEFAULT_BATCH_SIZE, auto_send=True):
    """
    G·ª≠i b√†i vi·∫øt t·ªõi backend API
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt
        batch_size (int): S·ªë l∆∞·ª£ng b√†i vi·∫øt g·ª≠i trong m·ªói request
        auto_send (bool): T·ª± ƒë·ªông g·ª≠i kh√¥ng c·∫ßn x√°c nh·∫≠n
        
    Returns:
        bool: Tr·∫°ng th√°i th√†nh c√¥ng
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ g·ª≠i!")
        return False
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi g·ª≠i
    normalized_articles = []
    for article in articles:
        normalized = article.copy()
        
        # ƒê·∫£m b·∫£o source_name l√† chu·ªói
        if isinstance(normalized.get("source_name"), dict):
            normalized["source_name"] = normalized["source_name"].get("name", "Unknown Source")
        
        # ƒê·∫£m b·∫£o meta_data l√† chu·ªói JSON
        if isinstance(normalized.get("meta_data"), dict):
            normalized["meta_data"] = json.dumps(normalized["meta_data"])
        
        # ƒê·∫£m b·∫£o summary kh√¥ng bao gi·ªù l√† None
        if normalized.get("summary") is None:
            normalized["summary"] = ""
        
        # ƒê·∫£m b·∫£o content kh√¥ng bao gi·ªù l√† None
        if normalized.get("content") is None:
            normalized["content"] = ""
        
        # ƒê·∫£m b·∫£o published_at c√≥ ƒë·ªãnh d·∫°ng chu·∫©n
        if normalized.get("published_at"):
            try:
                # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y th√°ng
                date_obj = datetime.fromisoformat(normalized["published_at"].replace('Z', '+00:00'))
                normalized["published_at"] = date_obj.isoformat()
            except (ValueError, TypeError):
                # N·∫øu kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        
        normalized_articles.append(normalized)
    
    # T·ª± ƒë·ªông g·ª≠i m√† kh√¥ng c·∫ßn h·ªèi
    logger.info(f"T·ª± ƒë·ªông g·ª≠i {len(normalized_articles)} b√†i vi·∫øt t·ªõi backend")
    
    success = True
    total_batches = (len(normalized_articles) + batch_size - 1) // batch_size
    
    for i in range(0, len(normalized_articles), batch_size):
        batch = normalized_articles[i:i+batch_size]
        batch_num = i // batch_size + 1
        
        logger.info(f"G·ª≠i batch {batch_num}/{total_batches} ({len(batch)} b√†i vi·∫øt)")
        
        try:
            payload = {"articles": batch}
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json"
            }
            
            # In payload ƒë·ªÉ ki·ªÉm tra
            logger.debug(f"Payload m·∫´u (b√†i vi·∫øt ƒë·∫ßu ti√™n): {json.dumps(batch[0], ensure_ascii=False)[:200]}...")
            
            # Thi·∫øt l·∫≠p timeout v√† s·ªë l·∫ßn th·ª≠ l·∫°i
            max_retries = 3
            retry_count = 0
            
            while retry_count < max_retries:
                try:
                    response = requests.post(
                        BACKEND_API_URL, 
                        json=payload, 
                        headers=headers,
                        timeout=30  # 30 gi√¢y timeout
                    )
                    
                    # Ki·ªÉm tra xem response c√≥ ph·∫£i JSON kh√¥ng
                    response_content = response.text
                    logger.debug(f"Response raw: {response_content[:200]}...")
                    
                    if response.status_code in (200, 201):
                        try:
                            result = response.json()
                            logger.info(f"[OK] Batch {batch_num}: {result.get('message', '')}")
                            if 'errors' in result and result['errors']:
                                logger.warning(f"[WARN] C√≥ {len(result['errors'])} l·ªói trong batch {batch_num}:")
                                for error in result['errors']:
                                    logger.warning(f"  - {error}")
                        except json.JSONDecodeError:
                            logger.warning(f"[WARN] Response kh√¥ng ph·∫£i JSON h·ª£p l·ªá m·∫∑c d√π status code = {response.status_code}")
                            logger.warning(f"[WARN] Response content: {response_content[:200]}...")
                        break
                    else:
                        logger.error(f"[ERROR] Batch {batch_num}: L·ªói {response.status_code} - {response.text}")
                        retry_count += 1
                        if retry_count < max_retries:
                            wait_time = 2 ** retry_count  # TƒÉng th·ªùi gian ch·ªù theo c·∫•p s·ªë nh√¢n
                            logger.info(f"Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                            time.sleep(wait_time)
                        else:
                            success = False
                            logger.error(f"[ERROR] ƒê√£ th·ª≠ l·∫°i {max_retries} l·∫ßn kh√¥ng th√†nh c√¥ng cho batch {batch_num}")
                
                except requests.exceptions.Timeout:
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = 2 ** retry_count
                        logger.info(f"Request timeout. Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                    else:
                        success = False
                        logger.error(f"[ERROR] Timeout sau {max_retries} l·∫ßn th·ª≠ l·∫°i cho batch {batch_num}")
                
                except requests.exceptions.ConnectionError:
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = 2 ** retry_count
                        logger.info(f"L·ªói k·∫øt n·ªëi. Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                    else:
                        success = False
                        logger.error(f"[ERROR] L·ªói k·∫øt n·ªëi sau {max_retries} l·∫ßn th·ª≠ l·∫°i cho batch {batch_num}")
                
                except Exception as e:
                    logger.error(f"[ERROR] Batch {batch_num}: L·ªói kh√¥ng mong ƒë·ª£i: {str(e)}")
                    success = False
                    break
            
        except Exception as e:
            logger.error(f"[ERROR] L·ªói khi g·ª≠i batch {batch_num}: {str(e)}")
            success = False
    
    if success:
        logger.info(f"[OK] ƒê√£ g·ª≠i th√†nh c√¥ng {len(normalized_articles)} b√†i vi·∫øt t·ªõi backend")
    else:
        logger.warning("[WARN] C√≥ l·ªói x·∫£y ra khi g·ª≠i b√†i vi·∫øt t·ªõi backend")
    
    return success

def cleanup_temp_files():
    """
    D·ªçn d·∫πp c√°c file t·∫°m th·ªùi
    """
    if os.path.exists(TEMP_DIR):
        try:
            shutil.rmtree(TEMP_DIR)
            logger.info(f"[OK] ƒê√£ d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {TEMP_DIR}")
        except Exception as e:
            logger.error(f"[ERROR] L·ªói khi d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {str(e)}")

def cleanup_old_files(days=RETENTION_DAYS):
    """
    D·ªçn d·∫πp c√°c file c≈© (logs, outputs) ƒë·ªÉ tr√°nh t·ªën dung l∆∞·ª£ng
    
    Args:
        days (int): S·ªë ng√†y ƒë·ªÉ gi·ªØ l·∫°i file
    """
    # Th·ªùi ƒëi·ªÉm hi·ªán t·∫°i
    now = datetime.now()
    cutoff_date = now - timedelta(days=days)
    
    # D·ªçn d·∫πp output files
    logger.info(f"D·ªçn d·∫πp files ƒë·∫ßu ra c≈© h∆°n {days} ng√†y...")
    cleaned_count = 0
    
    if os.path.exists(OUTPUT_DIR):
        for file_name in os.listdir(OUTPUT_DIR):
            file_path = os.path.join(OUTPUT_DIR, file_name)
            if os.path.isfile(file_path):
                # L·∫•y th·ªùi gian s·ª≠a ƒë·ªïi
                mtime = os.path.getmtime(file_path)
                file_date = datetime.fromtimestamp(mtime)
                
                # N·∫øu file c≈© h∆°n s·ªë ng√†y quy ƒë·ªãnh, x√≥a n√≥
                if file_date < cutoff_date:
                    try:
                        os.remove(file_path)
                        cleaned_count += 1
                        logger.debug(f"ƒê√£ x√≥a file c≈©: {file_path}")
                    except Exception as e:
                        logger.error(f"Kh√¥ng th·ªÉ x√≥a file {file_path}: {e}")
    
    # D·ªçn d·∫πp log files
    log_files = [f for f in os.listdir(SCRAPER_DIR) if f.startswith("scraper_") and f.endswith(".log")]
    for log_file in log_files:
        # Tr√≠ch xu·∫•t ng√†y t·ª´ t√™n file (ƒë·ªãnh d·∫°ng scraper_YYYYMMDD.log)
        try:
            date_str = log_file.replace("scraper_", "").replace(".log", "")
            file_date = datetime.strptime(date_str, "%Y%m%d")
            
            # N·∫øu file c≈© h∆°n s·ªë ng√†y quy ƒë·ªãnh, x√≥a n√≥
            if file_date < cutoff_date:
                file_path = os.path.join(SCRAPER_DIR, log_file)
                try:
                    os.remove(file_path)
                    cleaned_count += 1
                    logger.debug(f"ƒê√£ x√≥a log file c≈©: {log_file}")
                except Exception as e:
                    logger.error(f"Kh√¥ng th·ªÉ x√≥a file {file_path}: {e}")
        except (ValueError, IndexError):
            continue
    
    logger.info(f"[OK] ƒê√£ d·ªçn d·∫πp {cleaned_count} files c≈©")

def main():
    """
    H√†m ch√≠nh ƒëi·ªÅu ph·ªëi quy tr√¨nh scraping
    """
    parser = argparse.ArgumentParser(description="Scraper b√†i vi·∫øt tin t·ª©c")
    parser.add_argument("--skip-search", action="store_true", help="B·ªè qua b∆∞·ªõc t√¨m ki·∫øm b√†i vi·∫øt")
    parser.add_argument("--skip-extraction", action="store_true", help="B·ªè qua b∆∞·ªõc tr√≠ch xu·∫•t n·ªôi dung")
    parser.add_argument("--skip-send", action="store_true", help="B·ªè qua b∆∞·ªõc g·ª≠i ƒë·∫øn backend")
    parser.add_argument("--input-file", type=str, help="File JSON ch·ª©a b√†i vi·∫øt ƒë·ªÉ x·ª≠ l√Ω")
    parser.add_argument("--auto-send", action="store_true", help="T·ª± ƒë·ªông g·ª≠i b√†i vi·∫øt kh√¥ng c·∫ßn x√°c nh·∫≠n")
    parser.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE, help="S·ªë l∆∞·ª£ng b√†i vi·∫øt g·ª≠i trong m·ªói request")
    parser.add_argument("--verbose", action="store_true", help="Hi·ªÉn th·ªã nhi·ªÅu th√¥ng tin h∆°n")
    parser.add_argument("--retention-days", type=int, default=RETENTION_DAYS, help="S·ªë ng√†y gi·ªØ l·∫°i files tr∆∞·ªõc khi x√≥a")
    parser.add_argument("--no-cleanup", action="store_true", help="Kh√¥ng x√≥a files c≈©")
    
    args = parser.parse_args()
    
    # Thi·∫øt l·∫≠p m·ª©c ƒë·ªô chi ti·∫øt logging
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    
    # T·∫°o th∆∞ m·ª•c l√†m vi·ªác n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(TEMP_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # D·ªçn d·∫πp files c≈© n·∫øu kh√¥ng c√≥ c·ªù --no-cleanup
    if not args.no_cleanup:
        cleanup_old_files(args.retention_days)
    
    articles = []
    
    try:
        # B∆Ø·ªöC 1: T√¨m ki·∫øm b√†i vi·∫øt t·ª´ danh m·ª•c
        if not args.skip_search and not args.input_file:
            logger.info("[STEP 1] T√¨m ki·∫øm b√†i vi·∫øt t·ª´ danh m·ª•c")
            
            # L·∫•y danh s√°ch danh m·ª•c t·ª´ backend
            categories = get_categories()
            
            # T√¨m ki·∫øm b√†i vi·∫øt cho m·ªói danh m·ª•c
            for category in categories:
                category_articles = fetch_articles_by_category(category)
                articles.extend(category_articles)
            
            logger.info(f"[OK] ƒê√£ t√¨m th·∫•y t·ªïng c·ªông {len(articles)} b√†i vi·∫øt")
            
            # L∆∞u tr·ªØ k·∫øt qu·∫£ t√¨m ki·∫øm
            if articles:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = os.path.join(OUTPUT_DIR, f"search_results_{timestamp}.json")
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(articles, f, ensure_ascii=False, indent=4)
                logger.info(f"[OK] ƒê√£ l∆∞u k·∫øt qu·∫£ t√¨m ki·∫øm v√†o {output_file}")
        
        # N·∫øu c√≥ input_file, ƒë·ªçc t·ª´ file
        elif args.input_file:
            try:
                with open(args.input_file, "r", encoding="utf-8") as f:
                    articles = json.load(f)
                logger.info(f"[OK] ƒê√£ ƒë·ªçc {len(articles)} b√†i vi·∫øt t·ª´ {args.input_file}")
            except Exception as e:
                logger.error(f"[ERROR] L·ªói khi ƒë·ªçc file {args.input_file}: {str(e)}")
                return
        
        # B∆Ø·ªöC 2: Tr√≠ch xu·∫•t n·ªôi dung b√†i vi·∫øt
        if not args.skip_extraction and articles:
            # Ki·ªÉm tra n·∫øu b√†i vi·∫øt ch∆∞a c√≥ n·ªôi dung
            articles_without_content = [a for a in articles if not a.get("content")]
            
            if articles_without_content:
                logger.info(f"[STEP 2] Tr√≠ch xu·∫•t n·ªôi dung cho {len(articles_without_content)} b√†i vi·∫øt")
                enriched_articles = extract_content(articles_without_content)
                
                # C·∫≠p nh·∫≠t danh s√°ch b√†i vi·∫øt g·ªëc v·ªõi n·ªôi dung ƒë√£ tr√≠ch xu·∫•t
                articles_with_content = [a for a in articles if a.get("content")]
                articles = articles_with_content + enriched_articles
                
                # L∆∞u b√†i vi·∫øt ƒë√£ ƒë∆∞·ª£c l√†m gi√†u
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                enriched_output_file = os.path.join(OUTPUT_DIR, f"enriched_articles_{timestamp}.json")
                save_articles_to_file(articles, enriched_output_file)
            else:
                logger.info("[STEP 2] T·∫•t c·∫£ b√†i vi·∫øt ƒë√£ c√≥ n·ªôi dung, b·ªè qua b∆∞·ªõc tr√≠ch xu·∫•t")
        
        # B∆Ø·ªöC 3: G·ª≠i b√†i vi·∫øt ƒë·∫øn backend
        if not args.skip_send and articles:
            logger.info(f"[STEP 3] G·ª≠i {len(articles)} b√†i vi·∫øt t·ªõi backend")
            
            # √Åp d·ª•ng t·∫≠p tin enrich_json.py ƒë·ªÉ chu·∫©n h√≥a d·ªØ li·ªáu
            import enrich_json
            
            # L∆∞u d·ªØ li·ªáu v√†o t·ªáp t·∫°m th·ªùi
            temp_file = os.path.join(TEMP_DIR, f"temp_articles_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            with open(temp_file, "w", encoding="utf-8") as f:
                json.dump(articles, f, ensure_ascii=False, indent=4)
                
            # S·ª≠ d·ª•ng function t·ª´ enrich_json ƒë·ªÉ chu·∫©n h√≥a
            api_ready_file = enrich_json.fix_json_for_api(temp_file)
            
            # ƒê·ªçc d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a
            with open(api_ready_file, "r", encoding="utf-8") as f:
                api_ready_articles = json.load(f)
                
            # G·ª≠i d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a - lu√¥n t·ª± ƒë·ªông g·ª≠i
            success = send_to_backend(api_ready_articles, args.batch_size, True)
            
            if success:
                logger.info("[OK] ƒê√£ ho√†n th√†nh quy tr√¨nh scraping!")
            else:
                logger.warning("[WARN] Qu√° tr√¨nh g·ª≠i b√†i vi·∫øt t·ªõi backend kh√¥ng ho√†n ch·ªânh")
                
    except KeyboardInterrupt:
        logger.info("[INFO] Qu√° tr√¨nh b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
    except Exception as e:
        logger.error(f"[ERROR] L·ªói kh√¥ng mong ƒë·ª£i: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
    finally:
        # D·ªçn d·∫πp c√°c file t·∫°m th·ªùi
        cleanup_temp_files()
        logger.info("[INFO] K·∫øt th√∫c ch∆∞∆°ng tr√¨nh")

if __name__ == "__main__":
    main() 