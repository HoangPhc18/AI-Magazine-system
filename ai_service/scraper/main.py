#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ƒê√¢y l√† module ch√≠nh cho scraper b√†i vi·∫øt tin t·ª©c
Quy tr√¨nh: 
1. T√¨m ki·∫øm URL b√†i vi·∫øt t·ª´ google_news.py 
2. Tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL b·∫±ng scrape_articles_selenium.py
3. L∆∞u k·∫øt qu·∫£ v√†o file JSON trong th∆∞ m·ª•c output
"""

import os
import sys
import json
import uuid
import time
import shutil
import logging
import argparse
import requests
from pathlib import Path
from datetime import datetime, timedelta
from urllib.parse import urlparse
import glob
import traceback
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import random
from dotenv import load_dotenv
import re
import mysql.connector
from mysql.connector import Error
from unidecode import unidecode

# Import c·∫•u h√¨nh t·ª´ module config
from config import get_config, reload_config

# Import c√°c module n·ªôi b·ªô
from google_news import (
    fetch_categories_from_backend,
    search_with_category,
    process_all_categories,
    save_article_to_json,
    import_article_to_backend,
    get_category_by_id
)
from scrape_articles_selenium import extract_article_content

# T·∫£i c·∫•u h√¨nh t·ª´ file .env th√¥ng qua module config
config = get_config()

# Th√¥ng tin k·∫øt n·ªëi database t·ª´ c·∫•u h√¨nh
DB_HOST = config['DB_HOST']
DB_USER = config['DB_USER']
DB_PASSWORD = config['DB_PASSWORD']
DB_NAME = config['DB_NAME']
DB_PORT = config['DB_PORT']

# Config k·∫øt n·ªëi database
DB_CONFIG = {
    "host": DB_HOST,
    "user": DB_USER,
    "password": DB_PASSWORD,
    "database": DB_NAME,
    "port": DB_PORT
}

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(f"scraper_{datetime.now().strftime('%Y%m%d')}.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# üîπ C√°c th√¥ng s·ªë m·∫∑c ƒë·ªãnh t·ª´ c·∫•u h√¨nh
SCRAPER_DIR = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.path.join(SCRAPER_DIR, "temp")
OUTPUT_DIR = os.path.join(SCRAPER_DIR, "output")
DEFAULT_BATCH_SIZE = config.get("DEFAULT_BATCH_SIZE", 5)
# S·ªë ng√†y ƒë·ªÉ gi·ªØ l·∫°i log files v√† output files
RETENTION_DAYS = config.get("RETENTION_DAYS", 7)

# üîπ Laravel Backend API URLs t·ª´ c·∫•u h√¨nh
BACKEND_URL = config['BACKEND_URL']
BACKEND_PORT = config['BACKEND_PORT']
BASE_API_URL = config['BASE_API_URL']
CATEGORIES_API_URL = config['CATEGORIES_API_URL']
ARTICLES_API_URL = config['ARTICLES_API_URL']
ARTICLES_BATCH_API_URL = config['ARTICLES_BATCH_API_URL']
ARTICLES_IMPORT_API_URL = config['ARTICLES_IMPORT_API_URL']
ARTICLES_CHECK_API_URL = config['ARTICLES_CHECK_API_URL']

# C√°c bi·∫øn to√†n c·ª•c
# Cache l∆∞u c√°c URL ƒë√£ x·ª≠ l√Ω ƒë·ªÉ tr√°nh tr√πng l·∫∑p
processed_urls = set()

def check_api_keys():
    """
    Ki·ªÉm tra c√°c API keys t·ª´ file .env
    """
    config = get_config()  # T·∫£i l·∫°i c·∫•u h√¨nh ƒë·ªÉ c√≥ th√¥ng tin m·ªõi nh·∫•t
    api_keys_status = {}
    
    # Ki·ªÉm tra WorldNewsAPI
    worldnews_api_key = config.get('WORLDNEWS_API_KEY', '')
    if worldnews_api_key:
        api_keys_status['WorldNewsAPI'] = True
    else:
        api_keys_status['WorldNewsAPI'] = False
        
    # Ki·ªÉm tra Currents API
    currents_api_key = config.get('CURRENTS_API_KEY', '')
    if currents_api_key:
        api_keys_status['CurrentsAPI'] = True
    else:
        api_keys_status['CurrentsAPI'] = False
    
    # In th√¥ng tin
    logger.info("=== Tr·∫°ng th√°i API keys ===")
    for api_name, status in api_keys_status.items():
        if status:
            logger.info(f"‚úÖ {api_name}: OK")
        else:
            logger.warning(f"‚ö†Ô∏è {api_name}: Kh√¥ng t√¨m th·∫•y API key")
    
    return api_keys_status

def save_articles_to_file(articles, output_file=None):
    """
    L∆∞u danh s√°ch b√†i vi·∫øt v√†o file JSON
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt
        output_file (str): ƒê∆∞·ªùng d·∫´n file ƒë·∫ßu ra
        
    Returns:
        str: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ƒë√£ l∆∞u
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ l∆∞u!")
        return None
    
    # T·∫°o th∆∞ m·ª•c ƒë·∫ßu ra n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # T·∫°o t√™n file m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
    if not output_file:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(OUTPUT_DIR, f"enriched_articles_{timestamp}.json")
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u
    normalized_articles = []
    for article in articles:
        normalized = article.copy()
        
        # ƒê·∫£m b·∫£o source_name l√† chu·ªói
        if isinstance(normalized.get("source_name"), dict):
            normalized["source_name"] = normalized["source_name"].get("name", "Unknown Source")
        
        # ƒê·∫£m b·∫£o meta_data l√† chu·ªói JSON
        if isinstance(normalized.get("meta_data"), dict):
            normalized["meta_data"] = json.dumps(normalized["meta_data"])
        
        # ƒê·∫£m b·∫£o summary kh√¥ng bao gi·ªù l√† None
        if normalized.get("summary") is None:
            normalized["summary"] = ""
        
        # ƒê·∫£m b·∫£o content kh√¥ng bao gi·ªù l√† None
        if normalized.get("content") is None:
            normalized["content"] = ""
        
        # ƒê·∫£m b·∫£o published_at c√≥ ƒë·ªãnh d·∫°ng chu·∫©n
        if normalized.get("published_at"):
            try:
                # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y th√°ng
                date_obj = datetime.fromisoformat(normalized["published_at"].replace('Z', '+00:00'))
                normalized["published_at"] = date_obj.isoformat()
            except (ValueError, TypeError):
                # N·∫øu kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        
        normalized_articles.append(normalized)
    
    # L∆∞u b√†i vi·∫øt v√†o file JSON
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(normalized_articles, f, ensure_ascii=False, indent=4)
    
    logger.info(f"[OK] ƒê√£ l∆∞u {len(normalized_articles)} b√†i vi·∫øt v√†o {output_file}")
    return output_file

def send_to_backend(articles, batch_size=DEFAULT_BATCH_SIZE, auto_send=True):
    """
    G·ª≠i b√†i vi·∫øt t·ªõi backend API
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt
        batch_size (int): S·ªë l∆∞·ª£ng b√†i vi·∫øt g·ª≠i trong m·ªói request
        auto_send (bool): T·ª± ƒë·ªông g·ª≠i kh√¥ng c·∫ßn x√°c nh·∫≠n (lu√¥n True)
        
    Returns:
        bool: Tr·∫°ng th√°i th√†nh c√¥ng
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ g·ª≠i!")
        return False
    
    # Ghi log t·ªïng s·ªë b√†i vi·∫øt c·∫ßn g·ª≠i
    logger.info(f"Chu·∫©n b·ªã g·ª≠i {len(articles)} b√†i vi·∫øt t·ªõi backend")
    
    # Debug: Ki·ªÉm tra API URL
    api_url = ARTICLES_IMPORT_API_URL
    logger.info(f"API URL: {api_url}")
    logger.debug(f"BASE_API_URL: {get_config('BASE_API_URL')}")
    logger.debug(f"BACKEND_URL: {get_config('BACKEND_URL')}")
    logger.debug(f"BACKEND_PORT: {get_config('BACKEND_PORT')}")
    
    # Debug ti√™u ƒë·ªÅ v√† URL c·ªßa b√†i vi·∫øt ƒë·∫ßu ti√™n
    if len(articles) > 0:
        logger.info(f"B√†i vi·∫øt ƒë·∫ßu ti√™n: {articles[0].get('title', 'N/A')}")
        logger.info(f"Source URL: {articles[0].get('source_url', 'N/A')}")
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi g·ª≠i
    normalized_articles = []
    for article in articles:
        normalized = article.copy()
        
        # ƒê·∫£m b·∫£o source_name l√† chu·ªói
        if isinstance(normalized.get("source_name"), dict):
            normalized["source_name"] = normalized["source_name"].get("name", "Unknown Source")
        
        # ƒê·∫£m b·∫£o meta_data l√† chu·ªói JSON
        if isinstance(normalized.get("meta_data"), dict):
            try:
                normalized["meta_data"] = json.dumps(normalized["meta_data"], ensure_ascii=False)
            except Exception as e:
                logger.warning(f"L·ªói khi chuy·ªÉn ƒë·ªïi meta_data sang JSON: {str(e)}")
                normalized["meta_data"] = json.dumps({"error": "Invalid metadata"})
        
        # ƒê·∫£m b·∫£o summary kh√¥ng bao gi·ªù l√† None
        if normalized.get("summary") is None:
            normalized["summary"] = ""
        
        # ƒê·∫£m b·∫£o content kh√¥ng bao gi·ªù l√† None
        if normalized.get("content") is None:
            normalized["content"] = ""
        
        # ƒê·∫£m b·∫£o published_at c√≥ ƒë·ªãnh d·∫°ng chu·∫©n
        if normalized.get("published_at"):
            try:
                # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y th√°ng
                date_obj = datetime.fromisoformat(normalized["published_at"].replace('Z', '+00:00'))
                normalized["published_at"] = date_obj.isoformat()
            except (ValueError, TypeError):
                # N·∫øu kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        else:
            # N·∫øu kh√¥ng c√≥ published_at, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
                
        normalized_articles.append(normalized)
    
    # Chia th√†nh c√°c batch ƒë·ªÉ g·ª≠i
    batches = [normalized_articles[i:i + batch_size] for i in range(0, len(normalized_articles), batch_size)]
    logger.info(f"G·ª≠i {len(normalized_articles)} b√†i vi·∫øt t·ªõi backend trong {len(batches)} batches...")
    
    total_success = True
    total_imported = 0
    total_skipped = 0
    
    for i, batch in enumerate(batches, 1):
        logger.info(f"G·ª≠i batch {i}/{len(batches)} ({len(batch)} b√†i vi·∫øt)")
        
        # In ra ti√™u ƒë·ªÅ c·ªßa b√†i ƒë·∫ßu ti√™n trong batch ƒë·ªÉ debug
        if len(batch) > 0:
            logger.debug(f"B√†i vi·∫øt ƒë·∫ßu ti√™n trong batch: {batch[0].get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')}")
        
        # Chu·∫©n b·ªã payload theo ƒë√∫ng ƒë·ªãnh d·∫°ng API y√™u c·∫ßu
        payload = {
            "articles": batch
        }
        
        # Thi·∫øt l·∫≠p header
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        
        # G·ª≠i request, v·ªõi c∆° ch·∫ø th·ª≠ l·∫°i n·∫øu l·ªói
        max_retries = 3
        retry_count = 0
        retry_delay = 2  # Gi√¢y
        
        while retry_count < max_retries:
            try:
                # S·ª≠ d·ª•ng endpoint import t·ª´ c·∫•u h√¨nh
                api_url = ARTICLES_IMPORT_API_URL
                logger.info(f"G·ª≠i d·ªØ li·ªáu ƒë·∫øn API: {api_url}")
                
                # Log payload size
                payload_size = len(json.dumps(payload))
                logger.debug(f"K√≠ch th∆∞·ªõc payload: {payload_size} bytes")
                
                response = requests.post(
                    api_url,
                    json=payload,
                    headers=headers,
                    timeout=30
                )
                
                # In ra response status v√† ph·∫ßn ƒë·∫ßu c·ªßa response body ƒë·ªÉ debug
                logger.info(f"API Response Status: {response.status_code}")
                response_text = response.text[:200] + "..." if len(response.text) > 200 else response.text
                logger.info(f"API Response Text: {response_text}")
                
                if response.status_code == 200 or response.status_code == 201:
                    logger.info(f"[OK] Batch {i}: G·ª≠i th√†nh c√¥ng")
                    # Ph√¢n t√≠ch ph·∫£n h·ªìi ƒë·ªÉ t√≠nh s·ªë b√†i vi·∫øt ƒë√£ import
                    try:
                        result = response.json()
                        success = result.get('success', 0)
                        skipped = result.get('skipped', 0)
                        total_imported += int(success) if isinstance(success, (int, str)) else 0
                        total_skipped += int(skipped) if isinstance(skipped, (int, str)) else 0
                        logger.info(f"[INFO] Batch {i}: {success} b√†i vi·∫øt ƒë√£ import, {skipped} b√†i vi·∫øt b·ªã b·ªè qua")
                    except Exception as e:
                        logger.warning(f"Kh√¥ng th·ªÉ ph√¢n t√≠ch JSON response: {str(e)}")
                    break
                else:
                    error_msg = response.text[:100] + "..." if len(response.text) > 100 else response.text
                    logger.error(f"[ERROR] Batch {i}: L·ªói {response.status_code} - {error_msg}")
                    
                    # Th·ª≠ l·∫°i sau th·ªùi gian delay
                    retry_count += 1
                    if retry_count < max_retries:
                        logger.info(f"Th·ª≠ l·∫°i sau {retry_delay} gi√¢y...")
                        time.sleep(retry_delay)
                        # TƒÉng th·ªùi gian delay cho l·∫ßn th·ª≠ ti·∫øp theo
                        retry_delay *= 2
                    else:
                        logger.error(f"[ERROR] ƒê√£ th·ª≠ l·∫°i {max_retries} l·∫ßn kh√¥ng th√†nh c√¥ng cho batch {i}")
                        total_success = False
                        
            except Exception as e:
                logger.error(f"[ERROR] Batch {i}: Exception - {str(e)}")
                # Log stack trace
                import traceback
                logger.error(f"Stack trace: {traceback.format_exc()}")
                
                retry_count += 1
                if retry_count < max_retries:
                    logger.info(f"Th·ª≠ l·∫°i sau {retry_delay} gi√¢y...")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    logger.error(f"[ERROR] ƒê√£ th·ª≠ l·∫°i {max_retries} l·∫ßn kh√¥ng th√†nh c√¥ng cho batch {i}")
                    total_success = False
    
    if total_success:
        logger.info(f"[OK] T·ªïng c·ªông: {total_imported} b√†i vi·∫øt ƒë√£ ƒë∆∞·ª£c import th√†nh c√¥ng, {total_skipped} b√†i vi·∫øt b·ªã b·ªè qua")
    else:
        logger.warning(f"[WARN] C√≥ l·ªói x·∫£y ra khi g·ª≠i b√†i vi·∫øt t·ªõi backend. ƒê√£ import: {total_imported}, ƒê√£ b·ªè qua: {total_skipped}")
        
    return total_success

def cleanup_temp_files():
    """
    D·ªçn d·∫πp c√°c file t·∫°m th·ªùi
    """
    if os.path.exists(TEMP_DIR):
        try:
            shutil.rmtree(TEMP_DIR)
            logger.info(f"[OK] ƒê√£ d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {TEMP_DIR}")
        except Exception as e:
            logger.error(f"[ERROR] L·ªói khi d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {str(e)}")

def cleanup_old_files(retention_days=RETENTION_DAYS):
    """
    D·ªçn d·∫πp c√°c file c≈© trong th∆∞ m·ª•c output
    
    Args:
        retention_days (int): S·ªë ng√†y gi·ªØ l·∫°i files (m·∫∑c ƒë·ªãnh: 2)
    """
    if not os.path.exists(OUTPUT_DIR):
        return
    
    cutoff_date = time.time() - (retention_days * 24 * 60 * 60)
    logger.info(f"D·ªçn d·∫πp c√°c file c≈© h∆°n {retention_days} ng√†y trong th∆∞ m·ª•c {OUTPUT_DIR}")
    
    count = 0
    for filename in os.listdir(OUTPUT_DIR):
        filepath = os.path.join(OUTPUT_DIR, filename)
        if os.path.isfile(filepath):
            file_time = os.path.getmtime(filepath)
            if file_time < cutoff_date:
                try:
                    os.remove(filepath)
                    count += 1
                    logger.debug(f"ƒê√£ x√≥a file c≈©: {filename}")
                except Exception as e:
                    logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {filename}: {str(e)}")
    
    if count > 0:
        logger.info(f"ƒê√£ x√≥a {count} file c≈©")
    else:
        logger.info("Kh√¥ng c√≥ file n√†o c·∫ßn x√≥a")

def process_category(category_id, category_name, max_articles=2):
    """
    X·ª≠ l√Ω m·ªôt danh m·ª•c c·ª• th·ªÉ v√† tr√≠ch xu·∫•t b√†i vi·∫øt
    
    Args:
        category_id: ID c·ªßa danh m·ª•c
        category_name: T√™n c·ªßa danh m·ª•c
        max_articles: S·ªë b√†i vi·∫øt t·ªëi ƒëa c·∫ßn l·∫•y
        
    Returns:
        dict: Danh s√°ch c√°c b√†i vi·∫øt ƒë√£ t√¨m th·∫•y cho danh m·ª•c
    """
    articles = []
    
    for i in range(1, max_articles + 1):
        logger.info(f"T√¨m ki·∫øm b√†i vi·∫øt th·ª© {i}/{max_articles} cho danh m·ª•c: {category_name}")
        
        try:
            article_data = search_with_category(category_id)
            
            if article_data and "url" in article_data:
                article_url = article_data["url"]
                
                # Ki·ªÉm tra xem URL n√†y ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ch∆∞a
                if article_url in processed_urls:
                    logger.warning(f"URL ƒë√£ x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥, b·ªè qua: {article_url}")
                    continue
                
                # ƒê√°nh d·∫•u URL n√†y ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
                processed_urls.add(article_url)
                
                articles.append(article_data)
                
                logger.info(f"ƒê√£ t√¨m th·∫•y b√†i vi·∫øt: {article_data.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')}")
                logger.info(f"URL: {article_url}")
                logger.info(f"ƒê√£ l∆∞u v√†o: {article_data.get('file_path', 'Kh√¥ng l∆∞u file')}")
            else:
                logger.warning(f"Kh√¥ng t√¨m th·∫•y th√™m b√†i vi·∫øt n√†o cho danh m·ª•c: {category_name}")
                break
            
            # Th√™m kho·∫£ng th·ªùi gian ngh·ªâ gi·ªØa c√°c l·∫ßn t√¨m ki·∫øm ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
            if i < max_articles:
                time.sleep(2)
                
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω b√†i vi·∫øt th·ª© {i} cho danh m·ª•c {category_name}: {str(e)}")
            logger.error(traceback.format_exc())
    
    return articles

def find_and_process_all_categories(max_articles_per_category=2):
    """
    T√¨m ki·∫øm v√† x·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c t·ª´ backend.
    M·ªói danh m·ª•c s·∫Ω ƒë∆∞·ª£c t√¨m ki·∫øm b√†i vi·∫øt v√† l∆∞u tr·ªØ.
    
    Args:
        max_articles_per_category (int): S·ªë b√†i vi·∫øt t·ªëi ƒëa cho m·ªói danh m·ª•c
        
    Returns:
        dict: K·∫øt qu·∫£ x·ª≠ l√Ω c√°c danh m·ª•c
    """
    results = {
        'success': 0,
        'failed': 0,
        'articles_by_category': {},
        'all_articles': [],
        'total_articles': 0
    }
    
    logger.info(f"B·∫Øt ƒë·∫ßu t√¨m ki·∫øm v√† x·ª≠ l√Ω b√†i vi·∫øt cho c√°c danh m·ª•c (t·ªëi ƒëa {max_articles_per_category} b√†i/danh m·ª•c)")
    
    # L·∫•y danh s√°ch c√°c danh m·ª•c t·ª´ backend
    categories = fetch_categories_from_backend()
    
    if not categories:
        # Backup plan when backend is not available - use hardcoded categories
        logger.warning("Kh√¥ng th·ªÉ l·∫•y danh m·ª•c t·ª´ backend, s·ª≠ d·ª•ng danh m·ª•c m·∫´u...")
        categories = [
            {"id": 1, "name": "Th·ªùi s·ª±", "slug": "thoi-su"},
            {"id": 2, "name": "Th·∫ø gi·ªõi", "slug": "the-gioi"},
            {"id": 3, "name": "Kinh doanh", "slug": "kinh-doanh"},
            {"id": 4, "name": "Gi·∫£i tr√≠", "slug": "giai-tri"},
            {"id": 5, "name": "Th·ªÉ thao", "slug": "the-thao"},
            {"id": 6, "name": "Ph√°p lu·∫≠t", "slug": "phap-luat"},
            {"id": 7, "name": "Gi√°o d·ª•c", "slug": "giao-duc"},
            {"id": 8, "name": "S·ª©c kh·ªèe", "slug": "suc-khoe"},
            {"id": 9, "name": "ƒê·ªùi s·ªëng", "slug": "doi-song"},
            {"id": 10, "name": "Du l·ªãch", "slug": "du-lich"}
        ]
        logger.info(f"S·ª≠ d·ª•ng {len(categories)} danh m·ª•c m·∫´u")
        
    # X·ª≠ l√Ω t·ª´ng danh m·ª•c
    total_categories = len(categories)
    logger.info(f"ƒê√£ t√¨m th·∫•y {total_categories} danh m·ª•c ƒë·ªÉ x·ª≠ l√Ω")
    
    # X·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c theo gi·ªõi h·∫°n
    categories_processed = 0
    
    for category in categories:
        if categories_processed >= total_categories:
            break
            
        category_id = category.get('id')
        category_name = category.get('name')
        
        if not category_id or not category_name:
            logger.warning(f"Danh m·ª•c kh√¥ng h·ª£p l·ªá, thi·∫øu ID ho·∫∑c t√™n: {category}")
            results['failed'] += 1
            continue
        
        logger.info(f"X·ª≠ l√Ω danh m·ª•c: ID={category_id}, T√™n={category_name}")
        
        try:
            # T√¨m ki·∫øm b√†i vi·∫øt cho danh m·ª•c n√†y
            articles = process_category(category_id, category_name, max_articles_per_category)
            
            if not articles or len(articles) == 0:
                logger.warning(f"Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o cho danh m·ª•c: {category_name}")
                results['failed'] += 1
                continue
            
            # Th√™m b√†i vi·∫øt v√†o k·∫øt qu·∫£
            results['articles_by_category'][category_id] = articles
            results['all_articles'].extend(articles)
            results['success'] += 1
            results['total_articles'] += len(articles)
            
            logger.info(f"ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng danh m·ª•c: {category_name}, s·ªë b√†i vi·∫øt t√¨m ƒë∆∞·ª£c: {len(articles)}")
            
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω danh m·ª•c {category_name}: {str(e)}")
            results['failed'] += 1
            
        categories_processed += 1
    
    # T·ªïng k·∫øt
    logger.info(f"\nƒê√£ x·ª≠ l√Ω {categories_processed} danh m·ª•c")
    logger.info(f"Th√†nh c√¥ng: {results['success']} danh m·ª•c, Th·∫•t b·∫°i: {results['failed']} danh m·ª•c")
    logger.info(f"T·ªïng s·ªë b√†i vi·∫øt ƒë√£ t√¨m ƒë∆∞·ª£c: {results['total_articles']}")
    
    # T·ª± ƒë·ªông import c√°c b√†i vi·∫øt v√†o database
    if results['all_articles'] and len(results['all_articles']) > 0:
        logger.info(f"T·ª± ƒë·ªông import {len(results['all_articles'])} b√†i vi·∫øt v√†o database")
        send_to_backend(results['all_articles'])
    
    return results

def import_all_to_backend(directory=None):
    """
    Import t·∫•t c·∫£ c√°c file JSON t·ª´ th∆∞ m·ª•c output v√†o backend
    
    Args:
        directory (str): Th∆∞ m·ª•c ch·ª©a c√°c file JSON (m·∫∑c ƒë·ªãnh: OUTPUT_DIR)
        
    Returns:
        tuple: (success_count, failed_count) - S·ªë l∆∞·ª£ng file th√†nh c√¥ng v√† th·∫•t b·∫°i
    """
    if directory is None:
        directory = OUTPUT_DIR
    
    if not os.path.exists(directory):
        logger.error(f"Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {directory}")
        return 0, 0
    
    logger.info(f"B·∫Øt ƒë·∫ßu import t·∫•t c·∫£ c√°c file JSON t·ª´ th∆∞ m·ª•c: {directory}")
    
    # T√¨m t·∫•t c·∫£ c√°c file JSON trong th∆∞ m·ª•c
    json_files = [f for f in os.listdir(directory) if f.endswith('.json')]
    
    if not json_files:
        logger.warning(f"Kh√¥ng t√¨m th·∫•y file JSON n√†o trong th∆∞ m·ª•c: {directory}")
        return 0, 0
    
    logger.info(f"T√¨m th·∫•y {len(json_files)} file JSON")
    
    success_count = 0
    failed_count = 0
    
    for json_file in json_files:
        filepath = os.path.join(directory, json_file)
        logger.info(f"ƒêang x·ª≠ l√Ω file: {json_file}")
        
        try:
            # ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Ki·ªÉm tra xem d·ªØ li·ªáu c√≥ ph·∫£i l√† dictionary v·ªõi c√°c tr∆∞·ªùng c·∫ßn thi·∫øt kh√¥ng
            if isinstance(data, dict) and 'category_id' in data and 'title' in data and 'content' in data:
                # Import b√†i vi·∫øt v√†o backend
                result = import_article_to_backend(
                    category_id=data['category_id'],
                    article_url=data.get('url', ''),
                    title=data['title'],
                    content=data['content']
                )
                
                if result:
                    logger.info(f"Import th√†nh c√¥ng file: {json_file}")
                    success_count += 1
                    # X√≥a file JSON sau khi import th√†nh c√¥ng
                    try:
                        os.remove(filepath)
                        logger.info(f"ƒê√£ x√≥a file JSON sau khi import th√†nh c√¥ng: {json_file}")
                    except Exception as e:
                        logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {json_file}: {str(e)}")
                else:
                    logger.error(f"Import th·∫•t b·∫°i file: {json_file}")
                    failed_count += 1
            else:
                logger.warning(f"File kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng: {json_file}")
                failed_count += 1
        
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω file {json_file}: {str(e)}")
            failed_count += 1
    
    logger.info(f"K·∫øt qu·∫£ import: Th√†nh c√¥ng: {success_count}, Th·∫•t b·∫°i: {failed_count}")
    return success_count, failed_count

def save_all_articles_to_single_file(results):
    """
    L∆∞u t·∫•t c·∫£ b√†i vi·∫øt t·ª´ t·∫•t c·∫£ c√°c danh m·ª•c v√†o m·ªôt file JSON duy nh·∫•t
    :param results: K·∫øt qu·∫£ t·ª´ h√†m find_and_process_all_categories
    :return: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON ƒë√£ l∆∞u
    """
    logger.info(f"B·∫Øt ƒë·∫ßu l∆∞u t·∫•t c·∫£ b√†i vi·∫øt v√†o file JSON duy nh·∫•t, d·ªØ li·ªáu ƒë·∫ßu v√†o: {type(results)}")
    
    # Ki·ªÉm tra d·ªØ li·ªáu ƒë·∫ßu v√†o
    if not isinstance(results, dict):
        logger.error(f"D·ªØ li·ªáu ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá: {type(results)}")
        return None
    
    # Log c·∫•u tr√∫c c·ªßa results ƒë·ªÉ debug
    logger.info(f"C·∫•u tr√∫c results: {list(results.keys())}")
    
    # ∆Øu ti√™n s·ª≠ d·ª•ng all_articles n·∫øu c√≥
    all_articles = []
    
    if "all_articles" in results and isinstance(results["all_articles"], list) and results["all_articles"]:
        all_articles_raw = results["all_articles"]
        logger.info(f"S·ª≠ d·ª•ng {len(all_articles_raw)} b√†i vi·∫øt t·ª´ all_articles")
        
        # Thu th·∫≠p b√†i vi·∫øt v·ªõi n·ªôi dung ƒë·∫ßy ƒë·ªß
        for article in all_articles_raw:
            if not isinstance(article, dict):
                logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng h·ª£p l·ªá: {type(article)}")
                continue
                
            # ƒê·ªçc n·ªôi dung t·ª´ file JSON n·∫øu b√†i vi·∫øt kh√¥ng c√≥ tr∆∞·ªùng 'content'
            article_copy = article.copy()
            if "content" not in article_copy and "json_filepath" in article_copy:
                try:
                    json_content = read_json_file(article_copy["json_filepath"])
                    if json_content and "content" in json_content:
                        # C·∫≠p nh·∫≠t n·ªôi dung t·ª´ file JSON
                        article_copy.update(json_content)
                        logger.info(f"ƒê√£ l·∫•y n·ªôi dung t·ª´ file: {article_copy['json_filepath']}")
                except Exception as e:
                    logger.error(f"L·ªói khi ƒë·ªçc file {article_copy['json_filepath']}: {str(e)}")
            
            # Th√™m v√†o danh s√°ch n·∫øu c√≥ n·ªôi dung
            if article_copy.get("content"):
                all_articles.append(article_copy)
            else:
                logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng c√≥ n·ªôi dung: {article_copy.get('title', 'Kh√¥ng ti√™u ƒë·ªÅ')}")
    else:
        # Thu th·∫≠p b√†i vi·∫øt t·ª´ c·∫•u tr√∫c articles_by_category
        if "articles_by_category" in results and isinstance(results["articles_by_category"], dict):
            articles_by_category = results["articles_by_category"]
            
            # Thu th·∫≠p b√†i vi·∫øt t·ª´ m·ªói danh m·ª•c
            for category_name, articles in articles_by_category.items():
                if not isinstance(articles, list):
                    logger.warning(f"B·ªè qua danh m·ª•c {category_name} v√¨ d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá: {type(articles)}")
                    continue
                
                logger.info(f"X·ª≠ l√Ω {len(articles)} b√†i vi·∫øt t·ª´ danh m·ª•c {category_name}")
                
                for article in articles:
                    if not isinstance(article, dict):
                        logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng h·ª£p l·ªá: {type(article)}")
                        continue
                    
                    # T·∫°o b·∫£n sao v√† th√™m th√¥ng tin danh m·ª•c
                    article_copy = article.copy()
                    article_copy["category_name"] = category_name
                    
                    # ƒê·ªçc n·ªôi dung t·ª´ file JSON n·∫øu b√†i vi·∫øt kh√¥ng c√≥ tr∆∞·ªùng 'content'
                    if "content" not in article_copy and "json_filepath" in article_copy:
                        try:
                            json_content = read_json_file(article_copy["json_filepath"])
                            if json_content and "content" in json_content:
                                # C·∫≠p nh·∫≠t n·ªôi dung t·ª´ file JSON
                                article_copy.update(json_content)
                                logger.info(f"ƒê√£ l·∫•y n·ªôi dung t·ª´ file: {article_copy['json_filepath']}")
                        except Exception as e:
                            logger.error(f"L·ªói khi ƒë·ªçc file {article_copy['json_filepath']}: {str(e)}")
                    
                    # Th√™m v√†o danh s√°ch n·∫øu c√≥ n·ªôi dung
                    if article_copy.get("content"):
                        all_articles.append(article_copy)
                    else:
                        logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng c√≥ n·ªôi dung: {article_copy.get('title', 'Kh√¥ng ti√™u ƒë·ªÅ')}")
    
    # Ki·ªÉm tra s·ªë l∆∞·ª£ng b√†i vi·∫øt sau khi thu th·∫≠p
    if not all_articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt h·ª£p l·ªá ƒë·ªÉ l∆∞u v√†o file JSON duy nh·∫•t")
        return None
    
    # T·∫°o t√™n file duy nh·∫•t
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    unique_id = str(uuid.uuid4())[:8]  # L·∫•y 8 k√Ω t·ª± ƒë·∫ßu c·ªßa UUID
    
    # Ki·ªÉm tra xem ƒë∆∞·ªùng d·∫´n /app/output c√≥ t·ªìn t·∫°i kh√¥ng (Docker environment)
    docker_output = "/app/output"
    if os.path.exists(docker_output) and os.path.isdir(docker_output):
        output_file = os.path.join(docker_output, f"all_articles_{current_time}_{unique_id}.json")
        # B·∫£o ƒë·∫£m c≈©ng t·∫°o b·∫£n sao trong OUTPUT_DIR ƒë·ªÉ import_json_file_to_backend c√≥ th·ªÉ t√¨m th·∫•y
        local_output_file = os.path.join(OUTPUT_DIR, f"all_articles_{current_time}_{unique_id}.json")
    else:
        output_file = os.path.join(OUTPUT_DIR, f"all_articles_{current_time}_{unique_id}.json")
        local_output_file = output_file
    
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c output t·ªìn t·∫°i
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    if output_file != local_output_file:
        os.makedirs(os.path.dirname(local_output_file), exist_ok=True)
    
    # L∆∞u v√†o file JSON
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_articles, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ƒê√£ l∆∞u {len(all_articles)} b√†i vi·∫øt v√†o file JSON duy nh·∫•t: {output_file}")
        
        # N·∫øu ƒëang ch·∫°y trong Docker, t·∫°o b·∫£n sao ƒë·ªÉ ƒë·∫£m b·∫£o import
        if output_file != local_output_file:
            shutil.copy2(output_file, local_output_file)
            logger.info(f"ƒê√£ t·∫°o b·∫£n sao t·∫°i: {local_output_file} ƒë·ªÉ ƒë·∫£m b·∫£o import v√†o DB")
        
        # In th√¥ng tin b√†i vi·∫øt ƒë·∫ßu ti√™n ƒë·ªÉ debug
        if all_articles:
            first_article = all_articles[0]
            logger.info(f"B√†i vi·∫øt m·∫´u: Title={first_article.get('title', 'Kh√¥ng ti√™u ƒë·ªÅ')}, Content length={len(first_article.get('content', ''))}")
            
        return local_output_file  # Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n ƒë·∫øn file c√≥ th·ªÉ ƒë·ªçc ƒë∆∞·ª£c t·ª´ h√†m import
    except Exception as e:
        logger.error(f"L·ªói khi l∆∞u file JSON: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def read_json_file(file_path):
    """
    ƒê·ªçc n·ªôi dung t·ª´ file JSON
    :param file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON
    :return: D·ªØ li·ªáu t·ª´ file JSON ho·∫∑c None n·∫øu l·ªói
    """
    if not os.path.exists(file_path):
        logger.error(f"File kh√¥ng t·ªìn t·∫°i: {file_path}")
        return None
        
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        logger.error(f"L·ªói khi ƒë·ªçc file JSON: {str(e)}")
        return None

def import_json_file_to_backend(json_file_path):
    """
    Import t·∫•t c·∫£ b√†i vi·∫øt t·ª´ file JSON v√†o backend
    :param json_file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON
    :return: tuple (s·ªë b√†i vi·∫øt import th√†nh c√¥ng, s·ªë b√†i vi·∫øt th·∫•t b·∫°i)
    """
    logger.info(f"ƒêang import b√†i vi·∫øt t·ª´ file: {json_file_path}")
    
    # Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa file
    file_exists = os.path.exists(json_file_path)
    
    # Th·ª≠ t√¨m file ·ªü ƒë∆∞·ªùng d·∫´n /app/output/ n·∫øu kh√¥ng t√¨m th·∫•y ·ªü ƒë∆∞·ªùng d·∫´n g·ªëc
    if not file_exists and not json_file_path.startswith('/app/output/'):
        filename = os.path.basename(json_file_path)
        docker_path = f"/app/output/{filename}"
        if os.path.exists(docker_path):
            logger.info(f"Kh√¥ng t√¨m th·∫•y file t·∫°i {json_file_path}, nh∆∞ng t√¨m th·∫•y t·∫°i {docker_path}")
            json_file_path = docker_path
            file_exists = True
    
    # Th·ª≠ t√¨m file ·ªü OUTPUT_DIR n·∫øu kh√¥ng t√¨m th·∫•y ·ªü ƒë∆∞·ªùng d·∫´n /app/output/
    if not file_exists and json_file_path.startswith('/app/output/'):
        filename = os.path.basename(json_file_path)
        local_path = os.path.join(OUTPUT_DIR, filename)
        if os.path.exists(local_path):
            logger.info(f"Kh√¥ng t√¨m th·∫•y file t·∫°i {json_file_path}, nh∆∞ng t√¨m th·∫•y t·∫°i {local_path}")
            json_file_path = local_path
            file_exists = True
    
    if not file_exists:
        logger.error(f"File kh√¥ng t·ªìn t·∫°i: {json_file_path}")
        return 0, 0
    
    # ƒê·ªçc file JSON
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        logger.error(f"L·ªói khi ƒë·ªçc file JSON: {str(e)}")
        logger.error(traceback.format_exc())
        return 0, 0
    
    # Ki·ªÉm tra d·ªØ li·ªáu l√† list
    if not isinstance(data, list):
        logger.error(f"D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá, ph·∫£i l√† list: {type(data)}")
        return 0, 0
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ g·ª≠i l√™n API
    articles_to_import = []
    
    for article in data:
        if not isinstance(article, dict):
            logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng h·ª£p l·ªá, kh√¥ng ph·∫£i dict: {type(article)}")
            continue
            
        # ƒê·ªçc n·ªôi dung t·ª´ file l∆∞u tr·ªØ n·∫øu c·∫ßn
        article_content = article.get("content", "")
        if not article_content and "json_filepath" in article:
            json_data = read_json_file(article["json_filepath"])
            if json_data and isinstance(json_data, dict):
                article_content = json_data.get("content", "")
                # C·∫≠p nh·∫≠t th√¥ng tin kh√°c t·ª´ file JSON
                if not article.get("title") and json_data.get("title"):
                    article["title"] = json_data["title"]
                if not article.get("summary") and json_data.get("summary"):
                    article["summary"] = json_data["summary"]
                
        # Chu·∫©n b·ªã d·ªØ li·ªáu b√†i vi·∫øt
        article_data = {
            "title": article.get("title", ""),
            "slug": generate_slug(article.get("title", "Kh√¥ng ti√™u ƒë·ªÅ")),
            "summary": article.get("summary", ""),
            "content": article_content,
            "source_name": article.get("source_name", ""),
            "source_url": article.get("source_url", article.get("url", "")),
            "source_icon": article.get("source_icon", ""),
            "published_at": article.get("published_at", datetime.now().isoformat()),
            "meta_data": article.get("meta_data", {}),
            "category": article.get("category_id", 1)  # ƒê·∫£m b·∫£o s·ª≠ d·ª•ng category_id, kh√¥ng ph·∫£i category
        }
        
        # T·∫°o summary t·ª´ n·ªôi dung n·∫øu kh√¥ng c√≥
        if not article_data["summary"] and article_data["content"]:
            sentences = re.split(r'[.!?]+', article_data["content"])
            if len(sentences) >= 2:
                article_data["summary"] = '. '.join(s.strip() for s in sentences[:2] if s.strip()) + '.'
            else:
                article_data["summary"] = sentences[0] if sentences else ""
                
        # T·∫°o source_name t·ª´ URL n·∫øu kh√¥ng c√≥
        if not article_data["source_name"] and article_data["source_url"]:
            try:
                parsed_url = urlparse(article_data["source_url"])
                article_data["source_name"] = parsed_url.netloc
            except:
                article_data["source_name"] = "Kh√¥ng r√µ"
                
        # T·∫°o source_icon t·ª´ domain n·∫øu kh√¥ng c√≥
        if not article_data["source_icon"] and article_data["source_url"]:
            try:
                parsed_url = urlparse(article_data["source_url"])
                domain = parsed_url.netloc
                article_data["source_icon"] = f"https://www.google.com/s2/favicons?domain={domain}"
            except:
                article_data["source_icon"] = ""
        
        # Ki·ªÉm tra n·ªôi dung b√†i vi·∫øt
        if not article_data["content"]:
            logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng c√≥ n·ªôi dung: {article_data['title']}")
            continue
            
        articles_to_import.append(article_data)
    
    if not articles_to_import:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt h·ª£p l·ªá ƒë·ªÉ import")
        return 0, 0
    
    logger.info(f"ƒêang import {len(articles_to_import)} b√†i vi·∫øt")
    
    # In ph·∫ßn ƒë·∫ßu c·ªßa b√†i vi·∫øt ƒë·∫ßu ti√™n ƒë·ªÉ debug
    if articles_to_import:
        first_article = articles_to_import[0]
        logger.info(f"M·∫´u b√†i vi·∫øt ƒë·∫ßu ti√™n: Title={first_article['title']}, Content length={len(first_article['content'])}, Source={first_article['source_name']}")
    
    # Chu·∫©n b·ªã payload theo ƒë√∫ng ƒë·ªãnh d·∫°ng API y√™u c·∫ßu
    payload = {
        "articles": articles_to_import
    }
    
    # Th·ª≠ import v·ªõi endpoint import
    try:
        logger.info(f"Th·ª≠ import v·ªõi endpoint: {ARTICLES_IMPORT_API_URL}")
        
        # G·ªçi API import
        response = requests.post(
            ARTICLES_IMPORT_API_URL,
            json=payload,
            headers={"Content-Type": "application/json", "Accept": "application/json"}
        )
        
        logger.info(f"Ph·∫£n h·ªìi t·ª´ endpoint import: Status code: {response.status_code}")
        logger.info(f"Ph·∫£n h·ªìi n·ªôi dung: {response.text[:500]}")
        
        if response.status_code == 200:
            try:
                result = response.json()
                success = result.get("success", 0) or (len(articles_to_import) - result.get("skipped", 0))
                failed = result.get("failed", 0) or result.get("skipped", 0)
                logger.info(f"Import th√†nh c√¥ng v·ªõi endpoint import: {success} th√†nh c√¥ng, {failed} th·∫•t b·∫°i")
                
                # X√≥a file JSON sau khi import th√†nh c√¥ng n·∫øu c√≥ b√†i vi·∫øt ƒë∆∞·ª£c import
                if success > 0:
                    try:
                        os.remove(json_file_path)
                        logger.info(f"ƒê√£ x√≥a file JSON sau khi import th√†nh c√¥ng: {json_file_path}")
                    except Exception as e:
                        logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {json_file_path}: {str(e)}")
                return success, failed
            except Exception as e:
                logger.error(f"L·ªói khi x·ª≠ l√Ω ph·∫£n h·ªìi t·ª´ endpoint import: {str(e)}")
        else:
            logger.warning(f"Import v·ªõi endpoint import th·∫•t b·∫°i. Status code: {response.status_code}")
    except Exception as e:
        logger.error(f"L·ªói khi g·ªçi endpoint import: {str(e)}")
        logger.error(traceback.format_exc())
    
    # N·∫øu endpoint import th·∫•t b·∫°i, th·ª≠ import tr·ª±c ti·∫øp v√†o DB
    logger.info("Th·ª≠ import tr·ª±c ti·∫øp v√†o database...")
    success, failed = import_articles_to_database_directly(articles_to_import)
    
    # X√≥a file JSON sau khi import th√†nh c√¥ng n·∫øu c√≥ b√†i vi·∫øt ƒë∆∞·ª£c import
    if success > 0:
        try:
            os.remove(json_file_path)
            logger.info(f"ƒê√£ x√≥a file JSON sau khi import th√†nh c√¥ng: {json_file_path}")
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {json_file_path}: {str(e)}")
    
    return success, failed

def import_articles_to_database_directly(articles):
    """
    Import b√†i vi·∫øt tr·ª±c ti·∫øp v√†o database (fallback khi API th·∫•t b·∫°i)
    :param articles: Danh s√°ch b√†i vi·∫øt c·∫ßn import
    :return: tuple (s·ªë b√†i vi·∫øt import th√†nh c√¥ng, s·ªë b√†i vi·∫øt th·∫•t b·∫°i)
    """
    if not check_db_connection():
        logger.error("Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn database")
        return 0, 0
    
    logger.info(f"ƒêang import {len(articles)} b√†i vi·∫øt tr·ª±c ti·∫øp v√†o database")
    
    # K·∫øt n·ªëi ƒë·∫øn database
    conn = mysql.connector.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    success_count = 0
    failed_count = 0
    
    try:
        for article in articles:
            try:
                # Chu·∫©n b·ªã d·ªØ li·ªáu
                title = article.get('title', '')
                slug = article.get('slug', '')
                summary = article.get('summary', '')
                content = article.get('content', '')
                source_name = article.get('source_name', '')
                source_url = article.get('source_url', '')
                source_icon = article.get('source_icon', '')
                published_at = article.get('published_at', datetime.now().isoformat())
                category = article.get('category', 1)  # ƒê·∫£m b·∫£o s·ª≠ d·ª•ng ƒë√∫ng t√™n c·ªôt trong database
                meta_data = article.get('meta_data', {})
                
                # ƒê·∫£m b·∫£o meta_data l√† string JSON
                if isinstance(meta_data, dict):
                    meta_data = json.dumps(meta_data)
                
                # SQL query ƒë·ªÉ insert b√†i vi·∫øt
                query = """
                INSERT INTO articles 
                (title, slug, summary, content, source_name, source_url, source_icon, 
                published_at, category, meta_data, is_processed, created_at, updated_at) 
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
                """
                
                # T·∫°o tuple c√°c gi√° tr·ªã
                values = (
                    title, slug, summary, content, source_name, source_url, source_icon,
                    published_at, category, meta_data, True
                )
                
                # Th·ª±c hi·ªán query
                cursor.execute(query, values)
                
                # ƒê·∫øm s·ªë b√†i vi·∫øt th√†nh c√¥ng
                success_count += 1
                logger.info(f"ƒê√£ import th√†nh c√¥ng b√†i vi·∫øt: {title}")
                
            except mysql.connector.Error as e:
                # B·ªè qua l·ªói duplicate key
                if e.errno == 1062:  # M√£ l·ªói cho duplicate key
                    logger.warning(f"B√†i vi·∫øt ƒë√£ t·ªìn t·∫°i (slug tr√πng l·∫∑p): {article.get('title', '')}")
                else:
                    logger.error(f"L·ªói MySQL khi import b√†i vi·∫øt: {str(e)}")
                
                failed_count += 1
            except Exception as e:
                logger.error(f"L·ªói khi import b√†i vi·∫øt: {str(e)}")
                failed_count += 1
        
        # Commit c√°c thay ƒë·ªïi
        conn.commit()
        logger.info(f"ƒê√£ import tr·ª±c ti·∫øp v√†o database: {success_count} th√†nh c√¥ng, {failed_count} th·∫•t b·∫°i")
        
    except Exception as e:
        logger.error(f"L·ªói khi import v√†o database: {str(e)}")
        logger.error(traceback.format_exc())
        conn.rollback()
    finally:
        cursor.close()
        conn.close()
    
    return success_count, failed_count

def check_db_connection():
    """
    Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn c∆° s·ªü d·ªØ li·ªáu
    
    Returns:
        bool: True n·∫øu k·∫øt n·ªëi th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        if conn.is_connected():
            logger.info("K·∫øt n·ªëi CSDL th√†nh c√¥ng")
            conn.close()
            return True
        else:
            logger.error("Kh√¥ng th·ªÉ k·∫øt n·ªëi CSDL")
            return False
    except Exception as e:
        logger.error(f"L·ªói k·∫øt n·ªëi CSDL: {str(e)}")
        return False

def check_backend_api():
    """
    Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn backend API
    
    Returns:
        bool: True n·∫øu k·∫øt n·ªëi th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    try:
        logger.info(f"Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn Backend API: {CATEGORIES_API_URL}")
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}
        response = requests.get(CATEGORIES_API_URL, headers=headers, timeout=10)
        
        if response.status_code == 200:
            categories = response.json()
            logger.info(f"K·∫øt n·ªëi API th√†nh c√¥ng! T√¨m th·∫•y {len(categories)} danh m·ª•c.")
            
            # In ra 3 danh m·ª•c ƒë·∫ßu ti√™n ƒë·ªÉ ki·ªÉm tra
            for i, category in enumerate(categories[:3]):
                logger.info(f"Danh m·ª•c {i+1}: ID={category.get('id')}, T√™n={category.get('name')}")
                
            return True
        else:
            logger.error(f"L·ªói API HTTP {response.status_code}: {response.text}")
            return False
    except Exception as e:
        logger.error(f"L·ªói k·∫øt n·ªëi API: {str(e)}")
        return False

def generate_slug(title, add_uuid=True):
    """
    T·∫°o slug t·ª´ ti√™u ƒë·ªÅ b√†i vi·∫øt
    :param title: Ti√™u ƒë·ªÅ b√†i vi·∫øt
    :param add_uuid: Th√™m UUID v√†o slug ƒë·ªÉ ƒë·∫£m b·∫£o duy nh·∫•t
    :return: Slug ƒë√£ ƒë∆∞·ª£c t·∫°o
    """
    # Lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát
    slug = unidecode(title)
    
    # Chuy·ªÉn ƒë·ªïi th√†nh ch·ªØ th∆∞·ªùng v√† thay th·∫ø c√°c k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ c√°i, s·ªë b·∫±ng d·∫•u g·∫°ch ngang
    slug = re.sub(r'[^\w\s-]', '', slug.lower())
    slug = re.sub(r'[\s_-]+', '-', slug).strip('-')
    
    # Th√™m UUID ƒë·ªÉ ƒë·∫£m b·∫£o duy nh·∫•t
    if add_uuid:
        unique_id = str(uuid.uuid4())[:8]
        slug = f"{slug}-{unique_id}"
        
    return slug

def main():
    """
    H√†m main x·ª≠ l√Ω c√°c tham s·ªë d√≤ng l·ªánh v√† th·ª±c thi c√°c l·ªánh t∆∞∆°ng ·ª©ng
    """
    parser = argparse.ArgumentParser(description="Scraper Service - T√¨m ki·∫øm v√† x·ª≠ l√Ω tin t·ª©c t·ª± ƒë·ªông")
    
    parser.add_argument("--all", action="store_true", help="T√¨m ki·∫øm v√† x·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c")
    parser.add_argument("--max", type=int, default=2, help="S·ªë b√†i vi·∫øt t·ªëi ƒëa cho m·ªói danh m·ª•c")
    parser.add_argument("--category", type=int, help="ID c·ªßa danh m·ª•c c·∫ßn x·ª≠ l√Ω")
    parser.add_argument("--auto-send", action="store_true", help="T·ª± ƒë·ªông g·ª≠i b√†i vi·∫øt ƒë·∫øn backend")
    parser.add_argument("--import", action="store_true", dest="import_all", help="Import t·∫•t c·∫£ c√°c file JSON v√†o backend")
    parser.add_argument("--import-file", type=str, help="Path to JSON file to import")
    parser.add_argument("--cleanup", action="store_true", help="D·ªçn d·∫πp t·ªáp tin t·∫°m v√† log c≈©")
    parser.add_argument("--check", action="store_true", help="Ki·ªÉm tra k·∫øt n·ªëi database v√† API")
    
    args = parser.parse_args()
    
    # Ki·ªÉm tra k·∫øt n·ªëi database n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
    if args.check:
        print("\n----- KI·ªÇM TRA K·∫æT N·ªêI -----")
        db_status = check_db_connection()
        api_status = check_backend_api()
        
        print("\n----- K·∫æT QU·∫¢ KI·ªÇM TRA -----")
        print(f"K·∫øt n·ªëi CSDL: {'‚úì Th√†nh c√¥ng' if db_status else '‚úó Th·∫•t b·∫°i'}")
        print(f"K·∫øt n·ªëi API: {'‚úì Th√†nh c√¥ng' if api_status else '‚úó Th·∫•t b·∫°i'}")
        print("---------------------------\n")
        return
    
    # X·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c
    if args.all:
        print(f"X·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c, t·ªëi ƒëa {args.max} b√†i vi·∫øt m·ªói danh m·ª•c")
        logger.info(f"B·∫Øt ƒë·∫ßu t√¨m v√† x·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c, t·ªëi ƒëa {args.max} b√†i vi·∫øt m·ªói danh m·ª•c")
        
        results = find_and_process_all_categories(max_articles_per_category=args.max)
        
        # Log k·∫øt qu·∫£ thu ƒë∆∞·ª£c
        logger.info(f"K·∫øt qu·∫£ t√¨m ki·∫øm: {results['success']} danh m·ª•c th√†nh c√¥ng, {results['failed']} danh m·ª•c th·∫•t b·∫°i")
        logger.info(f"T·ªïng s·ªë b√†i vi·∫øt ƒë√£ t√¨m ƒë∆∞·ª£c: {results['total_articles']}")
        
        # T·ª± ƒë·ªông g·ª≠i b√†i vi·∫øt t·ªõi backend n·∫øu c·ªù auto-send ƒë∆∞·ª£c b·∫≠t
        if args.auto_send and results['all_articles'] and len(results['all_articles']) > 0:
            logger.info(f"T·ª± ƒë·ªông g·ª≠i {len(results['all_articles'])} b√†i vi·∫øt t·ªõi backend")
            
            # G·ªçi h√†m g·ª≠i c√°c b√†i vi·∫øt t·ªõi backend API
            send_status = send_to_backend(results['all_articles'])
            
            if send_status:
                logger.info("ƒê√£ g·ª≠i t·∫•t c·∫£ b√†i vi·∫øt th√†nh c√¥ng ƒë·∫øn backend")
            else:
                logger.warning("C√≥ l·ªói khi g·ª≠i b√†i vi·∫øt ƒë·∫øn backend")
        
        # L∆∞u t·∫•t c·∫£ b√†i vi·∫øt v√†o 1 file JSON
        logger.info(f"B·∫Øt ƒë·∫ßu l∆∞u t·∫•t c·∫£ b√†i vi·∫øt v√†o file JSON duy nh·∫•t, d·ªØ li·ªáu ƒë·∫ßu v√†o: {type(results)}")
        logger.info(f"C·∫•u tr√∫c results: {list(results.keys()) if isinstance(results, dict) else type(results)}")
        
        json_file = save_all_articles_to_single_file(results)
        
        if json_file and os.path.exists(json_file):
            logger.info(f"ƒê√£ l∆∞u t·∫•t c·∫£ b√†i vi·∫øt v√†o file JSON: {json_file}")
            
            # Import file JSON v√†o backend n·∫øu c·ªù auto-send ƒë∆∞·ª£c b·∫≠t
            if args.auto_send:
                imported, failed = import_json_file_to_backend(json_file)
                logger.info(f"ƒê√£ import file JSON v√†o backend: {imported} th√†nh c√¥ng, {failed} th·∫•t b·∫°i")
        else:
            logger.warning("Kh√¥ng th·ªÉ l∆∞u b√†i vi·∫øt v√†o file JSON")
            
        logger.info(f"Scraper ho√†n th√†nh: X·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c, t·ªëi ƒëa {args.max} b√†i vi·∫øt m·ªói danh m·ª•c")
    
    # X·ª≠ l√Ω m·ªôt danh m·ª•c c·ª• th·ªÉ
    elif args.category:
        category_id = args.category
        print(f"X·ª≠ l√Ω danh m·ª•c ID: {category_id}, t·ªëi ƒëa {args.max} b√†i vi·∫øt")
        
        # L·∫•y th√¥ng tin danh m·ª•c t·ª´ backend
        try:
            category_info = get_category_by_id(category_id)
            if category_info and 'name' in category_info:
                category_name = category_info['name']
                print(f"Danh m·ª•c: {category_name}")
                
                # X·ª≠ l√Ω danh m·ª•c
                articles = process_category(category_id, category_name, max_articles=args.max)
                
                if articles:
                    print(f"T√¨m th·∫•y {len(articles)} b√†i vi·∫øt")
                    
                    # T·ª± ƒë·ªông g·ª≠i b√†i vi·∫øt t·ªõi backend n·∫øu c·ªù auto-send ƒë∆∞·ª£c b·∫≠t
                    if args.auto_send and len(articles) > 0:
                        send_status = send_to_backend(articles)
                        if send_status:
                            print("ƒê√£ g·ª≠i t·∫•t c·∫£ b√†i vi·∫øt th√†nh c√¥ng ƒë·∫øn backend")
                        else:
                            print("C√≥ l·ªói khi g·ª≠i b√†i vi·∫øt ƒë·∫øn backend")
                else:
                    print("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o ph√π h·ª£p")
            else:
                print(f"Kh√¥ng t√¨m th·∫•y th√¥ng tin danh m·ª•c v·ªõi ID: {category_id}")
        except Exception as e:
            print(f"L·ªói khi x·ª≠ l√Ω danh m·ª•c {category_id}: {str(e)}")
    
    # Import t·∫•t c·∫£ c√°c file JSON trong th∆∞ m·ª•c output
    elif args.import_all:
        print("Import t·∫•t c·∫£ c√°c file JSON trong th∆∞ m·ª•c output")
        success, failed = import_all_to_backend()
        print(f"K·∫øt qu·∫£: {success} th√†nh c√¥ng, {failed} th·∫•t b·∫°i")
    
    # Import file JSON ch·ªâ ƒë·ªãnh
    elif args.import_file:
        json_file = args.import_file
        if not os.path.exists(json_file):
            print(f"File kh√¥ng t·ªìn t·∫°i: {json_file}")
            return
        
        print(f"Import file JSON: {json_file}")
        success, failed = import_json_file_to_backend(json_file)
        print(f"K·∫øt qu·∫£: {success} th√†nh c√¥ng, {failed} th·∫•t b·∫°i")
    
    # D·ªçn d·∫πp t·ªáp tin c≈©
    elif args.cleanup:
        print("D·ªçn d·∫πp c√°c t·ªáp tin t·∫°m v√† log c≈©")
        cleanup_temp_files()
        cleanup_old_files()
    
    # Kh√¥ng c√≥ tham s·ªë d√≤ng l·ªánh, hi·ªÉn th·ªã tr·ª£ gi√∫p
    else:
        parser.print_help()

if __name__ == "__main__":
    main() 