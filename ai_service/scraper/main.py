#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ƒê√¢y l√† module ch√≠nh cho scraper b√†i vi·∫øt tin t·ª©c
Quy tr√¨nh: 
1. T√¨m ki·∫øm URL b√†i vi·∫øt t·ª´ google_news_serpapi.py 
2. Tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL b·∫±ng scrape_articles_selenium.py
"""

import os
import sys
import json
import uuid
import time
import shutil
import logging
import argparse
import requests
from pathlib import Path
from datetime import datetime, timedelta
from urllib.parse import urlparse

# Import c√°c module n·ªôi b·ªô
import google_news_serpapi
import scrape_articles_selenium

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(f"scraper_{datetime.now().strftime('%Y%m%d')}.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

# üîπ C√°c th√¥ng s·ªë m·∫∑c ƒë·ªãnh
SCRAPER_DIR = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.path.join(SCRAPER_DIR, "temp")
OUTPUT_DIR = os.path.join(SCRAPER_DIR, "output")
DEFAULT_BATCH_SIZE = 5
# S·ªë ng√†y ƒë·ªÉ gi·ªØ l·∫°i log files v√† output files
RETENTION_DAYS = 2

# üîπ Laravel Backend API URLs
BACKEND_API_URL = "http://localhost:8000/api/articles/import"
CATEGORIES_API_URL = "http://localhost:8000/api/categories"

def save_articles_to_file(articles, output_file=None):
    """
    L∆∞u danh s√°ch b√†i vi·∫øt v√†o file JSON
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt
        output_file (str): ƒê∆∞·ªùng d·∫´n file ƒë·∫ßu ra
        
    Returns:
        str: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ƒë√£ l∆∞u
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ l∆∞u!")
        return None
    
    # T·∫°o th∆∞ m·ª•c ƒë·∫ßu ra n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # T·∫°o t√™n file m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
    if not output_file:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(OUTPUT_DIR, f"enriched_articles_{timestamp}.json")
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u
    normalized_articles = []
    for article in articles:
        normalized = article.copy()
        
        # ƒê·∫£m b·∫£o source_name l√† chu·ªói
        if isinstance(normalized.get("source_name"), dict):
            normalized["source_name"] = normalized["source_name"].get("name", "Unknown Source")
        
        # ƒê·∫£m b·∫£o meta_data l√† chu·ªói JSON
        if isinstance(normalized.get("meta_data"), dict):
            normalized["meta_data"] = json.dumps(normalized["meta_data"])
        
        # ƒê·∫£m b·∫£o summary kh√¥ng bao gi·ªù l√† None
        if normalized.get("summary") is None:
            normalized["summary"] = ""
        
        # ƒê·∫£m b·∫£o content kh√¥ng bao gi·ªù l√† None
        if normalized.get("content") is None:
            normalized["content"] = ""
        
        # ƒê·∫£m b·∫£o published_at c√≥ ƒë·ªãnh d·∫°ng chu·∫©n
        if normalized.get("published_at"):
            try:
                # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y th√°ng
                date_obj = datetime.fromisoformat(normalized["published_at"].replace('Z', '+00:00'))
                normalized["published_at"] = date_obj.isoformat()
            except (ValueError, TypeError):
                # N·∫øu kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        
        normalized_articles.append(normalized)
    
    # L∆∞u b√†i vi·∫øt v√†o file JSON
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(normalized_articles, f, ensure_ascii=False, indent=4)
    
    logger.info(f"[OK] ƒê√£ l∆∞u {len(normalized_articles)} b√†i vi·∫øt v√†o {output_file}")
    return output_file

def send_to_backend(articles, batch_size=DEFAULT_BATCH_SIZE, auto_send=True):
    """
    G·ª≠i b√†i vi·∫øt t·ªõi backend API
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt
        batch_size (int): S·ªë l∆∞·ª£ng b√†i vi·∫øt g·ª≠i trong m·ªói request
        auto_send (bool): T·ª± ƒë·ªông g·ª≠i kh√¥ng c·∫ßn x√°c nh·∫≠n
        
    Returns:
        bool: Tr·∫°ng th√°i th√†nh c√¥ng
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ g·ª≠i!")
        return False
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi g·ª≠i
    normalized_articles = []
    for article in articles:
        normalized = article.copy()
        
        # ƒê·∫£m b·∫£o source_name l√† chu·ªói
        if isinstance(normalized.get("source_name"), dict):
            normalized["source_name"] = normalized["source_name"].get("name", "Unknown Source")
        
        # ƒê·∫£m b·∫£o meta_data l√† chu·ªói JSON
        if isinstance(normalized.get("meta_data"), dict):
            normalized["meta_data"] = json.dumps(normalized["meta_data"])
        
        # ƒê·∫£m b·∫£o summary kh√¥ng bao gi·ªù l√† None
        if normalized.get("summary") is None:
            normalized["summary"] = ""
        
        # ƒê·∫£m b·∫£o content kh√¥ng bao gi·ªù l√† None
        if normalized.get("content") is None:
            normalized["content"] = ""
        
        # ƒê·∫£m b·∫£o published_at c√≥ ƒë·ªãnh d·∫°ng chu·∫©n
        if normalized.get("published_at"):
            try:
                # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y th√°ng
                date_obj = datetime.fromisoformat(normalized["published_at"].replace('Z', '+00:00'))
                normalized["published_at"] = date_obj.isoformat()
            except (ValueError, TypeError):
                # N·∫øu kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        
        normalized_articles.append(normalized)
    
    # X√°c nh·∫≠n g·ª≠i n·∫øu kh√¥ng t·ª± ƒë·ªông
    if not auto_send:
        confirm = input(f"B·∫°n c√≥ mu·ªën g·ª≠i {len(normalized_articles)} b√†i vi·∫øt t·ªõi backend? (y/n): ").lower().strip()
        if confirm != 'y':
            logger.info("ƒê√£ h·ªßy g·ª≠i b√†i vi·∫øt t·ªõi backend.")
            return False
    
    logger.info(f"G·ª≠i {len(normalized_articles)} b√†i vi·∫øt t·ªõi backend...")
    
    success = True
    total_batches = (len(normalized_articles) + batch_size - 1) // batch_size
    
    for i in range(0, len(normalized_articles), batch_size):
        batch = normalized_articles[i:i+batch_size]
        batch_num = i // batch_size + 1
        
        logger.info(f"G·ª≠i batch {batch_num}/{total_batches} ({len(batch)} b√†i vi·∫øt)")
        
        try:
            payload = {"articles": batch}
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json"
            }
            
            # In payload ƒë·ªÉ ki·ªÉm tra
            logger.debug(f"Payload m·∫´u (b√†i vi·∫øt ƒë·∫ßu ti√™n): {json.dumps(batch[0], ensure_ascii=False)[:200]}...")
            
            # Thi·∫øt l·∫≠p timeout v√† s·ªë l·∫ßn th·ª≠ l·∫°i
            max_retries = 3
            retry_count = 0
            
            while retry_count < max_retries:
                try:
                    response = requests.post(
                        BACKEND_API_URL, 
                        json=payload, 
                        headers=headers,
                        timeout=30  # 30 gi√¢y timeout
                    )
                    
                    # Ki·ªÉm tra xem response c√≥ ph·∫£i JSON kh√¥ng
                    response_content = response.text
                    logger.debug(f"Response raw: {response_content[:200]}...")
                    
                    if response.status_code in (200, 201):
                        try:
                            result = response.json()
                            logger.info(f"[OK] Batch {batch_num}: {result.get('message', '')}")
                            if 'errors' in result and result['errors']:
                                logger.warning(f"[WARN] C√≥ {len(result['errors'])} l·ªói trong batch {batch_num}:")
                                for error in result['errors']:
                                    logger.warning(f"  - {error}")
                        except json.JSONDecodeError:
                            logger.warning(f"[WARN] Response kh√¥ng ph·∫£i JSON h·ª£p l·ªá m·∫∑c d√π status code = {response.status_code}")
                            logger.warning(f"[WARN] Response content: {response_content[:200]}...")
                        break
                    else:
                        logger.error(f"[ERROR] Batch {batch_num}: L·ªói {response.status_code} - {response.text}")
                        retry_count += 1
                        if retry_count < max_retries:
                            wait_time = 2 ** retry_count  # TƒÉng th·ªùi gian ch·ªù theo c·∫•p s·ªë nh√¢n
                            logger.info(f"Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                            time.sleep(wait_time)
                        else:
                            success = False
                            logger.error(f"[ERROR] ƒê√£ th·ª≠ l·∫°i {max_retries} l·∫ßn kh√¥ng th√†nh c√¥ng cho batch {batch_num}")
                
                except requests.exceptions.Timeout:
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = 2 ** retry_count
                        logger.info(f"Request timeout. Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                    else:
                        success = False
                        logger.error(f"[ERROR] Timeout sau {max_retries} l·∫ßn th·ª≠ l·∫°i cho batch {batch_num}")
                
                except requests.exceptions.ConnectionError:
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = 2 ** retry_count
                        logger.info(f"L·ªói k·∫øt n·ªëi. Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                    else:
                        success = False
                        logger.error(f"[ERROR] L·ªói k·∫øt n·ªëi sau {max_retries} l·∫ßn th·ª≠ l·∫°i cho batch {batch_num}")
                
                except Exception as e:
                    logger.error(f"[ERROR] Batch {batch_num}: L·ªói kh√¥ng mong ƒë·ª£i: {str(e)}")
                    success = False
                    break
            
        except Exception as e:
            logger.error(f"[ERROR] L·ªói khi g·ª≠i batch {batch_num}: {str(e)}")
            success = False
    
    if success:
        logger.info(f"[OK] ƒê√£ g·ª≠i th√†nh c√¥ng {len(normalized_articles)} b√†i vi·∫øt t·ªõi backend")
    else:
        logger.warning("[WARN] C√≥ l·ªói x·∫£y ra khi g·ª≠i b√†i vi·∫øt t·ªõi backend")
    
    return success

def cleanup_temp_files():
    """
    D·ªçn d·∫πp c√°c file t·∫°m th·ªùi
    """
    if os.path.exists(TEMP_DIR):
        try:
            shutil.rmtree(TEMP_DIR)
            logger.info(f"[OK] ƒê√£ d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {TEMP_DIR}")
        except Exception as e:
            logger.error(f"[ERROR] L·ªói khi d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {str(e)}")

def cleanup_old_files(days=RETENTION_DAYS):
    """
    D·ªçn d·∫πp c√°c file c≈© (logs, outputs) ƒë·ªÉ tr√°nh t·ªën dung l∆∞·ª£ng
    
    Args:
        days (int): S·ªë ng√†y ƒë·ªÉ gi·ªØ l·∫°i file
    """
    # Th·ªùi ƒëi·ªÉm hi·ªán t·∫°i
    now = datetime.now()
    cutoff_date = now - timedelta(days=days)
    
    # D·ªçn d·∫πp output files
    logger.info(f"D·ªçn d·∫πp files ƒë·∫ßu ra c≈© h∆°n {days} ng√†y...")
    cleaned_count = 0
    
    if os.path.exists(OUTPUT_DIR):
        for file_name in os.listdir(OUTPUT_DIR):
            file_path = os.path.join(OUTPUT_DIR, file_name)
            if os.path.isfile(file_path):
                # L·∫•y th·ªùi gian s·ª≠a ƒë·ªïi
                mtime = os.path.getmtime(file_path)
                file_date = datetime.fromtimestamp(mtime)
                
                # N·∫øu file c≈© h∆°n s·ªë ng√†y quy ƒë·ªãnh, x√≥a n√≥
                if file_date < cutoff_date:
                    try:
                        os.remove(file_path)
                        cleaned_count += 1
                        logger.debug(f"ƒê√£ x√≥a file c≈©: {file_path}")
                    except Exception as e:
                        logger.error(f"Kh√¥ng th·ªÉ x√≥a file {file_path}: {e}")
    
    # D·ªçn d·∫πp log files
    log_files = [f for f in os.listdir(SCRAPER_DIR) if f.startswith("scraper_") and f.endswith(".log")]
    for log_file in log_files:
        # Tr√≠ch xu·∫•t ng√†y t·ª´ t√™n file (ƒë·ªãnh d·∫°ng scraper_YYYYMMDD.log)
        try:
            date_str = log_file.replace("scraper_", "").replace(".log", "")
            file_date = datetime.strptime(date_str, "%Y%m%d")
            
            # N·∫øu file c≈© h∆°n s·ªë ng√†y quy ƒë·ªãnh, x√≥a n√≥
            if file_date < cutoff_date:
                file_path = os.path.join(SCRAPER_DIR, log_file)
                try:
                    os.remove(file_path)
                    cleaned_count += 1
                    logger.debug(f"ƒê√£ x√≥a log file c≈©: {log_file}")
                except Exception as e:
                    logger.error(f"Kh√¥ng th·ªÉ x√≥a file {file_path}: {e}")
        except (ValueError, IndexError):
            continue
    
    logger.info(f"[OK] ƒê√£ d·ªçn d·∫πp {cleaned_count} files c≈©")

def search_articles():
    """
    T√¨m ki·∫øm URL b√†i vi·∫øt s·ª≠ d·ª•ng google_news_serpapi.py
    
    Returns:
        tuple: (articles, output_file)
    """
    logger.info("[STEP 1] T√¨m ki·∫øm URL b√†i vi·∫øt t·ª´ danh m·ª•c")
    
    # L·∫•y danh s√°ch danh m·ª•c t·ª´ backend
    categories = google_news_serpapi.get_categories()
    
    articles = []
    # T√¨m ki·∫øm b√†i vi·∫øt cho m·ªói danh m·ª•c
    for category in categories:
        logger.info(f"T√¨m ki·∫øm b√†i vi·∫øt cho danh m·ª•c: {category}")
        category_articles = google_news_serpapi.fetch_articles_by_category(category)
        articles.extend(category_articles)
    
    logger.info(f"[OK] ƒê√£ t√¨m th·∫•y t·ªïng c·ªông {len(articles)} b√†i vi·∫øt")
    
    # L∆∞u tr·ªØ k·∫øt qu·∫£ t√¨m ki·∫øm
    if articles:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(OUTPUT_DIR, f"search_results_{timestamp}.json")
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(articles, f, ensure_ascii=False, indent=4)
        logger.info(f"[OK] ƒê√£ l∆∞u k·∫øt qu·∫£ t√¨m ki·∫øm v√†o {output_file}")
        return articles, output_file
    
    return articles, None

def extract_content_from_articles(articles):
    """
    Tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL b√†i vi·∫øt s·ª≠ d·ª•ng scrape_articles_selenium.py
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt v·ªõi URL
        
    Returns:
        list: Danh s√°ch b√†i vi·∫øt ƒë√£ tr√≠ch xu·∫•t n·ªôi dung
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ tr√≠ch xu·∫•t n·ªôi dung!")
        return []
    
    logger.info(f"[STEP 2] Tr√≠ch xu·∫•t n·ªôi dung cho {len(articles)} b√†i vi·∫øt")
    
    # Thi·∫øt l·∫≠p driver Selenium
    driver = scrape_articles_selenium.setup_driver()
    
    try:
        enriched_articles = []
        
        for i, article in enumerate(articles, 1):
            title = article.get("title", "Unknown title")
            url = article.get("source_url", "")
            
            if not url:
                logger.warning(f"B√†i vi·∫øt {i} kh√¥ng c√≥ URL: {title}")
                continue
            
            # Ki·ªÉm tra URL ph√π h·ª£p
            if not scrape_articles_selenium.filter_article(url):
                logger.info(f"[INFO] B·ªè qua URL kh√¥ng ph√π h·ª£p: {url}")
                continue
            
            logger.info(f"[INFO] ƒêang tr√≠ch xu·∫•t n·ªôi dung cho b√†i vi·∫øt {i}/{len(articles)}: {title}")
            
            # S·ª≠ d·ª•ng h√†m enrich_article t·ª´ scrape_articles_selenium
            enriched = scrape_articles_selenium.enrich_article(driver, article)
            if enriched:
                enriched_articles.append(enriched)
            
            # Th√™m ƒë·ªô tr·ªÖ gi·ªØa c√°c request ƒë·ªÉ tr√°nh t·∫£i qu√° m·ª©c
            if i < len(articles):
                time.sleep(2)
    
    finally:
        # ƒê√≥ng driver khi ho√†n th√†nh
        driver.quit()
    
    logger.info(f"[OK] ƒê√£ tr√≠ch xu·∫•t n·ªôi dung cho {len(enriched_articles)} b√†i vi·∫øt")
    return enriched_articles

def main():
    """
    H√†m ch√≠nh ƒëi·ªÅu ph·ªëi quy tr√¨nh scraping
    """
    parser = argparse.ArgumentParser(description="Scraper b√†i vi·∫øt tin t·ª©c")
    parser.add_argument("--skip-search", action="store_true", help="B·ªè qua b∆∞·ªõc t√¨m ki·∫øm URL b√†i vi·∫øt")
    parser.add_argument("--skip-extraction", action="store_true", help="B·ªè qua b∆∞·ªõc tr√≠ch xu·∫•t n·ªôi dung")
    parser.add_argument("--skip-send", action="store_true", help="B·ªè qua b∆∞·ªõc g·ª≠i ƒë·∫øn backend")
    parser.add_argument("--input-file", type=str, help="File JSON ch·ª©a b√†i vi·∫øt ƒë·ªÉ x·ª≠ l√Ω")
    parser.add_argument("--auto-send", action="store_true", help="T·ª± ƒë·ªông g·ª≠i b√†i vi·∫øt kh√¥ng c·∫ßn x√°c nh·∫≠n")
    parser.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE, help="S·ªë l∆∞·ª£ng b√†i vi·∫øt g·ª≠i trong m·ªói request")
    parser.add_argument("--verbose", action="store_true", help="Hi·ªÉn th·ªã nhi·ªÅu th√¥ng tin h∆°n")
    parser.add_argument("--retention-days", type=int, default=RETENTION_DAYS, help="S·ªë ng√†y gi·ªØ l·∫°i files tr∆∞·ªõc khi x√≥a")
    parser.add_argument("--no-cleanup", action="store_true", help="Kh√¥ng x√≥a files c≈©")
    
    args = parser.parse_args()
    
    # Thi·∫øt l·∫≠p m·ª©c ƒë·ªô chi ti·∫øt logging
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    
    # T·∫°o th∆∞ m·ª•c l√†m vi·ªác n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(TEMP_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # D·ªçn d·∫πp files c≈© n·∫øu kh√¥ng c√≥ c·ªù --no-cleanup
    if not args.no_cleanup:
        cleanup_old_files(args.retention_days)
    
    try:
        articles = []
        search_file = None
        
        # B∆Ø·ªöC 1: T√¨m ki·∫øm URL b√†i vi·∫øt
        if not args.skip_search and not args.input_file:
            articles, search_file = search_articles()
        
        # N·∫øu c√≥ input_file, ƒë·ªçc t·ª´ file
        elif args.input_file:
            try:
                with open(args.input_file, "r", encoding="utf-8") as f:
                    articles = json.load(f)
                logger.info(f"[OK] ƒê√£ ƒë·ªçc {len(articles)} b√†i vi·∫øt t·ª´ {args.input_file}")
                search_file = args.input_file
            except Exception as e:
                logger.error(f"[ERROR] L·ªói khi ƒë·ªçc file {args.input_file}: {str(e)}")
                return
        
        # B∆Ø·ªöC 2: Tr√≠ch xu·∫•t n·ªôi dung b√†i vi·∫øt
        enriched_articles = []
        enriched_file = None
        
        if not args.skip_extraction and articles:
            # Ki·ªÉm tra n·∫øu b√†i vi·∫øt ƒë√£ c√≥ n·ªôi dung ƒë·∫ßy ƒë·ªß
            articles_without_content = [a for a in articles if not a.get("content")]
            
            if articles_without_content:
                logger.info(f"Ti·∫øn h√†nh tr√≠ch xu·∫•t n·ªôi dung cho {len(articles_without_content)} b√†i vi·∫øt")
                enriched_articles = extract_content_from_articles(articles_without_content)
                
                # C·∫≠p nh·∫≠t danh s√°ch b√†i vi·∫øt g·ªëc v·ªõi n·ªôi dung ƒë√£ tr√≠ch xu·∫•t
                articles_with_content = [a for a in articles if a.get("content")]
                enriched_articles = articles_with_content + enriched_articles
                
                # L∆∞u b√†i vi·∫øt ƒë√£ ƒë∆∞·ª£c l√†m gi√†u
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                enriched_file = os.path.join(OUTPUT_DIR, f"enriched_articles_{timestamp}.json")
                save_articles_to_file(enriched_articles, enriched_file)
            else:
                logger.info("[INFO] T·∫•t c·∫£ b√†i vi·∫øt ƒë√£ c√≥ n·ªôi dung, b·ªè qua b∆∞·ªõc tr√≠ch xu·∫•t")
                enriched_articles = articles
                enriched_file = search_file
        else:
            # N·∫øu b·ªè qua b∆∞·ªõc tr√≠ch xu·∫•t nh∆∞ng v·∫´n c√≥ b√†i vi·∫øt, sao ch√©p danh s√°ch
            enriched_articles = articles
        
        # B∆Ø·ªöC 3: G·ª≠i b√†i vi·∫øt ƒë·∫øn backend
        if not args.skip_send and enriched_articles:
            logger.info(f"[STEP 3] G·ª≠i {len(enriched_articles)} b√†i vi·∫øt t·ªõi backend")
            success = send_to_backend(enriched_articles, args.batch_size, args.auto_send)
            
            if success:
                logger.info("[OK] ƒê√£ ho√†n th√†nh quy tr√¨nh scraping!")
            else:
                logger.warning("[WARN] Qu√° tr√¨nh g·ª≠i b√†i vi·∫øt t·ªõi backend kh√¥ng ho√†n ch·ªânh")
        else:
            logger.info("B·ªè qua b∆∞·ªõc g·ª≠i b√†i vi·∫øt t·ªõi backend.")
            if enriched_file:
                logger.info(f"C√°c b√†i vi·∫øt ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {enriched_file}")
                logger.info(f"B·∫°n c√≥ th·ªÉ g·ª≠i th·ªß c√¥ng b·∫±ng c√°ch ch·∫°y: python main.py --skip-search --skip-extraction --input-file {enriched_file}")
    
    except KeyboardInterrupt:
        logger.info("[INFO] Qu√° tr√¨nh b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
    except Exception as e:
        logger.error(f"[ERROR] L·ªói kh√¥ng mong ƒë·ª£i: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
    finally:
        # D·ªçn d·∫πp c√°c file t·∫°m th·ªùi
        cleanup_temp_files()
        logger.info("[INFO] K·∫øt th√∫c ch∆∞∆°ng tr√¨nh")

if __name__ == "__main__":
    main() 