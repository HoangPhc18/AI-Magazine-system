#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ƒê√¢y l√† module ch√≠nh cho scraper b√†i vi·∫øt tin t·ª©c
Quy tr√¨nh: 
1. T√¨m ki·∫øm URL b√†i vi·∫øt t·ª´ google_news_serpapi.py 
2. Tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL b·∫±ng scrape_articles_selenium.py
"""

import os
import sys
import json
import uuid
import time
import shutil
import logging
import argparse
import requests
from pathlib import Path
from datetime import datetime, timedelta
from urllib.parse import urlparse
import glob
import traceback
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import random
from dotenv import load_dotenv

# Import c√°c module n·ªôi b·ªô
import google_news_serpapi
import scrape_articles_selenium

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(f"scraper_{datetime.now().strftime('%Y%m%d')}.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

# üîπ C√°c th√¥ng s·ªë m·∫∑c ƒë·ªãnh
SCRAPER_DIR = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.path.join(SCRAPER_DIR, "temp")
OUTPUT_DIR = os.path.join(SCRAPER_DIR, "output")
DEFAULT_BATCH_SIZE = 5
# S·ªë ng√†y ƒë·ªÉ gi·ªØ l·∫°i log files v√† output files
RETENTION_DAYS = 2

# üîπ Laravel Backend API URLs
BACKEND_API_URL = "http://localhost:8000/api/articles/import"
CATEGORIES_API_URL = "http://localhost:8000/api/categories"

# Ki·ªÉm tra c√°c API keys
def check_api_keys():
    """
    Ki·ªÉm tra c√°c API keys t·ª´ file .env
    """
    api_keys_status = {}
    
    # Ki·ªÉm tra WorldNewsAPI
    worldnews_api_key = os.environ.get('WORLDNEWS_API_KEY', '')
    if worldnews_api_key:
        api_keys_status['WorldNewsAPI'] = True
    else:
        api_keys_status['WorldNewsAPI'] = False
        
    # Ki·ªÉm tra Currents API
    currents_api_key = os.environ.get('CURRENTS_API_KEY', '')
    if currents_api_key:
        api_keys_status['CurrentsAPI'] = True
    else:
        api_keys_status['CurrentsAPI'] = False
    
    # In th√¥ng tin
    logger.info("=== Tr·∫°ng th√°i API keys ===")
    for api_name, status in api_keys_status.items():
        if status:
            logger.info(f"‚úÖ {api_name}: OK")
        else:
            logger.warning(f"‚ö†Ô∏è {api_name}: Kh√¥ng t√¨m th·∫•y API key")
    
    return api_keys_status

def save_articles_to_file(articles, output_file=None):
    """
    L∆∞u danh s√°ch b√†i vi·∫øt v√†o file JSON
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt
        output_file (str): ƒê∆∞·ªùng d·∫´n file ƒë·∫ßu ra
        
    Returns:
        str: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ƒë√£ l∆∞u
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ l∆∞u!")
        return None
    
    # T·∫°o th∆∞ m·ª•c ƒë·∫ßu ra n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # T·∫°o t√™n file m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
    if not output_file:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(OUTPUT_DIR, f"enriched_articles_{timestamp}.json")
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u
    normalized_articles = []
    for article in articles:
        normalized = article.copy()
        
        # ƒê·∫£m b·∫£o source_name l√† chu·ªói
        if isinstance(normalized.get("source_name"), dict):
            normalized["source_name"] = normalized["source_name"].get("name", "Unknown Source")
        
        # ƒê·∫£m b·∫£o meta_data l√† chu·ªói JSON
        if isinstance(normalized.get("meta_data"), dict):
            normalized["meta_data"] = json.dumps(normalized["meta_data"])
        
        # ƒê·∫£m b·∫£o summary kh√¥ng bao gi·ªù l√† None
        if normalized.get("summary") is None:
            normalized["summary"] = ""
        
        # ƒê·∫£m b·∫£o content kh√¥ng bao gi·ªù l√† None
        if normalized.get("content") is None:
            normalized["content"] = ""
        
        # ƒê·∫£m b·∫£o published_at c√≥ ƒë·ªãnh d·∫°ng chu·∫©n
        if normalized.get("published_at"):
            try:
                # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y th√°ng
                date_obj = datetime.fromisoformat(normalized["published_at"].replace('Z', '+00:00'))
                normalized["published_at"] = date_obj.isoformat()
            except (ValueError, TypeError):
                # N·∫øu kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        
        normalized_articles.append(normalized)
    
    # L∆∞u b√†i vi·∫øt v√†o file JSON
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(normalized_articles, f, ensure_ascii=False, indent=4)
    
    logger.info(f"[OK] ƒê√£ l∆∞u {len(normalized_articles)} b√†i vi·∫øt v√†o {output_file}")
    return output_file

def send_to_backend(articles, batch_size=DEFAULT_BATCH_SIZE, auto_send=True):
    """
    G·ª≠i b√†i vi·∫øt t·ªõi backend API
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt
        batch_size (int): S·ªë l∆∞·ª£ng b√†i vi·∫øt g·ª≠i trong m·ªói request
        auto_send (bool): T·ª± ƒë·ªông g·ª≠i kh√¥ng c·∫ßn x√°c nh·∫≠n (lu√¥n True)
        
    Returns:
        bool: Tr·∫°ng th√°i th√†nh c√¥ng
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ g·ª≠i!")
        return False
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi g·ª≠i
    normalized_articles = []
    for article in articles:
        normalized = article.copy()
        
        # ƒê·∫£m b·∫£o source_name l√† chu·ªói
        if isinstance(normalized.get("source_name"), dict):
            normalized["source_name"] = normalized["source_name"].get("name", "Unknown Source")
        
        # ƒê·∫£m b·∫£o meta_data l√† chu·ªói JSON
        if isinstance(normalized.get("meta_data"), dict):
            try:
                normalized["meta_data"] = json.dumps(normalized["meta_data"], ensure_ascii=False)
            except Exception as e:
                logger.warning(f"L·ªói khi chuy·ªÉn ƒë·ªïi meta_data sang JSON: {str(e)}")
                normalized["meta_data"] = json.dumps({"error": "Invalid metadata"})
        
        # ƒê·∫£m b·∫£o summary kh√¥ng bao gi·ªù l√† None
        if normalized.get("summary") is None:
            normalized["summary"] = ""
        
        # ƒê·∫£m b·∫£o content kh√¥ng bao gi·ªù l√† None
        if normalized.get("content") is None:
            normalized["content"] = ""
        
        # ƒê·∫£m b·∫£o published_at c√≥ ƒë·ªãnh d·∫°ng chu·∫©n
        if normalized.get("published_at"):
            try:
                # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y th√°ng
                date_obj = datetime.fromisoformat(normalized["published_at"].replace('Z', '+00:00'))
                normalized["published_at"] = date_obj.isoformat()
            except (ValueError, TypeError):
                # N·∫øu kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        else:
            # N·∫øu kh√¥ng c√≥ published_at, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        
        normalized_articles.append(normalized)
    
    logger.info(f"G·ª≠i {len(normalized_articles)} b√†i vi·∫øt t·ªõi backend...")
    
    success = True
    total_batches = (len(normalized_articles) + batch_size - 1) // batch_size
    
    for i in range(0, len(normalized_articles), batch_size):
        batch = normalized_articles[i:i+batch_size]
        batch_num = i // batch_size + 1
        
        logger.info(f"G·ª≠i batch {batch_num}/{total_batches} ({len(batch)} b√†i vi·∫øt)")
        
        try:
            payload = {"articles": batch}
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json"
            }
            
            # In payload m·∫´u ƒë·ªÉ ki·ªÉm tra
            if batch:
                sample = batch[0].copy()
                if 'content' in sample and sample['content'] and len(sample['content']) > 100:
                    sample['content'] = sample['content'][:100] + "..."
                logger.debug(f"Payload m·∫´u (b√†i vi·∫øt ƒë·∫ßu ti√™n): {json.dumps(sample, ensure_ascii=False)}")
            
            # Thi·∫øt l·∫≠p timeout v√† s·ªë l·∫ßn th·ª≠ l·∫°i
            max_retries = 3
            retry_count = 0
            
            while retry_count < max_retries:
                try:
                    response = requests.post(
                        BACKEND_API_URL, 
                        json=payload, 
                        headers=headers,
                        timeout=30  # 30 gi√¢y timeout
                    )
                    
                    # Ki·ªÉm tra xem response c√≥ ph·∫£i JSON kh√¥ng
                    response_content = response.text
                    logger.debug(f"Response status: {response.status_code}")
                    
                    if response.status_code in (200, 201):
                        try:
                            result = response.json()
                            logger.info(f"[OK] Batch {batch_num}: {result.get('message', '')}")
                            if 'errors' in result and result['errors']:
                                logger.warning(f"[WARN] C√≥ {len(result['errors'])} l·ªói trong batch {batch_num}:")
                                for error in result['errors']:
                                    logger.warning(f"  - {error}")
                        except json.JSONDecodeError:
                            logger.warning(f"[WARN] Response kh√¥ng ph·∫£i JSON h·ª£p l·ªá m·∫∑c d√π status code = {response.status_code}")
                            logger.warning(f"[WARN] Response content: {response_content[:200]}...")
                        break
                    else:
                        logger.error(f"[ERROR] Batch {batch_num}: L·ªói {response.status_code} - {response.text[:200]}...")
                        retry_count += 1
                        if retry_count < max_retries:
                            wait_time = 2 ** retry_count  # TƒÉng th·ªùi gian ch·ªù theo c·∫•p s·ªë nh√¢n
                            logger.info(f"Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                            time.sleep(wait_time)
                        else:
                            success = False
                            logger.error(f"[ERROR] ƒê√£ th·ª≠ l·∫°i {max_retries} l·∫ßn kh√¥ng th√†nh c√¥ng cho batch {batch_num}")
                
                except requests.exceptions.Timeout:
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = 2 ** retry_count
                        logger.info(f"Request timeout. Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                    else:
                        success = False
                        logger.error(f"[ERROR] Timeout sau {max_retries} l·∫ßn th·ª≠ l·∫°i cho batch {batch_num}")
                
                except requests.exceptions.ConnectionError:
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = 2 ** retry_count
                        logger.info(f"L·ªói k·∫øt n·ªëi. Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                    else:
                        success = False
                        logger.error(f"[ERROR] L·ªói k·∫øt n·ªëi sau {max_retries} l·∫ßn th·ª≠ l·∫°i cho batch {batch_num}")
                
                except Exception as e:
                    logger.error(f"[ERROR] Batch {batch_num}: L·ªói kh√¥ng mong ƒë·ª£i: {str(e)}")
                    success = False
                    break
            
        except Exception as e:
            logger.error(f"[ERROR] L·ªói khi g·ª≠i batch {batch_num}: {str(e)}")
            success = False
    
    if success:
        logger.info(f"[OK] ƒê√£ g·ª≠i th√†nh c√¥ng {len(normalized_articles)} b√†i vi·∫øt t·ªõi backend")
    else:
        logger.warning("[WARN] C√≥ l·ªói x·∫£y ra khi g·ª≠i b√†i vi·∫øt t·ªõi backend")
    
    return success

def cleanup_temp_files():
    """
    D·ªçn d·∫πp c√°c file t·∫°m th·ªùi
    """
    if os.path.exists(TEMP_DIR):
        try:
            shutil.rmtree(TEMP_DIR)
            logger.info(f"[OK] ƒê√£ d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {TEMP_DIR}")
        except Exception as e:
            logger.error(f"[ERROR] L·ªói khi d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {str(e)}")

def cleanup_old_files(retention_days=RETENTION_DAYS):
    """
    D·ªçn d·∫πp c√°c file c≈© trong th∆∞ m·ª•c output
    
    Args:
        retention_days (int): S·ªë ng√†y gi·ªØ l·∫°i files (m·∫∑c ƒë·ªãnh: 7)
    """
    if not os.path.exists(OUTPUT_DIR):
        return
    
    cutoff_date = time.time() - (retention_days * 24 * 60 * 60)
    logger.info(f"D·ªçn d·∫πp c√°c file c≈© h∆°n {retention_days} ng√†y trong th∆∞ m·ª•c {OUTPUT_DIR}")
    
    count = 0
    for filename in os.listdir(OUTPUT_DIR):
        filepath = os.path.join(OUTPUT_DIR, filename)
        if os.path.isfile(filepath):
            file_time = os.path.getmtime(filepath)
            if file_time < cutoff_date:
                try:
                    os.remove(filepath)
                    count += 1
                    logger.debug(f"ƒê√£ x√≥a file c≈©: {filename}")
                except Exception as e:
                    logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {filename}: {str(e)}")
    
    if count > 0:
        logger.info(f"ƒê√£ x√≥a {count} file c≈©")
    else:
        logger.info("Kh√¥ng c√≥ file n√†o c·∫ßn x√≥a")

def search_articles():
    """
    T√¨m ki·∫øm b√†i vi·∫øt t·ª´ nhi·ªÅu ngu·ªìn API kh√°c nhau
    
    Returns:
        tuple: (articles, search_file) - Danh s√°ch b√†i vi·∫øt v√† ƒë∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
    """
    logger.info("[STEP 1] T√¨m ki·∫øm b√†i vi·∫øt t·ª´ c√°c ngu·ªìn tin t·ª©c")
    
    try:
        # Chu·∫©n b·ªã bi·∫øn ƒë·ªÉ l∆∞u tr·ªØ k·∫øt qu·∫£
        articles = []
        search_file = None
        
        # L·∫•y danh s√°ch categories t·ª´ API
        categories = google_news_serpapi.get_categories()
        
        if not categories:
            logger.warning("[WARN] Kh√¥ng l·∫•y ƒë∆∞·ª£c danh s√°ch th·ªÉ lo·∫°i t·ª´ API")
            # S·ª≠ d·ª•ng danh s√°ch th·ªÉ lo·∫°i m·∫∑c ƒë·ªãnh
            categories = [
                "Ch√≠nh tr·ªã", "Kinh t·∫ø", "X√£ h·ªôi", "Ph√°p lu·∫≠t", 
                "Th·∫ø gi·ªõi", "VƒÉn h√≥a", "Gi√°o d·ª•c", "Y t·∫ø", 
                "Khoa h·ªçc", "C√¥ng ngh·ªá", "Th·ªÉ thao", "Gi·∫£i tr√≠"
            ]
            logger.info(f"[INFO] S·ª≠ d·ª•ng danh s√°ch th·ªÉ lo·∫°i m·∫∑c ƒë·ªãnh: {categories}")
        else:
            logger.info(f"[INFO] ƒê√£ l·∫•y ƒë∆∞·ª£c {len(categories)} th·ªÉ lo·∫°i t·ª´ API")
        
        # T√¨m ki·∫øm b√†i vi·∫øt cho t·ª´ng th·ªÉ lo·∫°i
        all_articles = []
        
        # T√¨m ki·∫øm t·ª´ c√°c ngu·ªìn kh√°c nhau
        sources = [
            {"name": "Google News", "module": google_news_serpapi}
        ]
        
        # Th√™m WorldNewsAPI n·∫øu module t·ªìn t·∫°i
        try:
            import worldnews_api
            sources.append({"name": "WorldNewsAPI", "module": worldnews_api})
            logger.info("[INFO] ƒê√£ th√™m ngu·ªìn WorldNewsAPI")
        except ImportError:
            logger.warning("[WARN] Kh√¥ng th·ªÉ import module worldnews_api")
        
        # Th√™m Currents API n·∫øu module t·ªìn t·∫°i
        try:
            import currents_api
            sources.append({"name": "Currents API", "module": currents_api})
            logger.info("[INFO] ƒê√£ th√™m ngu·ªìn Currents API")
        except ImportError:
            logger.warning("[WARN] Kh√¥ng th·ªÉ import module currents_api")
        
        # Hi·ªÉn th·ªã t·ªïng s·ªë ngu·ªìn ƒë√£ k√≠ch ho·∫°t
        logger.info(f"[INFO] T√¨m ki·∫øm b√†i vi·∫øt t·ª´ {len(sources)} ngu·ªìn: {', '.join([s['name'] for s in sources])}")
        
        # T√¨m ki·∫øm b√†i vi·∫øt t·ª´ m·ªói ngu·ªìn cho t·ª´ng danh m·ª•c
        for category in categories:
            logger.info(f"\n=== ƒêang x·ª≠ l√Ω danh m·ª•c: {category} ===")
            
            category_articles = []
            for source in sources:
                source_name = source["name"]
                source_module = source["module"]
                
                try:
                    logger.info(f"[INFO] T√¨m ki·∫øm b√†i vi·∫øt cho danh m·ª•c '{category}' t·ª´ ngu·ªìn {source_name}")
                    source_articles = source_module.fetch_articles_by_category(category)
                    
                    if source_articles:
                        logger.info(f"[OK] T√¨m th·∫•y {len(source_articles)} b√†i vi·∫øt cho danh m·ª•c '{category}' t·ª´ ngu·ªìn {source_name}")
                        category_articles.extend(source_articles)
                    else:
                        logger.warning(f"[WARN] Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o cho danh m·ª•c '{category}' t·ª´ ngu·ªìn {source_name}")
                
                except Exception as e:
                    logger.error(f"[ERROR] L·ªói khi t√¨m ki·∫øm b√†i vi·∫øt t·ª´ ngu·ªìn {source_name} cho danh m·ª•c '{category}': {str(e)}")
            
            # Hi·ªÉn th·ªã t·ªïng s·ªë b√†i vi·∫øt ƒë√£ t√¨m th·∫•y cho danh m·ª•c
            if category_articles:
                logger.info(f"[OK] T√¨m th·∫•y t·ªïng c·ªông {len(category_articles)} b√†i vi·∫øt cho danh m·ª•c '{category}'")
                all_articles.extend(category_articles)
            else:
                logger.warning(f"[WARN] Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o cho danh m·ª•c '{category}' t·ª´ t·∫•t c·∫£ c√°c ngu·ªìn")
        
        if all_articles:
            # X√≥a c√°c b√†i vi·∫øt tr√πng l·∫∑p d·ª±a tr√™n URL
            unique_urls = set()
            articles = []
            
            for article in all_articles:
                url = article.get("source_url", "")
                if url and url not in unique_urls:
                    unique_urls.add(url)
                    articles.append(article)
            
            # L∆∞u danh s√°ch b√†i vi·∫øt v√†o file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            search_file = os.path.join(OUTPUT_DIR, f"search_results_{timestamp}.json")
            
            if save_articles_to_file(articles, search_file):
                logger.info(f"[OK] ƒê√£ t√¨m th·∫•y v√† l∆∞u {len(articles)} b√†i vi·∫øt ƒë·ªôc nh·∫•t v√†o {search_file}")
            else:
                logger.error("[ERROR] Kh√¥ng th·ªÉ l∆∞u k·∫øt qu·∫£ t√¨m ki·∫øm")
        else:
            logger.warning("[WARN] Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o t·ª´ t·∫•t c·∫£ c√°c danh m·ª•c v√† ngu·ªìn")
        
        return articles, search_file
    
    except Exception as e:
        logger.error(f"[ERROR] L·ªói trong qu√° tr√¨nh t√¨m ki·∫øm b√†i vi·∫øt: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return [], None

def load_articles_from_file(input_file):
    """
    ƒê·ªçc d·ªØ li·ªáu b√†i vi·∫øt t·ª´ file JSON
    
    Args:
        input_file (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON
        
    Returns:
        tuple: (articles, file_path) - Danh s√°ch b√†i vi·∫øt v√† ƒë∆∞·ªùng d·∫´n file
    """
    try:
        logger.info(f"ƒê·ªçc d·ªØ li·ªáu t·ª´ file: {input_file}")
        with open(input_file, "r", encoding="utf-8") as f:
            articles = json.load(f)
        
        if not articles:
            logger.warning(f"Kh√¥ng c√≥ b√†i vi·∫øt n√†o trong file {input_file}")
            return [], input_file
        
        # Chu·∫©n h√≥a d·ªØ li·ªáu n·∫øu c·∫ßn
        for article in articles:
            # ƒê·∫£m b·∫£o meta_data l√† ƒë·ªëi t∆∞·ª£ng Python
            if isinstance(article.get("meta_data"), str):
                try:
                    article["meta_data"] = json.loads(article["meta_data"])
                except json.JSONDecodeError:
                    article["meta_data"] = {}
        
        logger.info(f"ƒê√£ ƒë·ªçc {len(articles)} b√†i vi·∫øt t·ª´ {input_file}")
        return articles, input_file
    except Exception as e:
        logger.error(f"L·ªói khi ƒë·ªçc file {input_file}: {str(e)}")
        return [], input_file

def send_articles_to_backend(articles, batch_size=DEFAULT_BATCH_SIZE, auto_send=True):
    """
    G·ª≠i b√†i vi·∫øt t·ªõi backend API
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt
        batch_size (int): S·ªë l∆∞·ª£ng b√†i vi·∫øt g·ª≠i trong m·ªói request
        auto_send (bool): T·ª± ƒë·ªông g·ª≠i kh√¥ng c·∫ßn x√°c nh·∫≠n
    
    Returns:
        bool: Tr·∫°ng th√°i th√†nh c√¥ng
    """
    return send_to_backend(articles, batch_size, auto_send)

def extract_content_from_articles(articles):
    """
    Tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL b√†i vi·∫øt s·ª≠ d·ª•ng scrape_articles_selenium.py
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt v·ªõi URL
        
    Returns:
        list: Danh s√°ch b√†i vi·∫øt ƒë√£ tr√≠ch xu·∫•t n·ªôi dung
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ tr√≠ch xu·∫•t n·ªôi dung!")
        return []
    
    logger.info(f"[STEP 2] Tr√≠ch xu·∫•t n·ªôi dung cho {len(articles)} b√†i vi·∫øt")
    
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c output t·ªìn t·∫°i
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Thi·∫øt l·∫≠p driver Selenium
    try:
        driver = scrape_articles_selenium.setup_driver()
    except Exception as e:
        logger.error(f"[ERROR] Kh√¥ng th·ªÉ thi·∫øt l·∫≠p Selenium driver: {str(e)}")
        logger.error("Vui l√≤ng ki·ªÉm tra c√†i ƒë·∫∑t ChromeDriver v√† th·ª≠ l·∫°i.")
        return []
    
    try:
        enriched_articles = []
        errors = 0
        
        for i, article in enumerate(articles, 1):
            title = article.get("title", "Unknown title")
            url = article.get("source_url", "")
            
            if not url:
                logger.warning(f"B√†i vi·∫øt {i} kh√¥ng c√≥ URL: {title}")
                continue
            
            # Ki·ªÉm tra URL ph√π h·ª£p
            if not scrape_articles_selenium.filter_article(url):
                logger.info(f"[INFO] B·ªè qua URL kh√¥ng ph√π h·ª£p: {url}")
                continue
            
            logger.info(f"[INFO] ƒêang tr√≠ch xu·∫•t n·ªôi dung cho b√†i vi·∫øt {i}/{len(articles)}: {title}")
            
            try:
                # S·ª≠ d·ª•ng h√†m enrich_article t·ª´ scrape_articles_selenium
                enriched = scrape_articles_selenium.enrich_article(driver, article)
                if enriched:
                    # Ki·ªÉm tra xem b√†i vi·∫øt c√≥ n·ªôi dung tr√≠ch xu·∫•t kh√¥ng
                    if enriched.get("content") and len(enriched.get("content", "").strip()) > 100:
                        logger.info(f"[OK] ƒê√£ tr√≠ch xu·∫•t th√†nh c√¥ng n·ªôi dung ({len(enriched.get('content', '').split())} t·ª´)")
                        enriched_articles.append(enriched)
                    else:
                        logger.warning(f"[WARN] B√†i vi·∫øt c√≥ n·ªôi dung qu√° ng·∫Øn ho·∫∑c tr·ªëng: {url}")
                        # V·∫´n th√™m v√†o danh s√°ch n·∫øu c√≥ title
                        if enriched.get("title") and enriched.get("title") != "Unknown title":
                            enriched_articles.append(enriched)
            except Exception as e:
                logger.error(f"[ERROR] L·ªói khi tr√≠ch xu·∫•t n·ªôi dung cho URL {url}: {str(e)}")
                errors += 1
            
            # Th√™m ƒë·ªô tr·ªÖ gi·ªØa c√°c request ƒë·ªÉ tr√°nh t·∫£i qu√° m·ª©c
            if i < len(articles):
                time.sleep(2)
                
                # L∆∞u b√†i vi·∫øt ƒë√£ ƒë∆∞·ª£c l√†m gi√†u
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        enriched_file = os.path.join(OUTPUT_DIR, f"enriched_articles_{timestamp}.json")
        save_articles_to_file(enriched_articles, enriched_file)
        
        logger.info(f"[OK] ƒê√£ tr√≠ch xu·∫•t n·ªôi dung cho {len(enriched_articles)} b√†i vi·∫øt (b·ªè qua {errors} l·ªói)")
        return enriched_articles
    
    except Exception as e:
        logger.error(f"[ERROR] L·ªói trong qu√° tr√¨nh tr√≠ch xu·∫•t n·ªôi dung: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return []
    finally:
        # ƒê√≥ng driver khi ho√†n th√†nh
        try:
            driver.quit()
        except:
            pass

def main():
    """
    H√†m ch√≠nh ki·ªÉm so√°t quy tr√¨nh scraping
    """
    global OUTPUT_DIR  # Khai b√°o global ƒë·∫∑t ·ªü ƒë·∫ßu h√†m

    try:
        parser = argparse.ArgumentParser(description='Scraper cho tin t·ª©c v√† b√†i vi·∫øt.')
        parser.add_argument('--skip-search', action='store_true', help='B·ªè qua b∆∞·ªõc t√¨m ki·∫øm URL b√†i vi·∫øt')
        parser.add_argument('--skip-extraction', action='store_true', help='B·ªè qua b∆∞·ªõc tr√≠ch xu·∫•t n·ªôi dung')
        parser.add_argument('--skip-send', action='store_true', help='B·ªè qua b∆∞·ªõc g·ª≠i ƒë·∫øn backend')
        parser.add_argument('--input-file', help='File JSON ch·ª©a b√†i vi·∫øt ƒë·ªÉ x·ª≠ l√Ω')
        parser.add_argument('--auto-send', action='store_true', help='T·ª± ƒë·ªông g·ª≠i b√†i vi·∫øt kh√¥ng c·∫ßn x√°c nh·∫≠n')
        parser.add_argument('--batch-size', type=int, default=5, help='S·ªë l∆∞·ª£ng b√†i vi·∫øt g·ª≠i trong m·ªói request')
        parser.add_argument('--verbose', action='store_true', help='Hi·ªÉn th·ªã nhi·ªÅu th√¥ng tin h∆°n')
        parser.add_argument('--retention-days', type=int, default=2, help='S·ªë ng√†y gi·ªØ l·∫°i files tr∆∞·ªõc khi x√≥a')
        parser.add_argument('--no-cleanup', action='store_true', help='Kh√¥ng x√≥a files c≈©')
        parser.add_argument('--output-dir', default=OUTPUT_DIR, help='Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£')
        args = parser.parse_args()
        
        # ƒêi·ªÅu ch·ªânh m·ª©c log n·∫øu verbose
        if args.verbose:
            logger.setLevel(logging.DEBUG)
            logger.debug("Ch·∫ø ƒë·ªô verbose ƒë∆∞·ª£c b·∫≠t")
        
        # C·∫≠p nh·∫≠t th∆∞ m·ª•c output n·∫øu c·∫ßn
        if args.output_dir != OUTPUT_DIR:
            OUTPUT_DIR = args.output_dir
            logger.info(f"Th∆∞ m·ª•c output ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t: {OUTPUT_DIR}")
        
        # T·∫°o th∆∞ m·ª•c output v√† temp n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        os.makedirs(TEMP_DIR, exist_ok=True)
        
        logger.info(f"L∆∞u tr·ªØ k·∫øt qu·∫£ v√†o th∆∞ m·ª•c: {OUTPUT_DIR}")
        
        # D·ªçn d·∫πp files c≈© n·∫øu c·∫ßn
        if not args.no_cleanup:
            cleanup_old_files(args.retention_days)
            
        # Ki·ªÉm tra API keys t·ª´ file .env
        api_keys = check_api_keys()
        
        # T√¨m ki·∫øm b√†i vi·∫øt m·ªõi
        articles = []
        search_file = None
        
        if not args.skip_search:
            articles, search_file = search_articles()
        elif args.input_file:
            # S·ª≠ d·ª•ng file input ƒë√£ ch·ªâ ƒë·ªãnh
            if os.path.exists(args.input_file):
                articles, search_file = load_articles_from_file(args.input_file)
                logger.info(f"[OK] ƒê√£ t·∫£i {len(articles)} b√†i vi·∫øt t·ª´ {args.input_file}")
            else:
                logger.error(f"[ERROR] Kh√¥ng t√¨m th·∫•y file: {args.input_file}")
                return
        else:
            # T√¨m file JSON g·∫ßn nh·∫•t trong th∆∞ m·ª•c output
            search_files = sorted(glob.glob(os.path.join(OUTPUT_DIR, "search_results_*.json")), reverse=True)
            
            if search_files:
                search_file = search_files[0]
                articles, _ = load_articles_from_file(search_file)
                logger.info(f"[INFO] S·ª≠ d·ª•ng file t√¨m ki·∫øm g·∫ßn nh·∫•t: {search_file}")
                logger.info(f"[OK] ƒê√£ t·∫£i {len(articles)} b√†i vi·∫øt t·ª´ {search_file}")
            else:
                logger.error("[ERROR] Kh√¥ng t√¨m th·∫•y file t√¨m ki·∫øm n√†o trong th∆∞ m·ª•c output")
                logger.error("[ERROR] Vui l√≤ng ch·∫°y l·∫°i m√† kh√¥ng c√≥ --skip-search ho·∫∑c cung c·∫•p --input-file")
                return
        
        # Tr√≠ch xu·∫•t n·ªôi dung cho c√°c b√†i vi·∫øt
        if not args.skip_extraction and articles:
            extraction_results = extract_content_from_articles(articles)
            
            # G·ª≠i b√†i vi·∫øt ƒë·∫øn backend
            if extraction_results and not args.skip_send:
                send_articles_to_backend(extraction_results, args.batch_size, args.auto_send)
        elif not args.skip_send and articles:
            # Ki·ªÉm tra n·∫øu c√°c b√†i vi·∫øt ƒë√£ c√≥ n·ªôi dung
            articles_with_content = [a for a in articles if a.get('content')]
            
            if articles_with_content:
                send_articles_to_backend(articles_with_content, args.batch_size, args.auto_send)
            else:
                logger.warning("[WARN] Kh√¥ng c√≥ b√†i vi·∫øt n√†o c√≥ n·ªôi dung ƒë·ªÉ g·ª≠i")
                logger.info("[INFO] Vui l√≤ng ch·∫°y l·∫°i m√† kh√¥ng c√≥ --skip-extraction ƒë·ªÉ tr√≠ch xu·∫•t n·ªôi dung tr∆∞·ªõc khi g·ª≠i")
        
        logger.info(f"[OK] Ho√†n th√†nh qu√° tr√¨nh x·ª≠ l√Ω. K·∫øt qu·∫£ n·∫±m trong th∆∞ m·ª•c: {OUTPUT_DIR}")
                
    except Exception as e:
        logger.error(f"[ERROR] L·ªói kh√¥ng x√°c ƒë·ªãnh trong qu√° tr√¨nh x·ª≠ l√Ω: {str(e)}")
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    main() 