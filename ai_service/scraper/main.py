#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ƒê√¢y l√† module ch√≠nh cho scraper b√†i vi·∫øt tin t·ª©c
Quy tr√¨nh: 
1. T√¨m ki·∫øm URL b√†i vi·∫øt t·ª´ google_news.py 
2. Tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL b·∫±ng scrape_articles_selenium.py
3. L∆∞u k·∫øt qu·∫£ v√†o file JSON trong th∆∞ m·ª•c output
"""

import os
import sys
import json
import uuid
import time
import shutil
import logging
import argparse
import requests
from pathlib import Path
from datetime import datetime, timedelta
from urllib.parse import urlparse
import glob
import traceback
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import random
from dotenv import load_dotenv
import re
import mysql.connector
from mysql.connector import Error
from unidecode import unidecode

# Import c√°c module n·ªôi b·ªô
from google_news import (
    fetch_categories_from_backend,
    search_with_category,
    process_all_categories,
    save_article_to_json,
    import_article_to_backend
)
from scrape_articles_selenium import extract_article_content

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# Th√¥ng tin k·∫øt n·ªëi database
DB_HOST = os.getenv('DB_HOST', 'localhost')
DB_USER = os.getenv('DB_USER', 'root')
DB_PASSWORD = os.getenv('DB_PASSWORD', '')
DB_NAME = os.getenv('DB_NAME', 'AiMagazineDB')

# Config k·∫øt n·ªëi database
DB_CONFIG = {
    "host": "localhost",
    "user": "root",
    "password": "",
    "database": "aimagazine",
    "port": 3306
}

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(f"scraper_{datetime.now().strftime('%Y%m%d')}.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

# üîπ C√°c th√¥ng s·ªë m·∫∑c ƒë·ªãnh
SCRAPER_DIR = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.path.join(SCRAPER_DIR, "temp")
OUTPUT_DIR = os.path.join(SCRAPER_DIR, "output")
DEFAULT_BATCH_SIZE = 5
# S·ªë ng√†y ƒë·ªÉ gi·ªØ l·∫°i log files v√† output files
RETENTION_DAYS = 2

# üîπ Laravel Backend API URLs
BACKEND_API_URL = "http://localhost:8000/api/articles"
ARTICLES_BATCH_API_URL = "http://localhost:8000/api/articles/batch"
ARTICLES_IMPORT_API_URL = "http://localhost:8000/api/articles/import"
CATEGORIES_API_URL = "http://localhost:8000/api/categories"

# C√°c bi·∫øn to√†n c·ª•c
# Cache l∆∞u c√°c URL ƒë√£ x·ª≠ l√Ω ƒë·ªÉ tr√°nh tr√πng l·∫∑p
processed_urls = set()

def check_api_keys():
    """
    Ki·ªÉm tra c√°c API keys t·ª´ file .env
    """
    api_keys_status = {}
    
    # Ki·ªÉm tra WorldNewsAPI
    worldnews_api_key = os.environ.get('WORLDNEWS_API_KEY', '')
    if worldnews_api_key:
        api_keys_status['WorldNewsAPI'] = True
    else:
        api_keys_status['WorldNewsAPI'] = False
        
    # Ki·ªÉm tra Currents API
    currents_api_key = os.environ.get('CURRENTS_API_KEY', '')
    if currents_api_key:
        api_keys_status['CurrentsAPI'] = True
    else:
        api_keys_status['CurrentsAPI'] = False
    
    # In th√¥ng tin
    logger.info("=== Tr·∫°ng th√°i API keys ===")
    for api_name, status in api_keys_status.items():
        if status:
            logger.info(f"‚úÖ {api_name}: OK")
        else:
            logger.warning(f"‚ö†Ô∏è {api_name}: Kh√¥ng t√¨m th·∫•y API key")
    
    return api_keys_status

def save_articles_to_file(articles, output_file=None):
    """
    L∆∞u danh s√°ch b√†i vi·∫øt v√†o file JSON
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt
        output_file (str): ƒê∆∞·ªùng d·∫´n file ƒë·∫ßu ra
        
    Returns:
        str: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ƒë√£ l∆∞u
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ l∆∞u!")
        return None
    
    # T·∫°o th∆∞ m·ª•c ƒë·∫ßu ra n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # T·∫°o t√™n file m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
    if not output_file:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(OUTPUT_DIR, f"enriched_articles_{timestamp}.json")
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u
    normalized_articles = []
    for article in articles:
        normalized = article.copy()
        
        # ƒê·∫£m b·∫£o source_name l√† chu·ªói
        if isinstance(normalized.get("source_name"), dict):
            normalized["source_name"] = normalized["source_name"].get("name", "Unknown Source")
        
        # ƒê·∫£m b·∫£o meta_data l√† chu·ªói JSON
        if isinstance(normalized.get("meta_data"), dict):
            normalized["meta_data"] = json.dumps(normalized["meta_data"])
        
        # ƒê·∫£m b·∫£o summary kh√¥ng bao gi·ªù l√† None
        if normalized.get("summary") is None:
            normalized["summary"] = ""
        
        # ƒê·∫£m b·∫£o content kh√¥ng bao gi·ªù l√† None
        if normalized.get("content") is None:
            normalized["content"] = ""
        
        # ƒê·∫£m b·∫£o published_at c√≥ ƒë·ªãnh d·∫°ng chu·∫©n
        if normalized.get("published_at"):
            try:
                # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y th√°ng
                date_obj = datetime.fromisoformat(normalized["published_at"].replace('Z', '+00:00'))
                normalized["published_at"] = date_obj.isoformat()
            except (ValueError, TypeError):
                # N·∫øu kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        
        normalized_articles.append(normalized)
    
    # L∆∞u b√†i vi·∫øt v√†o file JSON
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(normalized_articles, f, ensure_ascii=False, indent=4)
    
    logger.info(f"[OK] ƒê√£ l∆∞u {len(normalized_articles)} b√†i vi·∫øt v√†o {output_file}")
    return output_file

def send_to_backend(articles, batch_size=DEFAULT_BATCH_SIZE, auto_send=True):
    """
    G·ª≠i b√†i vi·∫øt t·ªõi backend API
    
    Args:
        articles (list): Danh s√°ch b√†i vi·∫øt
        batch_size (int): S·ªë l∆∞·ª£ng b√†i vi·∫øt g·ª≠i trong m·ªói request
        auto_send (bool): T·ª± ƒë·ªông g·ª≠i kh√¥ng c·∫ßn x√°c nh·∫≠n (lu√¥n True)
        
    Returns:
        bool: Tr·∫°ng th√°i th√†nh c√¥ng
    """
    if not articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë·ªÉ g·ª≠i!")
        return False
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi g·ª≠i
    normalized_articles = []
    for article in articles:
        normalized = article.copy()
        
        # ƒê·∫£m b·∫£o source_name l√† chu·ªói
        if isinstance(normalized.get("source_name"), dict):
            normalized["source_name"] = normalized["source_name"].get("name", "Unknown Source")
        
        # ƒê·∫£m b·∫£o meta_data l√† chu·ªói JSON
        if isinstance(normalized.get("meta_data"), dict):
            try:
                normalized["meta_data"] = json.dumps(normalized["meta_data"], ensure_ascii=False)
            except Exception as e:
                logger.warning(f"L·ªói khi chuy·ªÉn ƒë·ªïi meta_data sang JSON: {str(e)}")
                normalized["meta_data"] = json.dumps({"error": "Invalid metadata"})
        
        # ƒê·∫£m b·∫£o summary kh√¥ng bao gi·ªù l√† None
        if normalized.get("summary") is None:
            normalized["summary"] = ""
        
        # ƒê·∫£m b·∫£o content kh√¥ng bao gi·ªù l√† None
        if normalized.get("content") is None:
            normalized["content"] = ""
        
        # ƒê·∫£m b·∫£o published_at c√≥ ƒë·ªãnh d·∫°ng chu·∫©n
        if normalized.get("published_at"):
            try:
                # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y th√°ng
                date_obj = datetime.fromisoformat(normalized["published_at"].replace('Z', '+00:00'))
                normalized["published_at"] = date_obj.isoformat()
            except (ValueError, TypeError):
                # N·∫øu kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        else:
            # N·∫øu kh√¥ng c√≥ published_at, s·ª≠ d·ª•ng th·ªùi gian hi·ªán t·∫°i
                normalized["published_at"] = datetime.now().isoformat()
        
        normalized_articles.append(normalized)
    
    logger.info(f"G·ª≠i {len(normalized_articles)} b√†i vi·∫øt t·ªõi backend...")
    
    success = True
    total_batches = (len(normalized_articles) + batch_size - 1) // batch_size
    
    for i in range(0, len(normalized_articles), batch_size):
        batch = normalized_articles[i:i+batch_size]
        batch_num = i // batch_size + 1
        
        logger.info(f"G·ª≠i batch {batch_num}/{total_batches} ({len(batch)} b√†i vi·∫øt)")
        
        try:
            payload = {"articles": batch}
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json"
            }
            
            # In payload m·∫´u ƒë·ªÉ ki·ªÉm tra
            if batch:
                sample = batch[0].copy()
                if 'content' in sample and sample['content'] and len(sample['content']) > 100:
                    sample['content'] = sample['content'][:100] + "..."
                logger.debug(f"Payload m·∫´u (b√†i vi·∫øt ƒë·∫ßu ti√™n): {json.dumps(sample, ensure_ascii=False)}")
            
            # Thi·∫øt l·∫≠p timeout v√† s·ªë l·∫ßn th·ª≠ l·∫°i
            max_retries = 3
            retry_count = 0
            
            while retry_count < max_retries:
                try:
                    response = requests.post(
                        BACKEND_API_URL, 
                        json=payload, 
                        headers=headers,
                        timeout=30  # 30 gi√¢y timeout
                    )
                    
                    # Ki·ªÉm tra xem response c√≥ ph·∫£i JSON kh√¥ng
                    response_content = response.text
                    logger.debug(f"Response status: {response.status_code}")
                    
                    if response.status_code in (200, 201):
                        try:
                            result = response.json()
                            logger.info(f"[OK] Batch {batch_num}: {result.get('message', '')}")
                            if 'errors' in result and result['errors']:
                                logger.warning(f"[WARN] C√≥ {len(result['errors'])} l·ªói trong batch {batch_num}:")
                                for error in result['errors']:
                                    logger.warning(f"  - {error}")
                        except json.JSONDecodeError:
                            logger.warning(f"[WARN] Response kh√¥ng ph·∫£i JSON h·ª£p l·ªá m·∫∑c d√π status code = {response.status_code}")
                            logger.warning(f"[WARN] Response content: {response_content[:200]}...")
                        break
                    else:
                        logger.error(f"[ERROR] Batch {batch_num}: L·ªói {response.status_code} - {response.text[:200]}...")
                        retry_count += 1
                        if retry_count < max_retries:
                            wait_time = 2 ** retry_count  # TƒÉng th·ªùi gian ch·ªù theo c·∫•p s·ªë nh√¢n
                            logger.info(f"Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                            time.sleep(wait_time)
                        else:
                            success = False
                            logger.error(f"[ERROR] ƒê√£ th·ª≠ l·∫°i {max_retries} l·∫ßn kh√¥ng th√†nh c√¥ng cho batch {batch_num}")
                
                except requests.exceptions.Timeout:
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = 2 ** retry_count
                        logger.info(f"Request timeout. Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                    else:
                        success = False
                        logger.error(f"[ERROR] Timeout sau {max_retries} l·∫ßn th·ª≠ l·∫°i cho batch {batch_num}")
                
                except requests.exceptions.ConnectionError:
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = 2 ** retry_count
                        logger.info(f"L·ªói k·∫øt n·ªëi. Th·ª≠ l·∫°i sau {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                    else:
                        success = False
                        logger.error(f"[ERROR] L·ªói k·∫øt n·ªëi sau {max_retries} l·∫ßn th·ª≠ l·∫°i cho batch {batch_num}")
                
                except Exception as e:
                    logger.error(f"[ERROR] Batch {batch_num}: L·ªói kh√¥ng mong ƒë·ª£i: {str(e)}")
                    success = False
                    break
            
        except Exception as e:
            logger.error(f"[ERROR] L·ªói khi g·ª≠i batch {batch_num}: {str(e)}")
            success = False
    
    if success:
        logger.info(f"[OK] ƒê√£ g·ª≠i th√†nh c√¥ng {len(normalized_articles)} b√†i vi·∫øt t·ªõi backend")
    else:
        logger.warning("[WARN] C√≥ l·ªói x·∫£y ra khi g·ª≠i b√†i vi·∫øt t·ªõi backend")
    
    return success

def cleanup_temp_files():
    """
    D·ªçn d·∫πp c√°c file t·∫°m th·ªùi
    """
    if os.path.exists(TEMP_DIR):
        try:
            shutil.rmtree(TEMP_DIR)
            logger.info(f"[OK] ƒê√£ d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {TEMP_DIR}")
        except Exception as e:
            logger.error(f"[ERROR] L·ªói khi d·ªçn d·∫πp th∆∞ m·ª•c t·∫°m: {str(e)}")

def cleanup_old_files(retention_days=RETENTION_DAYS):
    """
    D·ªçn d·∫πp c√°c file c≈© trong th∆∞ m·ª•c output
    
    Args:
        retention_days (int): S·ªë ng√†y gi·ªØ l·∫°i files (m·∫∑c ƒë·ªãnh: 2)
    """
    if not os.path.exists(OUTPUT_DIR):
        return
    
    cutoff_date = time.time() - (retention_days * 24 * 60 * 60)
    logger.info(f"D·ªçn d·∫πp c√°c file c≈© h∆°n {retention_days} ng√†y trong th∆∞ m·ª•c {OUTPUT_DIR}")
    
    count = 0
    for filename in os.listdir(OUTPUT_DIR):
        filepath = os.path.join(OUTPUT_DIR, filename)
        if os.path.isfile(filepath):
            file_time = os.path.getmtime(filepath)
            if file_time < cutoff_date:
                try:
                    os.remove(filepath)
                    count += 1
                    logger.debug(f"ƒê√£ x√≥a file c≈©: {filename}")
                except Exception as e:
                    logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {filename}: {str(e)}")
    
    if count > 0:
        logger.info(f"ƒê√£ x√≥a {count} file c≈©")
    else:
        logger.info("Kh√¥ng c√≥ file n√†o c·∫ßn x√≥a")

def process_category(category_id, category_name, max_articles=2):
    """
    X·ª≠ l√Ω m·ªôt danh m·ª•c c·ª• th·ªÉ v√† tr√≠ch xu·∫•t b√†i vi·∫øt
    
    Args:
        category_id: ID c·ªßa danh m·ª•c
        category_name: T√™n c·ªßa danh m·ª•c
        max_articles: S·ªë b√†i vi·∫øt t·ªëi ƒëa c·∫ßn l·∫•y
        
    Returns:
        dict: Danh s√°ch c√°c b√†i vi·∫øt ƒë√£ t√¨m th·∫•y cho danh m·ª•c
    """
    articles = []
    
    for i in range(1, max_articles + 1):
        logger.info(f"T√¨m ki·∫øm b√†i vi·∫øt th·ª© {i}/{max_articles} cho danh m·ª•c: {category_name}")
        
        try:
            article_data = search_with_category(category_id)
            
            if article_data and "url" in article_data:
                article_url = article_data["url"]
                
                # Ki·ªÉm tra xem URL n√†y ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ch∆∞a
                if article_url in processed_urls:
                    logger.warning(f"URL ƒë√£ x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥, b·ªè qua: {article_url}")
                    continue
                
                # ƒê√°nh d·∫•u URL n√†y ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
                processed_urls.add(article_url)
                
                articles.append(article_data)
                
                logger.info(f"ƒê√£ t√¨m th·∫•y b√†i vi·∫øt: {article_data.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')}")
                logger.info(f"URL: {article_url}")
                logger.info(f"ƒê√£ l∆∞u v√†o: {article_data.get('file_path', 'Kh√¥ng l∆∞u file')}")
            else:
                logger.warning(f"Kh√¥ng t√¨m th·∫•y th√™m b√†i vi·∫øt n√†o cho danh m·ª•c: {category_name}")
                break
            
            # Th√™m kho·∫£ng th·ªùi gian ngh·ªâ gi·ªØa c√°c l·∫ßn t√¨m ki·∫øm ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
            if i < max_articles:
                time.sleep(2)
                
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω b√†i vi·∫øt th·ª© {i} cho danh m·ª•c {category_name}: {str(e)}")
            logger.error(traceback.format_exc())
    
    return articles

def find_and_process_all_categories(max_articles_per_category=2):
    """
    T√¨m ki·∫øm v√† x·ª≠ l√Ω b√†i vi·∫øt cho t·∫•t c·∫£ c√°c danh m·ª•c t·ª´ backend
    :param max_articles_per_category: S·ªë b√†i vi·∫øt t·ªëi ƒëa cho m·ªói danh m·ª•c
    :return: K·∫øt qu·∫£ t√¨m ki·∫øm
    """
    logger.info(f"B·∫Øt ƒë·∫ßu t√¨m ki·∫øm v√† x·ª≠ l√Ω b√†i vi·∫øt cho c√°c danh m·ª•c (t·ªëi ƒëa {max_articles_per_category} b√†i/danh m·ª•c)")
    
    # L·∫•y danh s√°ch danh m·ª•c t·ª´ backend
    categories = fetch_categories_from_backend()
    
    if not categories:
        logger.error("Kh√¥ng th·ªÉ l·∫•y danh m·ª•c t·ª´ backend")
        return {"success": 0, "failed": 0, "articles_by_category": {}, "all_articles": [], "total_articles": 0}
    
    logger.info(f"T√¨m th·∫•y {len(categories)} danh m·ª•c t·ª´ backend")
    
    # K·∫øt qu·∫£
    results = {
        "success": 0,
        "failed": 0,
        "articles_by_category": {},
        "all_articles": [],
        "total_articles": 0
    }
    
    # X·ª≠ l√Ω l·∫ßn l∆∞·ª£t t·ª´ng danh m·ª•c
    for category in categories:
        category_id = category["id"]
        category_name = category["name"]
        
        try:
            articles = process_category(category_id, category_name, max_articles_per_category)
            
            if articles:
                # C·∫≠p nh·∫≠t k·∫øt qu·∫£ th√†nh c√¥ng
                results["success"] += 1
                results["articles_by_category"][category_name] = articles
                results["all_articles"].extend(articles)
                results["total_articles"] += len(articles)
            else:
                # Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o
                results["failed"] += 1
                
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω danh m·ª•c {category_name}: {str(e)}")
            logger.error(traceback.format_exc())
            results["failed"] += 1
    
    return results

def import_all_to_backend(directory=None):
    """
    Import t·∫•t c·∫£ c√°c file JSON t·ª´ th∆∞ m·ª•c output v√†o backend
    
    Args:
        directory (str): Th∆∞ m·ª•c ch·ª©a c√°c file JSON (m·∫∑c ƒë·ªãnh: OUTPUT_DIR)
        
    Returns:
        tuple: (success_count, failed_count) - S·ªë l∆∞·ª£ng file th√†nh c√¥ng v√† th·∫•t b·∫°i
    """
    if directory is None:
        directory = OUTPUT_DIR
    
    if not os.path.exists(directory):
        logger.error(f"Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {directory}")
        return 0, 0
    
    logger.info(f"B·∫Øt ƒë·∫ßu import t·∫•t c·∫£ c√°c file JSON t·ª´ th∆∞ m·ª•c: {directory}")
    
    # T√¨m t·∫•t c·∫£ c√°c file JSON trong th∆∞ m·ª•c
    json_files = [f for f in os.listdir(directory) if f.endswith('.json')]
    
    if not json_files:
        logger.warning(f"Kh√¥ng t√¨m th·∫•y file JSON n√†o trong th∆∞ m·ª•c: {directory}")
        return 0, 0
    
    logger.info(f"T√¨m th·∫•y {len(json_files)} file JSON")
    
    success_count = 0
    failed_count = 0
    
    for json_file in json_files:
        filepath = os.path.join(directory, json_file)
        logger.info(f"ƒêang x·ª≠ l√Ω file: {json_file}")
        
        try:
            # ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Ki·ªÉm tra xem d·ªØ li·ªáu c√≥ ph·∫£i l√† dictionary v·ªõi c√°c tr∆∞·ªùng c·∫ßn thi·∫øt kh√¥ng
            if isinstance(data, dict) and 'category_id' in data and 'title' in data and 'content' in data:
                # Import b√†i vi·∫øt v√†o backend
                result = import_article_to_backend(
                    category_id=data['category_id'],
                    article_url=data.get('url', ''),
                    title=data['title'],
                    content=data['content']
                )
                
                if result:
                    logger.info(f"Import th√†nh c√¥ng file: {json_file}")
                    success_count += 1
                    # X√≥a file JSON sau khi import th√†nh c√¥ng
                    try:
                        os.remove(filepath)
                        logger.info(f"ƒê√£ x√≥a file JSON sau khi import th√†nh c√¥ng: {json_file}")
                    except Exception as e:
                        logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {json_file}: {str(e)}")
                else:
                    logger.error(f"Import th·∫•t b·∫°i file: {json_file}")
                    failed_count += 1
            else:
                logger.warning(f"File kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng: {json_file}")
                failed_count += 1
        
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω file {json_file}: {str(e)}")
            failed_count += 1
    
    logger.info(f"K·∫øt qu·∫£ import: Th√†nh c√¥ng: {success_count}, Th·∫•t b·∫°i: {failed_count}")
    return success_count, failed_count

def save_all_articles_to_single_file(results):
    """
    L∆∞u t·∫•t c·∫£ b√†i vi·∫øt t·ª´ t·∫•t c·∫£ c√°c danh m·ª•c v√†o m·ªôt file JSON duy nh·∫•t
    :param results: K·∫øt qu·∫£ t·ª´ h√†m find_and_process_all_categories
    :return: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON ƒë√£ l∆∞u
    """
    logger.info(f"B·∫Øt ƒë·∫ßu l∆∞u t·∫•t c·∫£ b√†i vi·∫øt v√†o file JSON duy nh·∫•t, d·ªØ li·ªáu ƒë·∫ßu v√†o: {type(results)}")
    
    # Ki·ªÉm tra d·ªØ li·ªáu ƒë·∫ßu v√†o
    if not isinstance(results, dict):
        logger.error(f"D·ªØ li·ªáu ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá: {type(results)}")
        return None
    
    # Log c·∫•u tr√∫c c·ªßa results ƒë·ªÉ debug
    logger.info(f"C·∫•u tr√∫c results: {list(results.keys())}")
    
    # ∆Øu ti√™n s·ª≠ d·ª•ng all_articles n·∫øu c√≥
    all_articles = []
    
    if "all_articles" in results and isinstance(results["all_articles"], list) and results["all_articles"]:
        all_articles_raw = results["all_articles"]
        logger.info(f"S·ª≠ d·ª•ng {len(all_articles_raw)} b√†i vi·∫øt t·ª´ all_articles")
        
        # Thu th·∫≠p b√†i vi·∫øt v·ªõi n·ªôi dung ƒë·∫ßy ƒë·ªß
        for article in all_articles_raw:
            if not isinstance(article, dict):
                logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng h·ª£p l·ªá: {type(article)}")
                continue
                
            # ƒê·ªçc n·ªôi dung t·ª´ file JSON n·∫øu b√†i vi·∫øt kh√¥ng c√≥ tr∆∞·ªùng 'content'
            article_copy = article.copy()
            if "content" not in article_copy and "json_filepath" in article_copy:
                try:
                    json_content = read_json_file(article_copy["json_filepath"])
                    if json_content and "content" in json_content:
                        # C·∫≠p nh·∫≠t n·ªôi dung t·ª´ file JSON
                        article_copy.update(json_content)
                        logger.info(f"ƒê√£ l·∫•y n·ªôi dung t·ª´ file: {article_copy['json_filepath']}")
                except Exception as e:
                    logger.error(f"L·ªói khi ƒë·ªçc file {article_copy['json_filepath']}: {str(e)}")
            
            # Th√™m v√†o danh s√°ch n·∫øu c√≥ n·ªôi dung
            if article_copy.get("content"):
                all_articles.append(article_copy)
            else:
                logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng c√≥ n·ªôi dung: {article_copy.get('title', 'Kh√¥ng ti√™u ƒë·ªÅ')}")
    else:
        # Thu th·∫≠p b√†i vi·∫øt t·ª´ c·∫•u tr√∫c articles_by_category
        if "articles_by_category" in results and isinstance(results["articles_by_category"], dict):
            articles_by_category = results["articles_by_category"]
            
            # Thu th·∫≠p b√†i vi·∫øt t·ª´ m·ªói danh m·ª•c
            for category_name, articles in articles_by_category.items():
                if not isinstance(articles, list):
                    logger.warning(f"B·ªè qua danh m·ª•c {category_name} v√¨ d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá: {type(articles)}")
                    continue
                
                logger.info(f"X·ª≠ l√Ω {len(articles)} b√†i vi·∫øt t·ª´ danh m·ª•c {category_name}")
                
                for article in articles:
                    if not isinstance(article, dict):
                        logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng h·ª£p l·ªá: {type(article)}")
                        continue
                    
                    # T·∫°o b·∫£n sao v√† th√™m th√¥ng tin danh m·ª•c
                    article_copy = article.copy()
                    article_copy["category_name"] = category_name
                    
                    # ƒê·ªçc n·ªôi dung t·ª´ file JSON n·∫øu b√†i vi·∫øt kh√¥ng c√≥ tr∆∞·ªùng 'content'
                    if "content" not in article_copy and "json_filepath" in article_copy:
                        try:
                            json_content = read_json_file(article_copy["json_filepath"])
                            if json_content and "content" in json_content:
                                # C·∫≠p nh·∫≠t n·ªôi dung t·ª´ file JSON
                                article_copy.update(json_content)
                                logger.info(f"ƒê√£ l·∫•y n·ªôi dung t·ª´ file: {article_copy['json_filepath']}")
                        except Exception as e:
                            logger.error(f"L·ªói khi ƒë·ªçc file {article_copy['json_filepath']}: {str(e)}")
                    
                    # Th√™m v√†o danh s√°ch n·∫øu c√≥ n·ªôi dung
                    if article_copy.get("content"):
                        all_articles.append(article_copy)
                    else:
                        logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng c√≥ n·ªôi dung: {article_copy.get('title', 'Kh√¥ng ti√™u ƒë·ªÅ')}")
    
    # Ki·ªÉm tra s·ªë l∆∞·ª£ng b√†i vi·∫øt sau khi thu th·∫≠p
    if not all_articles:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt h·ª£p l·ªá ƒë·ªÉ l∆∞u v√†o file JSON duy nh·∫•t")
        return None
    
    # T·∫°o t√™n file duy nh·∫•t
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    unique_id = str(uuid.uuid4())[:8]  # L·∫•y 8 k√Ω t·ª± ƒë·∫ßu c·ªßa UUID
    output_file = os.path.join(OUTPUT_DIR, f"all_articles_{current_time}_{unique_id}.json")
    
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c output t·ªìn t·∫°i
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # L∆∞u v√†o file JSON
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_articles, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ƒê√£ l∆∞u {len(all_articles)} b√†i vi·∫øt v√†o file JSON duy nh·∫•t: {output_file}")
        
        # In th√¥ng tin b√†i vi·∫øt ƒë·∫ßu ti√™n ƒë·ªÉ debug
        if all_articles:
            first_article = all_articles[0]
            logger.info(f"B√†i vi·∫øt m·∫´u: Title={first_article.get('title', 'Kh√¥ng ti√™u ƒë·ªÅ')}, Content length={len(first_article.get('content', ''))}")
            
        return output_file
    except Exception as e:
        logger.error(f"L·ªói khi l∆∞u file JSON: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def read_json_file(file_path):
    """
    ƒê·ªçc n·ªôi dung t·ª´ file JSON
    :param file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON
    :return: D·ªØ li·ªáu t·ª´ file JSON ho·∫∑c None n·∫øu l·ªói
    """
    if not os.path.exists(file_path):
        logger.error(f"File kh√¥ng t·ªìn t·∫°i: {file_path}")
        return None
        
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        logger.error(f"L·ªói khi ƒë·ªçc file JSON: {str(e)}")
        return None

def import_json_file_to_backend(json_file_path):
    """
    Import t·∫•t c·∫£ b√†i vi·∫øt t·ª´ file JSON v√†o backend
    :param json_file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON
    :return: tuple (s·ªë b√†i vi·∫øt import th√†nh c√¥ng, s·ªë b√†i vi·∫øt th·∫•t b·∫°i)
    """
    logger.info(f"ƒêang import b√†i vi·∫øt t·ª´ file: {json_file_path}")
    
    if not os.path.exists(json_file_path):
        logger.error(f"File kh√¥ng t·ªìn t·∫°i: {json_file_path}")
        return 0, 0
    
    # ƒê·ªçc file JSON
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        logger.error(f"L·ªói khi ƒë·ªçc file JSON: {str(e)}")
        logger.error(traceback.format_exc())
        return 0, 0
    
    # Ki·ªÉm tra d·ªØ li·ªáu l√† list
    if not isinstance(data, list):
        logger.error(f"D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá, ph·∫£i l√† list: {type(data)}")
        return 0, 0
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ g·ª≠i l√™n API
    articles_to_import = []
    
    for article in data:
        if not isinstance(article, dict):
            logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng h·ª£p l·ªá, kh√¥ng ph·∫£i dict: {type(article)}")
            continue
            
        # ƒê·ªçc n·ªôi dung t·ª´ file l∆∞u tr·ªØ n·∫øu c·∫ßn
        article_content = article.get("content", "")
        if not article_content and "json_filepath" in article:
            json_data = read_json_file(article["json_filepath"])
            if json_data and isinstance(json_data, dict):
                article_content = json_data.get("content", "")
                # C·∫≠p nh·∫≠t th√¥ng tin kh√°c t·ª´ file JSON
                if not article.get("title") and json_data.get("title"):
                    article["title"] = json_data["title"]
                if not article.get("summary") and json_data.get("summary"):
                    article["summary"] = json_data["summary"]
                
        # Chu·∫©n b·ªã d·ªØ li·ªáu b√†i vi·∫øt
        article_data = {
            "title": article.get("title", ""),
            "slug": generate_slug(article.get("title", "Kh√¥ng ti√™u ƒë·ªÅ")),
            "summary": article.get("summary", ""),
            "content": article_content,
            "source_name": article.get("source_name", ""),
            "source_url": article.get("source_url", article.get("url", "")),
            "source_icon": article.get("source_icon", ""),
            "published_at": article.get("published_at", datetime.now().isoformat()),
            "meta_data": article.get("meta_data", {}),
            "category": article.get("category_id", 1)  # ƒê·∫£m b·∫£o s·ª≠ d·ª•ng category, kh√¥ng ph·∫£i category_id
        }
        
        # T·∫°o summary t·ª´ n·ªôi dung n·∫øu kh√¥ng c√≥
        if not article_data["summary"] and article_data["content"]:
            sentences = re.split(r'[.!?]+', article_data["content"])
            if len(sentences) >= 2:
                article_data["summary"] = '. '.join(s.strip() for s in sentences[:2] if s.strip()) + '.'
            else:
                article_data["summary"] = sentences[0] if sentences else ""
                
        # T·∫°o source_name t·ª´ URL n·∫øu kh√¥ng c√≥
        if not article_data["source_name"] and article_data["source_url"]:
            try:
                parsed_url = urlparse(article_data["source_url"])
                article_data["source_name"] = parsed_url.netloc
            except:
                article_data["source_name"] = "Kh√¥ng r√µ"
                
        # T·∫°o source_icon t·ª´ domain n·∫øu kh√¥ng c√≥
        if not article_data["source_icon"] and article_data["source_url"]:
            try:
                parsed_url = urlparse(article_data["source_url"])
                domain = parsed_url.netloc
                article_data["source_icon"] = f"https://www.google.com/s2/favicons?domain={domain}"
            except:
                article_data["source_icon"] = ""
        
        # Ki·ªÉm tra n·ªôi dung b√†i vi·∫øt
        if not article_data["content"]:
            logger.warning(f"B·ªè qua b√†i vi·∫øt kh√¥ng c√≥ n·ªôi dung: {article_data['title']}")
            continue
            
        articles_to_import.append(article_data)
    
    if not articles_to_import:
        logger.warning("Kh√¥ng c√≥ b√†i vi·∫øt h·ª£p l·ªá ƒë·ªÉ import")
        return 0, 0
    
    logger.info(f"ƒêang import {len(articles_to_import)} b√†i vi·∫øt")
    
    # In ph·∫ßn ƒë·∫ßu c·ªßa b√†i vi·∫øt ƒë·∫ßu ti√™n ƒë·ªÉ debug
    if articles_to_import:
        first_article = articles_to_import[0]
        logger.info(f"M·∫´u b√†i vi·∫øt ƒë·∫ßu ti√™n: Title={first_article['title']}, Content length={len(first_article['content'])}, Source={first_article['source_name']}")
    
    # Chu·∫©n b·ªã payload theo ƒë√∫ng ƒë·ªãnh d·∫°ng API y√™u c·∫ßu
    payload = {
        "articles": articles_to_import
    }
    
    # Th·ª≠ import v·ªõi endpoint import
    try:
        logger.info(f"Th·ª≠ import v·ªõi endpoint: {ARTICLES_IMPORT_API_URL}")
        
        # G·ªçi API import
        response = requests.post(
            ARTICLES_IMPORT_API_URL,
            json=payload,
            headers={"Content-Type": "application/json", "Accept": "application/json"}
        )
        
        logger.info(f"Ph·∫£n h·ªìi t·ª´ endpoint import: Status code: {response.status_code}")
        logger.info(f"Ph·∫£n h·ªìi n·ªôi dung: {response.text[:500]}")
        
        if response.status_code == 200:
            try:
                result = response.json()
                success = result.get("success", 0) or (len(articles_to_import) - result.get("skipped", 0))
                failed = result.get("failed", 0) or result.get("skipped", 0)
                logger.info(f"Import th√†nh c√¥ng v·ªõi endpoint import: {success} th√†nh c√¥ng, {failed} th·∫•t b·∫°i")
                
                # X√≥a file JSON sau khi import th√†nh c√¥ng n·∫øu c√≥ b√†i vi·∫øt ƒë∆∞·ª£c import
                if success > 0:
                    try:
                        os.remove(json_file_path)
                        logger.info(f"ƒê√£ x√≥a file JSON sau khi import th√†nh c√¥ng: {json_file_path}")
                    except Exception as e:
                        logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {json_file_path}: {str(e)}")
                return success, failed
            except Exception as e:
                logger.error(f"L·ªói khi x·ª≠ l√Ω ph·∫£n h·ªìi t·ª´ endpoint import: {str(e)}")
        else:
            logger.warning(f"Import v·ªõi endpoint import th·∫•t b·∫°i. Status code: {response.status_code}")
    except Exception as e:
        logger.error(f"L·ªói khi g·ªçi endpoint import: {str(e)}")
        logger.error(traceback.format_exc())
    
    # N·∫øu endpoint import th·∫•t b·∫°i, th·ª≠ import tr·ª±c ti·∫øp v√†o DB
    logger.info("Th·ª≠ import tr·ª±c ti·∫øp v√†o database...")
    success, failed = import_articles_to_database_directly(articles_to_import)
    
    # X√≥a file JSON sau khi import th√†nh c√¥ng n·∫øu c√≥ b√†i vi·∫øt ƒë∆∞·ª£c import
    if success > 0:
        try:
            os.remove(json_file_path)
            logger.info(f"ƒê√£ x√≥a file JSON sau khi import th√†nh c√¥ng: {json_file_path}")
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {json_file_path}: {str(e)}")
    
    return success, failed

def import_articles_to_database_directly(articles):
    """
    Import b√†i vi·∫øt tr·ª±c ti·∫øp v√†o database (fallback khi API th·∫•t b·∫°i)
    :param articles: Danh s√°ch b√†i vi·∫øt c·∫ßn import
    :return: tuple (s·ªë b√†i vi·∫øt import th√†nh c√¥ng, s·ªë b√†i vi·∫øt th·∫•t b·∫°i)
    """
    if not check_db_connection():
        logger.error("Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn database")
        return 0, 0
    
    logger.info(f"ƒêang import {len(articles)} b√†i vi·∫øt tr·ª±c ti·∫øp v√†o database")
    
    # K·∫øt n·ªëi ƒë·∫øn database
    conn = mysql.connector.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    success_count = 0
    failed_count = 0
    
    try:
        for article in articles:
            try:
                # Chu·∫©n b·ªã d·ªØ li·ªáu
                title = article.get('title', '')
                slug = article.get('slug', '')
                summary = article.get('summary', '')
                content = article.get('content', '')
                source_name = article.get('source_name', '')
                source_url = article.get('source_url', '')
                source_icon = article.get('source_icon', '')
                published_at = article.get('published_at', datetime.now().isoformat())
                category = article.get('category', 1)  # ƒê·∫£m b·∫£o s·ª≠ d·ª•ng ƒë√∫ng t√™n c·ªôt trong database
                meta_data = article.get('meta_data', {})
                
                # ƒê·∫£m b·∫£o meta_data l√† string JSON
                if isinstance(meta_data, dict):
                    meta_data = json.dumps(meta_data)
                
                # SQL query ƒë·ªÉ insert b√†i vi·∫øt
                query = """
                INSERT INTO articles 
                (title, slug, summary, content, source_name, source_url, source_icon, 
                published_at, category, meta_data, is_processed, created_at, updated_at) 
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
                """
                
                # T·∫°o tuple c√°c gi√° tr·ªã
                values = (
                    title, slug, summary, content, source_name, source_url, source_icon,
                    published_at, category, meta_data, True
                )
                
                # Th·ª±c hi·ªán query
                cursor.execute(query, values)
                
                # ƒê·∫øm s·ªë b√†i vi·∫øt th√†nh c√¥ng
                success_count += 1
                logger.info(f"ƒê√£ import th√†nh c√¥ng b√†i vi·∫øt: {title}")
                
            except mysql.connector.Error as e:
                # B·ªè qua l·ªói duplicate key
                if e.errno == 1062:  # M√£ l·ªói cho duplicate key
                    logger.warning(f"B√†i vi·∫øt ƒë√£ t·ªìn t·∫°i (slug tr√πng l·∫∑p): {article.get('title', '')}")
                else:
                    logger.error(f"L·ªói MySQL khi import b√†i vi·∫øt: {str(e)}")
                
                failed_count += 1
            except Exception as e:
                logger.error(f"L·ªói khi import b√†i vi·∫øt: {str(e)}")
                failed_count += 1
        
        # Commit c√°c thay ƒë·ªïi
        conn.commit()
        logger.info(f"ƒê√£ import tr·ª±c ti·∫øp v√†o database: {success_count} th√†nh c√¥ng, {failed_count} th·∫•t b·∫°i")
        
    except Exception as e:
        logger.error(f"L·ªói khi import v√†o database: {str(e)}")
        logger.error(traceback.format_exc())
        conn.rollback()
    finally:
        cursor.close()
        conn.close()
    
    return success_count, failed_count

def check_db_connection():
    """
    Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn database
    :return: Boolean - True n·∫øu k·∫øt n·ªëi th√†nh c√¥ng, False n·∫øu kh√¥ng
    """
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        if conn.is_connected():
            logger.info("K·∫øt n·ªëi ƒë·∫øn database th√†nh c√¥ng")
            conn.close()
            return True
        else:
            logger.error("Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn database")
            return False
    except Exception as e:
        logger.error(f"L·ªói khi k·∫øt n·ªëi ƒë·∫øn database: {str(e)}")
        return False

def generate_slug(title, add_uuid=True):
    """
    T·∫°o slug t·ª´ ti√™u ƒë·ªÅ b√†i vi·∫øt
    :param title: Ti√™u ƒë·ªÅ b√†i vi·∫øt
    :param add_uuid: Th√™m UUID v√†o slug ƒë·ªÉ ƒë·∫£m b·∫£o duy nh·∫•t
    :return: Slug ƒë√£ ƒë∆∞·ª£c t·∫°o
    """
    # Lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát
    slug = unidecode(title)
    
    # Chuy·ªÉn ƒë·ªïi th√†nh ch·ªØ th∆∞·ªùng v√† thay th·∫ø c√°c k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ c√°i, s·ªë b·∫±ng d·∫•u g·∫°ch ngang
    slug = re.sub(r'[^\w\s-]', '', slug.lower())
    slug = re.sub(r'[\s_-]+', '-', slug).strip('-')
    
    # Th√™m UUID ƒë·ªÉ ƒë·∫£m b·∫£o duy nh·∫•t
    if add_uuid:
        unique_id = str(uuid.uuid4())[:8]
        slug = f"{slug}-{unique_id}"
        
    return slug

def main():
    """
    H√†m ch√≠nh ƒëi·ªÅu khi·ªÉn ch∆∞∆°ng tr√¨nh
    """
    parser = argparse.ArgumentParser(description="T√¨m ki·∫øm v√† tr√≠ch xu·∫•t b√†i vi·∫øt t·ª´ Google News")
    parser.add_argument('--all', action='store_true', help='X·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c t·ª´ backend')
    parser.add_argument('--max-articles', type=int, default=2, help='S·ªë b√†i vi·∫øt t·ªëi ƒëa cho m·ªói danh m·ª•c')
    parser.add_argument('--category', type=int, help='ID c·ªßa danh m·ª•c c·ª• th·ªÉ ƒë·ªÉ x·ª≠ l√Ω')
    parser.add_argument('--keyword', type=str, help='T·ª´ kh√≥a t√¨m ki·∫øm')
    parser.add_argument('--cleanup', action='store_true', help='D·ªçn d·∫πp c√°c file t·∫°m v√† file log c≈©')
    parser.add_argument('--retention', type=int, default=RETENTION_DAYS, help='S·ªë ng√†y gi·ªØ l·∫°i file (m·∫∑c ƒë·ªãnh: 7)')
    parser.add_argument('--import-all', action='store_true', help='Import t·∫•t c·∫£ file JSON trong th∆∞ m·ª•c output v√†o backend')
    parser.add_argument('--no-import', action='store_true', help='Kh√¥ng t·ª± ƒë·ªông import b√†i vi·∫øt v√†o backend')
    parser.add_argument('--single-file', action='store_true', help='L∆∞u t·∫•t c·∫£ b√†i vi·∫øt v√†o m·ªôt file JSON duy nh·∫•t')
    
    args = parser.parse_args()
    
    try:
        # Ki·ªÉm tra v√† x·ª≠ l√Ω tham s·ªë
        if args.cleanup:
            # D·ªçn d·∫πp file t·∫°m v√† log c≈©
            cleanup_temp_files()
            cleanup_old_files(args.retention)
            print(f"ƒê√£ d·ªçn d·∫πp c√°c file t·∫°m v√† file log c≈© h∆°n {args.retention} ng√†y")
            return
            
        if args.import_all:
            # Import t·∫•t c·∫£ file JSON trong th∆∞ m·ª•c output
            success, failed = import_all_to_backend()
            print(f"ƒê√£ import c√°c file JSON: {success} th√†nh c√¥ng, {failed} th·∫•t b·∫°i")
            return
            
        # X·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c
        if args.all:
            print(f"X·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c, t·ªëi ƒëa {args.max_articles} b√†i vi·∫øt m·ªói danh m·ª•c")
            results = find_and_process_all_categories(args.max_articles)
            
            # In k·∫øt qu·∫£
            total_categories = results['success'] + results['failed']
            total_articles = results['total_articles']
            
            print(f"\nƒê√£ x·ª≠ l√Ω {total_categories} danh m·ª•c")
            print(f"Th√†nh c√¥ng: {results['success']} danh m·ª•c, Th·∫•t b·∫°i: {results['failed']} danh m·ª•c")
            print(f"T·ªïng s·ªë b√†i vi·∫øt ƒë√£ t√¨m ƒë∆∞·ª£c: {total_articles}")
            
            if results['success'] > 0:
                # In th√¥ng tin chi ti·∫øt c√°c danh m·ª•c th√†nh c√¥ng
                print("\nC√°c danh m·ª•c ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng:\n")
                
                for category_name, articles in results['articles_by_category'].items():
                    if articles:
                        print(f"- {category_name}")
                        print(f"  S·ªë b√†i vi·∫øt: {len(articles)}")
                        
                        for i, article in enumerate(articles, 1):
                            print(f"  B√†i vi·∫øt #{i}:")
                            print(f"    URL: {article['url']}")
                            print(f"    Ti√™u ƒë·ªÅ: {article['title']}")
                            print(f"    Chi·ªÅu d√†i n·ªôi dung: {article.get('content_length', len(article.get('content', '')))} k√Ω t·ª±")
                            print(f"    L∆∞u t·∫°i: {article.get('json_filepath', article.get('file_path', 'Kh√¥ng r√µ'))}")
                            print()
            
            # L∆∞u t·∫•t c·∫£ b√†i vi·∫øt v√†o file JSON
            try:
                # Lu√¥n l∆∞u v√†o m·ªôt file JSON duy nh·∫•t
                json_file_path = save_all_articles_to_single_file(results)
                
                if json_file_path:
                    article_count = 0
                    # ƒê·∫øm s·ªë b√†i vi·∫øt ƒë√£ l∆∞u th·ª±c t·∫ø
                    if "all_articles" in results and isinstance(results["all_articles"], list):
                        article_count = len(results["all_articles"])
                    else:
                        # ƒê·∫øm t·ª´ t·ªïng s·ªë b√†i vi·∫øt trong articles_by_category
                        for articles in results['articles_by_category'].values():
                            article_count += len(articles)
                    
                    print(f"\nƒê√£ l∆∞u t·∫•t c·∫£ {article_count} b√†i vi·∫øt v√†o file: {json_file_path}")
                    
                    if not args.no_import:
                        # T·ª± ƒë·ªông import v√†o backend
                        print(f"\nƒêang import t·∫•t c·∫£ b√†i vi·∫øt v√†o backend...")
                        success, failed = import_json_file_to_backend(json_file_path)
                        print(f"ƒê√£ import v√†o database: {success} th√†nh c√¥ng, {failed} th·∫•t b·∫°i")
            except Exception as e:
                logger.error(f"L·ªói khi l∆∞u ho·∫∑c import b√†i vi·∫øt: {str(e)}")
                logger.error(traceback.format_exc())
                print(f"L·ªói khi l∆∞u ho·∫∑c import b√†i vi·∫øt: {str(e)}")
                
            return
        
        # X·ª≠ l√Ω theo ID danh m·ª•c n·∫øu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if args.category:
            from google_news import get_category_by_id
            category = get_category_by_id(args.category)
            
            if category:
                articles = process_category(args.category, category['name'], args.max_articles)
                if articles:
                    print(f"ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng danh m·ª•c: {category['name']}")
                    print(f"T√¨m th·∫•y {len(articles)} b√†i vi·∫øt:")
                    
                    for i, article in enumerate(articles, 1):
                        print(f"\nB√†i vi·∫øt #{i}:")
                        print(f"URL: {article['url']}")
                        print(f"Ti√™u ƒë·ªÅ: {article['title']}")
                        print(f"Chi·ªÅu d√†i n·ªôi dung: {article['content_length']} k√Ω t·ª±")
                        print(f"L∆∞u t·∫°i: {article['json_filepath']}")
                    
                    # L∆∞u t·∫•t c·∫£ v√†o m·ªôt file JSON duy nh·∫•t
                    result = {
                        "categories": [
                            {
                                "id": category["id"],
                                "name": category["name"],
                                "status": "success",
                                "articles": articles
                            }
                        ]
                    }
                    
                    json_file_path = save_all_articles_to_single_file(result)
                    
                    if json_file_path and not args.no_import:
                        # T·ª± ƒë·ªông import v√†o backend
                        success, failed = import_json_file_to_backend(json_file_path)
                        print(f"ƒê√£ import v√†o database: {success} th√†nh c√¥ng, {failed} th·∫•t b·∫°i")
                else:
                    print(f"Kh√¥ng th·ªÉ x·ª≠ l√Ω danh m·ª•c: {category['name']}")
            else:
                logger.error(f"Kh√¥ng t√¨m th·∫•y danh m·ª•c v·ªõi ID: {args.category}")
            return
        
        # X·ª≠ l√Ω theo t·ª´ kh√≥a n·∫øu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if args.keyword:
            from google_news import search_google_news
            
            logger.info(f"T√¨m ki·∫øm v·ªõi t·ª´ kh√≥a: {args.keyword}")
            url = search_google_news(args.keyword)
            
            if url:
                logger.info(f"ƒê√£ t√¨m th·∫•y URL: {url}")
                
                # Tr√≠ch xu·∫•t n·ªôi dung
                article_data = extract_article_content(url)
                
                if article_data and article_data.get("title") and article_data.get("content"):
                    # L∆∞u v√†o file JSON
                    json_filepath = save_article_to_json(
                        category_id=0,  # 0 v√¨ kh√¥ng c√≥ category_id
                        category_name=args.keyword,
                        article_url=url,
                        article_data=article_data
                    )
                    
                    if json_filepath:
                        print(f"ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng t·ª´ kh√≥a: {args.keyword}")
                        print(f"URL: {url}")
                        print(f"Ti√™u ƒë·ªÅ: {article_data['title']}")
                        print(f"Chi·ªÅu d√†i n·ªôi dung: {len(article_data['content'])} k√Ω t·ª±")
                        print(f"L∆∞u t·∫°i: {json_filepath}")
                        
                        if not args.no_import:
                            # T·ª± ƒë·ªông import v√†o backend
                            success, failed = import_json_file_to_backend(json_filepath)
                            print(f"ƒê√£ import v√†o database: {success} th√†nh c√¥ng, {failed} th·∫•t b·∫°i")
                else:
                    logger.error(f"Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´: {url}")
            else:
                logger.error(f"Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o cho t·ª´ kh√≥a: {args.keyword}")
            return
        
        # Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng n·∫øu kh√¥ng c√≥ l·ªánh n√†o ƒë∆∞·ª£c ch·∫°y
        parser.print_help()
    
    except Exception as e:
        logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh trong qu√° tr√¨nh x·ª≠ l√Ω: {str(e)}")
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    main() 