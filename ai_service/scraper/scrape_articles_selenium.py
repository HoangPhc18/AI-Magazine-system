#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Module Ä‘á»ƒ trÃ­ch xuáº¥t ná»™i dung bÃ i viáº¿t tá»« URL Ä‘Ã£ thu tháº­p.
Má»—i URL Ä‘Æ°á»£c truy cáº­p báº±ng Selenium Ä‘á»ƒ láº¥y ná»™i dung Ä‘áº§y Ä‘á»§.
Cháº¡y sau khi google_news_serpapi.py Ä‘Ã£ thu tháº­p cÃ¡c URL bÃ i viáº¿t.
"""

import json
import time
import sys
import os
import requests
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import logging
from bs4 import BeautifulSoup
from urllib.parse import urlparse

# ğŸ”¹ Laravel Backend API URL - Update as needed
BACKEND_API_URL = "http://localhost:8000/api/articles/import"

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(f"content_scraper_{datetime.now().strftime('%Y%m%d')}.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

def setup_driver():
    """Setup and return a configured Chrome WebDriver instance"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    return driver

def get_site_specific_selectors(url):
    """
    Tráº£ vá» cÃ¡c bá»™ chá»n CSS tÃ¹y chá»‰nh dá»±a trÃªn tÃªn miá»n
    
    Args:
        url (str): URL cá»§a bÃ i viáº¿t
    
    Returns:
        dict: CÃ¡c bá»™ chá»n cho tiÃªu Ä‘á» vÃ  ná»™i dung
    """
    domain = urlparse(url).netloc
    
    selectors = {
        # VnExpress
        "vnexpress.net": {
            "title": ["h1.title-detail", "h1.title-post", ".title-news", ".title_news_detail"],
            "content": ["article.fck_detail", "div.fck_detail", ".content_detail", "article.content-detail"],
            "paragraphs": ["p.Normal", "p"],
            "exclude": [".author", ".copyright", ".relatebox", ".box-tag", ".social_pin", ".list-news", ".width_common", ".footer", ".header"]
        },
        # Tuá»•i Tráº»
        "tuoitre.vn": {
            "title": ["h1.article-title", "h1.title-2", "h1.detail-title", ".article-title"],
            "content": ["div.content.fck", "#main-detail-body", "div.detail-content", "#mainContent", ".content-news", ".detail-content-body"],
            "paragraphs": ["p"],
            "exclude": [".VCSortableInPreviewMode", ".relate-container", ".source", ".author", ".date-time"]
        },
        # DÃ¢n TrÃ­
        "dantri.com.vn": {
            "title": ["h1.title-page", "h1.e-title", "h1.title", ".e-magazine", ".title-news"],
            "content": ["div.dt-news__content", "div.e-content", ".singular-content", ".article-body", ".dt-news__body"],
            "paragraphs": ["p"],
            "exclude": [".dt-news__sapo", ".author-info", ".article-topic", ".e-magazineplus", ".dt-news__meta"]
        },
        # Thanh NiÃªn
        "thanhnien.vn": {
            "title": ["h1.detail-title", "h1.cms-title", ".details__headline", "h1.title-news"],
            "content": ["div.detail-content", "div.cms-body", ".l-content", ".details__content", "#abody", "#content-id"],
            "paragraphs": ["p"],
            "exclude": [".author", ".source", ".details__meta", ".details__author", ".related-container"]
        },
        # VietnamNet
        "vietnamnet.vn": {
            "title": ["h1.content-detail-title", "h1.title", ".content-title", "h1.title-item-news", ".title-content"],
            "content": ["div.content-detail", ".ArticleContent", "#article-body", ".articleContent", ".boxPostDetail", ".detail-content"],
            "paragraphs": ["p"],
            "exclude": [".author-info", ".article-relate", ".box-taitro", ".box-title", ".article-tags"]
        },
        # NhÃ¢n DÃ¢n
        "nhandan.vn": {
            "title": ["div.box-title h1", ".nd-detail-title", ".title-detail", ".article-title", "h1.article-title"],
            "content": ["div.box-content-detail", "#nd-article-content", ".article-body", ".detail-content-wrap"],
            "paragraphs": ["p"],
            "exclude": [".box-author", ".article-meta", ".box-share"]
        },
        # Tiá»n Phong
        "tienphong.vn": {
            "title": ["h1.article__title", "h1.cms-title", ".headline", ".main-article-title", "h1.article-title"],
            "content": ["div.article__body", ".cms-body", ".article-body", ".article-content", ".main-article-body"],
            "paragraphs": ["p"],
            "exclude": [".article__author", ".article__tag", ".article__share", ".article__meta"]
        },
        # BÃ¡o Má»›i
        "baomoi.com": {
            "title": ["h1.bm-title", "h1.title", ".article-title", ".article-header", ".title", ".headline"],
            "content": ["div.content", ".bm-content", ".article-body", ".story__content", ".article__content"],
            "paragraphs": ["p"],
            "exclude": [".bm-source", ".bm-resource", ".relate-container", ".bm-avatar", ".top-comments"]
        },
        # Zing News
        "zingnews.vn": {
            "title": ["h1.the-article-title", "h1.article-title", ".the-article-header", ".page-title", ".article-header h1"],
            "content": ["div.the-article-body", "article.the-article-content", ".the-article-content", ".article-content"],
            "paragraphs": ["p"],
            "exclude": [".author", ".source", ".article-tags", ".article-related", ".the-article-tags", ".the-article-meta"]
        },
        # 24h
        "24h.com.vn": {
            "title": ["h1.bld", "h1.clrTit", ".titCM", ".tuht-dts", "article h1"],
            "content": ["div.text-conent", "div.baiviet-bailienquan", ".colCenter-in", ".boxDtlBody", "article .ctTp"],
            "paragraphs": ["p"],
            "exclude": [".nguontin", ".baiviet-tags", ".bv-cp", ".fb-like", ".imgCation"]
        },
        # CafeF
        "cafef.vn": {
            "title": ["h1.title", ".articledetail_title", ".title-detail", "h1.title_detail"],
            "content": ["div#mainContent", ".contentdetail", ".article-body", "#CMS_Detail", "#content_detail_news"],
            "paragraphs": ["p"],
            "exclude": [".author", ".source", ".relationnews", ".tindlienquan"]
        },
        # VOV
        "vov.vn": {
            "title": ["h1.article-title", ".main-article h1", ".cms-title", ".vovtitle"],
            "content": ["div.article-body", ".main-article-body", ".article-content", "#article_content", ".vov-content"],
            "paragraphs": ["p"],
            "exclude": [".article-author", ".article-tools", ".article-related"]
        },
        # Lao Äá»™ng
        "laodong.vn": {
            "title": ["h1.title", ".article-title", ".headline", "h1.headline_detail"],
            "content": ["div.article-content", ".cms-body", ".contentbody", ".detail-content-body", "#box_details"],
            "paragraphs": ["p"],
            "exclude": [".author", ".source", ".article-meta", ".article-tools", ".boxrelation"]
        },
        # BÃ¡o Thanh tra
        "thanhtra.com.vn": {
            "title": ["h1.title", ".article-title", ".news-title"],
            "content": [".article-body", ".news-content", ".content-body"],
            "paragraphs": ["p"],
            "exclude": [".author", ".article-info", ".tags"]
        },
        # PhÃ¡p Luáº­t TP.HCM
        "plo.vn": {
            "title": ["h1.title", ".article__title", ".article-title"],
            "content": [".article__body", ".article-content", ".cms-body"],
            "paragraphs": ["p"],
            "exclude": [".author", ".source", ".tags-container"]
        },
        # SÃ i GÃ²n Giáº£i PhÃ³ng
        "sggp.org.vn": {
            "title": ["h1.title", ".cms-title", ".article-title"],
            "content": [".article-content", ".cms-body", "#content_detail_news"],
            "paragraphs": ["p"],
            "exclude": [".author", ".nguon", ".source"]
        }
    }
    
    # Máº·c Ä‘á»‹nh cho cÃ¡c trang khÃ´ng cÃ³ cáº¥u hÃ¬nh cá»¥ thá»ƒ
    default_selectors = {
        "title": ["h1", "h1.title", "h1.article-title", ".headline", ".article-headline", ".entry-title", ".post-title", ".main-title"],
        "content": ["article", "main", ".content", ".article-content", ".entry-content", ".post-content", ".news-content", ".main-content", "#content", "#main"],
        "paragraphs": ["p"],
        "exclude": [".comments", ".sidebar", ".related", ".footer", ".header", ".navigation", ".menu", ".ads", ".social", ".sharing", ".tags", ".author", ".meta", ".date"]
    }
    
    # Tráº£ vá» bá»™ chá»n tÃ¹y chá»‰nh hoáº·c máº·c Ä‘á»‹nh
    for key in selectors:
        if key in domain:
            return selectors[key]
            
    return default_selectors

def extract_content(driver, url, title="Unknown title"):
    """
    TrÃ­ch xuáº¥t ná»™i dung dáº¡ng vÄƒn báº£n thuáº§n tÃºy tá»« URL bÃ i viáº¿t, vá»›i xá»­ lÃ½ tÃ¹y chá»‰nh cho cÃ¡c trang tin tá»©c phá»• biáº¿n.
    Giá»¯ nguyÃªn ná»™i dung tiÃªu Ä‘á» vÃ  bÃ i viáº¿t khÃ´ng sá»­a Ä‘á»•i Ä‘á»ƒ lÆ°u vÃ o database.
    
    Args:
        driver (WebDriver): Driver Selenium
        url (str): URL cá»§a bÃ i viáº¿t
        title (str): TiÃªu Ä‘á» bÃ i viáº¿t ban Ä‘áº§u (náº¿u cÃ³)
        
    Returns:
        tuple: (extracted_title, full_content)
    """
    try:
        logger.info(f"Äang truy cáº­p URL: {url}")
        driver.get(url)
        
        # Chá» ná»™i dung táº£i xong (tá»‘i Ä‘a 20 giÃ¢y)
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # Äáº£m báº£o trang Ä‘Ã£ táº£i Ä‘áº§y Ä‘á»§ báº±ng cÃ¡ch scroll xuá»‘ng
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.3);")
        time.sleep(1.5)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.7);")
        time.sleep(1.5)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.5)
        
        # Láº¥y ná»™i dung trang
        page_content = driver.page_source
        
        # Sá»­ dá»¥ng BeautifulSoup Ä‘á»ƒ phÃ¢n tÃ­ch HTML
        soup = BeautifulSoup(page_content, "html.parser")
        
        # XÃ¡c Ä‘á»‹nh tÃªn miá»n Ä‘á»ƒ Ã¡p dá»¥ng selector phÃ¹ há»£p
        domain = urlparse(url).netloc
        logger.info(f"Äang xá»­ lÃ½ trang web: {domain}")
        
        # Láº¥y cÃ¡c bá»™ chá»n tÃ¹y chá»‰nh theo trang web
        selectors = get_site_specific_selectors(url)
        
        # ---------- TrÃ­ch xuáº¥t tiÃªu Ä‘á» ----------
        extracted_title = ""
        
        # Thá»­ cÃ¡c bá»™ chá»n tiÃªu Ä‘á» tÃ¹y chá»‰nh
        for selector in selectors["title"]:
            title_elements = soup.select(selector)
            if title_elements:
                for title_element in title_elements:
                    if title_element and title_element.text.strip():
                        extracted_title = title_element.text.strip()
                        logger.info(f"ÄÃ£ tÃ¬m tháº¥y tiÃªu Ä‘á» tá»« bá»™ chá»n '{selector}': {extracted_title[:50]}...")
                        break
                if extracted_title:
                    break
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y, thá»­ tÃ¬m tá»« cÃ¡c tháº» h1/h2 phá»• biáº¿n
        if not extracted_title:
            heading_tags = soup.find_all(['h1', 'h2'], limit=5)
            for tag in heading_tags:
                text = tag.text.strip()
                if text and len(text) > 15:  # TiÃªu Ä‘á» thÆ°á»ng dÃ i hÆ¡n 15 kÃ½ tá»±
                    extracted_title = text
                    logger.info(f"ÄÃ£ tÃ¬m tháº¥y tiÃªu Ä‘á» tá»« tháº» '{tag.name}': {extracted_title[:50]}...")
                    break
        
        # Thá»­ láº¥y title tá»« tháº» title náº¿u váº«n khÃ´ng tÃ¬m tháº¥y
        if not extracted_title and soup.title:
            # Láº¥y tá»« tháº» title, loáº¡i bá» tÃªn trang web náº¿u cÃ³
            page_title = soup.title.text.strip()
            # Loáº¡i bá» pháº§n Ä‘uÃ´i nhÆ° "| VnExpress", "- DÃ¢n TrÃ­", v.v.
            extracted_title = re.sub(r'[-|]\s*[^-|]+$', '', page_title).strip()
            logger.info(f"ÄÃ£ láº¥y tiÃªu Ä‘á» tá»« tháº» title: {extracted_title[:50]}...")
        
        # Sá»­ dá»¥ng tiÃªu Ä‘á» Ä‘Ã£ cÃ³ náº¿u váº«n khÃ´ng tÃ¬m tháº¥y
        if not extracted_title and title and title != "Unknown title":
            extracted_title = title
            logger.info(f"Sá»­ dá»¥ng tiÃªu Ä‘á» Ä‘Ã£ cÃ³: {extracted_title[:50]}...")
            
        # ---------- TrÃ­ch xuáº¥t ná»™i dung ----------
        content_element = None
        
        # Thá»­ cÃ¡c bá»™ chá»n ná»™i dung tÃ¹y chá»‰nh
        for selector in selectors["content"]:
            content_elements = soup.select(selector)
            if content_elements:
                # Chá»n pháº§n tá»­ ná»™i dung lá»›n nháº¥t (cÃ³ nhiá»u text nháº¥t)
                max_length = 0
                for element in content_elements:
                    text_length = len(element.get_text())
                    if text_length > max_length:
                        max_length = text_length
                        content_element = element
                
                if content_element:
                    logger.info(f"ÄÃ£ tÃ¬m tháº¥y pháº§n tá»­ ná»™i dung tá»« bá»™ chá»n '{selector}' ({max_length} kÃ½ tá»±)")
                    break
        
        # TrÃ­ch xuáº¥t ná»™i dung vÄƒn báº£n thuáº§n tÃºy
        if content_element:
            # TrÆ°á»›c khi trÃ­ch xuáº¥t, loáº¡i bá» cÃ¡c pháº§n tá»­ khÃ´ng mong muá»‘n tá»« ná»™i dung
            for exclude_selector in selectors["exclude"]:
                for element in content_element.select(exclude_selector):
                    element.extract()
            
            # Láº¥y vÄƒn báº£n thuáº§n tÃºy, giá»¯ nguyÃªn cÃ¡ch Ä‘oáº¡n
            paragraphs = []
            
            # TÃ¬m táº¥t cáº£ cÃ¡c Ä‘oáº¡n vÄƒn trong pháº§n tá»­ ná»™i dung
            if "," in ", ".join(selectors["paragraphs"]):
                p_elements = content_element.select(", ".join(selectors["paragraphs"]))
            else:
                p_elements = content_element.find_all(selectors["paragraphs"][0])
                
            # Náº¿u khÃ´ng tÃ¬m tháº¥y Ä‘oáº¡n vÄƒn, láº¥y táº¥t cáº£ vÄƒn báº£n trong pháº§n tá»­ ná»™i dung
            if not p_elements or len(p_elements) < 3:
                # Thá»­ tÃ¬m táº¥t cáº£ cÃ¡c tháº» text nÃ³i chung
                all_text_elements = [el for el in content_element.find_all(text=True) if el.parent.name not in ['script', 'style']]
                clean_text = []
                for el in all_text_elements:
                    text = el.strip()
                    if text and len(text) > 10:
                        clean_text.append(text)
                
                if clean_text:
                    full_content = "\n\n".join(clean_text)
                else:
                    full_content = content_element.get_text(separator="\n\n", strip=True)
                
                logger.info(f"ÄÃ£ trÃ­ch xuáº¥t ná»™i dung vÄƒn báº£n ({len(full_content.split())} tá»«) tá»«: {url}")
            else:
                # Láº¥y ná»™i dung tá»« tá»«ng Ä‘oáº¡n vÄƒn
                for p in p_elements:
                    text = p.text.strip()
                    if text:  # Láº¥y táº¥t cáº£ Ä‘oáº¡n vÄƒn cÃ³ ná»™i dung
                        paragraphs.append(text)
                
                full_content = "\n\n".join(paragraphs)
                logger.info(f"ÄÃ£ trÃ­ch xuáº¥t ná»™i dung tá»« {len(paragraphs)} Ä‘oáº¡n vÄƒn, tá»•ng cá»™ng {len(full_content.split())} tá»«")
        else:
            # Náº¿u khÃ´ng tÃ¬m tháº¥y pháº§n tá»­ ná»™i dung, thá»­ phÆ°Æ¡ng phÃ¡p chá»§ Ä‘á»™ng hÆ¡n
            logger.warning(f"KhÃ´ng tÃ¬m tháº¥y pháº§n tá»­ ná»™i dung cá»¥ thá»ƒ cho URL: {url}, dÃ¹ng phÆ°Æ¡ng phÃ¡p dá»± phÃ²ng")
            
            # PhÆ°Æ¡ng phÃ¡p 1: TÃ¬m pháº§n tá»­ cÃ³ nhiá»u tháº» <p> nháº¥t
            article_candidates = []
            
            for tag in ['article', 'main', 'div.content', 'div.article', '.detail', '#detail', '#content']:
                elements = soup.select(tag)
                for element in elements:
                    p_count = len(element.find_all('p'))
                    if p_count >= 3:  # Ãt nháº¥t 3 Ä‘oáº¡n vÄƒn
                        article_candidates.append((element, p_count))
            
            if article_candidates:
                # Sáº¯p xáº¿p theo sá»‘ lÆ°á»£ng tháº» p, láº¥y pháº§n tá»­ cÃ³ nhiá»u tháº» p nháº¥t
                article_candidates.sort(key=lambda x: x[1], reverse=True)
                content_element = article_candidates[0][0]
                logger.info(f"ÄÃ£ tÃ¬m tháº¥y pháº§n tá»­ ná»™i dung báº±ng phÆ°Æ¡ng phÃ¡p phÃ¢n tÃ­ch cáº¥u trÃºc ({article_candidates[0][1]} Ä‘oáº¡n vÄƒn)")
                
                # Láº¥y táº¥t cáº£ cÃ¡c Ä‘oáº¡n vÄƒn tá»« pháº§n tá»­ nÃ y
                paragraphs = []
                for p in content_element.find_all('p'):
                    text = p.text.strip()
                    if text and len(text) > 15:  # Chá»‰ láº¥y Ä‘oáº¡n vÄƒn cÃ³ Ä‘á»§ ná»™i dung
                        paragraphs.append(text)
                
                if paragraphs:
                    full_content = "\n\n".join(paragraphs)
                    logger.info(f"ÄÃ£ trÃ­ch xuáº¥t ná»™i dung dá»± phÃ²ng tá»« {len(paragraphs)} Ä‘oáº¡n vÄƒn, tá»•ng cá»™ng {len(full_content.split())} tá»«")
                else:
                    # KhÃ´ng cÃ³ Ä‘oáº¡n vÄƒn, láº¥y táº¥t cáº£ vÄƒn báº£n
                    full_content = content_element.get_text(separator="\n\n", strip=True)
                    logger.info(f"ÄÃ£ trÃ­ch xuáº¥t toÃ n bá»™ vÄƒn báº£n tá»« pháº§n tá»­ ná»™i dung ({len(full_content.split())} tá»«)")
            else:
                # PhÆ°Æ¡ng phÃ¡p 2: Láº¥y táº¥t cáº£ cÃ¡c Ä‘oáº¡n vÄƒn tá»« trang
                paragraphs = []
                for p in soup.find_all('p'):
                    text = p.text.strip()
                    if text and len(text) > 20:  # Chá»‰ láº¥y Ä‘oáº¡n vÄƒn cÃ³ Ä‘á»§ ná»™i dung
                        paragraphs.append(text)
                
                if paragraphs:
                    full_content = "\n\n".join(paragraphs)
                    logger.info(f"ÄÃ£ trÃ­ch xuáº¥t ná»™i dung tá»« táº¥t cáº£ cÃ¡c Ä‘oáº¡n vÄƒn trÃªn trang ({len(paragraphs)} Ä‘oáº¡n)")
                else:
                    # PhÆ°Æ¡ng phÃ¡p cuá»‘i cÃ¹ng: láº¥y toÃ n bá»™ vÄƒn báº£n tá»« trang
                    main_content = soup.find('main') or soup.find('article') or soup.find('body')
                    full_content = main_content.get_text(separator="\n\n", strip=True)
                    logger.info(f"ÄÃ£ trÃ­ch xuáº¥t vÄƒn báº£n tá»•ng thá»ƒ ({len(full_content.split())} tá»«)")
        
        # Loáº¡i bá» cÃ¡c dÃ²ng trá»‘ng vÃ  lÃ m sáº¡ch ná»™i dung
        if full_content:
            # Loáº¡i bá» khoáº£ng tráº¯ng dÆ° thá»«a
            full_content = re.sub(r'\n{3,}', '\n\n', full_content)
            
            # Náº¿u ná»™i dung quÃ¡ ngáº¯n, thÃ´ng bÃ¡o cáº£nh bÃ¡o
            if len(full_content.split()) < 50:
                logger.warning(f"Ná»™i dung trÃ­ch xuáº¥t quÃ¡ ngáº¯n ({len(full_content.split())} tá»«), cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c")
        
        return extracted_title, full_content
    
    except Exception as e:
        logger.error(f"Lá»—i khi trÃ­ch xuáº¥t ná»™i dung tá»« {url}: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return title, ""

def filter_article(url):
    """
    Kiá»ƒm tra xem URL cÃ³ phÃ¹ há»£p cho viá»‡c trÃ­ch xuáº¥t ná»™i dung hay khÃ´ng
    
    Args:
        url (str): URL cá»§a bÃ i viáº¿t
        
    Returns:
        bool: True náº¿u URL phÃ¹ há»£p, False náº¿u cáº§n loáº¡i bá»
    """
    # Loáº¡i bá» cÃ¡c URL tá»« vtv.vn/video/ vÃ¬ Ä‘Ã¢y lÃ  ná»™i dung video
    if "vtv.vn/video/" in url:
        logger.warning(f"Bá» qua URL video khÃ´ng phÃ¹ há»£p: {url}")
        return False
    
    # Kiá»ƒm tra cÃ¡c Ä‘á»‹nh dáº¡ng URL Ä‘a phÆ°Æ¡ng tiá»‡n khÃ¡c
    media_patterns = ["/video/", "/clip/", "/podcast/", "/photo/", "/gallery/", "/infographic/"]
    for pattern in media_patterns:
        if pattern in url.lower():
            logger.warning(f"Bá» qua URL Ä‘a phÆ°Æ¡ng tiá»‡n khÃ´ng phÃ¹ há»£p: {url}")
            return False
    
    return True

def enrich_article(driver, article):
    """
    LÃ m phong phÃº thÃªm dá»¯ liá»‡u bÃ i viáº¿t báº±ng cÃ¡ch trÃ­ch xuáº¥t ná»™i dung Ä‘áº§y Ä‘á»§ tá»« URL
    
    Args:
        driver (WebDriver): Driver Selenium
        article (dict): ThÃ´ng tin bÃ i viáº¿t (vá»›i URL nhÆ°ng chÆ°a cÃ³ ná»™i dung)
        
    Returns:
        dict: BÃ i viáº¿t Ä‘Ã£ Ä‘Æ°á»£c lÃ m phong phÃº thÃªm vá»›i tiÃªu Ä‘á» vÃ  ná»™i dung Ä‘áº§y Ä‘á»§
    """
    url = article.get("source_url")
    # Bá» qua náº¿u khÃ´ng cÃ³ URL
    if not url:
        logger.warning(f"KhÃ´ng cÃ³ URL cho bÃ i viáº¿t: {article.get('title', 'Unknown title')}")
        return article
    
    # Kiá»ƒm tra URL cÃ³ phÃ¹ há»£p hay khÃ´ng
    if not filter_article(url):
        logger.info(f"Bá» qua URL khÃ´ng phÃ¹ há»£p: {url}")
        return None
    
    try:
        # TrÃ­ch xuáº¥t tiÃªu Ä‘á» vÃ  ná»™i dung tá»« URL
        extracted_title, full_content = extract_content(driver, url, article.get("title", "Unknown title"))
        
        # Cáº­p nháº­t tiÃªu Ä‘á» náº¿u Ä‘Ã£ trÃ­ch xuáº¥t Ä‘Æ°á»£c
        if extracted_title and (not article.get("title") or article.get("title") == "Unknown title" or article.get("title").startswith("Tin tá»©c má»›i nháº¥t vá»")):
            article["title"] = extracted_title
            logger.info(f"ÄÃ£ cáº­p nháº­t tiÃªu Ä‘á»: {extracted_title[:50]}...")
        
        # Cáº­p nháº­t ná»™i dung
        if full_content:
            article["content"] = full_content
            logger.info(f"ÄÃ£ cáº­p nháº­t ná»™i dung vÄƒn báº£n cho bÃ i viáº¿t ({len(full_content.split())} tá»«)")
        
        # Cáº­p nháº­t tÃ³m táº¯t (summary) náº¿u khÃ´ng cÃ³ hoáº·c lÃ  máº·c Ä‘á»‹nh
        if not article.get("summary") or article.get("summary").startswith("BÃ i viáº¿t liÃªn quan Ä‘áº¿n"):
            # Táº¡o tÃ³m táº¯t tá»« ná»™i dung vÄƒn báº£n
            if full_content:
                words = full_content.split()
                if len(words) > 30:
                    summary = " ".join(words[:30]) + "..."
                    article["summary"] = summary
                    logger.info(f"ÄÃ£ táº¡o tÃ³m táº¯t tá»± Ä‘á»™ng: {summary[:50]}...")
        
        # Xá»­ lÃ½ meta_data (cÃ³ thá»ƒ lÃ  chuá»—i JSON hoáº·c dict)
        if isinstance(article.get("meta_data"), str):
            try:
                meta_data = json.loads(article["meta_data"])
                meta_data["extracted_at"] = datetime.now().isoformat()
                meta_data["word_count"] = len(full_content.split())
                article["meta_data"] = json.dumps(meta_data)
            except json.JSONDecodeError:
                # Náº¿u khÃ´ng pháº£i JSON há»£p lá»‡, táº¡o má»›i
                article["meta_data"] = json.dumps({
                    "extracted_at": datetime.now().isoformat(),
                    "word_count": len(full_content.split())
                })
        else:
            # Xá»­ lÃ½ trÆ°á»ng há»£p lÃ  dict
            if not article.get("meta_data"):
                article["meta_data"] = {}
            article["meta_data"]["extracted_at"] = datetime.now().isoformat()
            article["meta_data"]["word_count"] = len(full_content.split())
            
        return article
    except Exception as e:
        logger.error(f"Lá»—i khi trÃ­ch xuáº¥t ná»™i dung cho {url}: {str(e)}")
        return article

def send_to_backend(articles):
    """
    Send articles to the Laravel backend API
    
    Args:
        articles (list): List of article data to send
        
    Returns:
        bool: Success status
    """
    try:
        payload = {"articles": articles}
        headers = {"Content-Type": "application/json"}
        
        print(f"ğŸš€ Äang gá»­i {len(articles)} bÃ i viáº¿t tá»›i backend...")
        response = requests.post(BACKEND_API_URL, json=payload, headers=headers)
        
        if response.status_code in (200, 201):
            result = response.json()
            print(f"âœ… ÄÃ£ gá»­i thÃ nh cÃ´ng {result.get('message', '')}")
            if 'errors' in result and result['errors']:
                print(f"âš ï¸ CÃ³ {len(result['errors'])} lá»—i trong quÃ¡ trÃ¬nh import:")
                for error in result['errors']:
                    print(f"  - {error}")
            return True
        else:
            print(f"âŒ Lá»—i khi gá»­i bÃ i viáº¿t tá»›i backend: {response.status_code} - {response.text}")
            return False
    except Exception as e:
        print(f"âŒ Lá»—i káº¿t ná»‘i tá»›i backend: {str(e)}")
        return False

def main():
    """
    HÃ m chÃ­nh Ä‘á»ƒ trÃ­ch xuáº¥t ná»™i dung tá»« cÃ¡c URL Ä‘Ã£ thu tháº­p
    Chá»©c nÄƒng: Äá»c file JSON lÆ°u URL tá»« google_news_serpapi.py, trÃ­ch xuáº¥t ná»™i dung vÃ  tiÃªu Ä‘á», 
    rá»“i lÆ°u cÃ¡c bÃ i viáº¿t Ä‘Ã£ lÃ m phong phÃº vÃ o file má»›i vÃ  cÃ³ thá»ƒ gá»­i tá»›i backend.
    """
    # Check for auto_send argument
    auto_send = "--auto-send" in sys.argv
    
    # Get input file from command line or use latest scraped file
    input_file = None
    for arg in sys.argv[1:]:
        if not arg.startswith("--") and arg.endswith(".json"):
            input_file = arg
            break
            
    if not input_file:
        # Find latest scraped_articles file
        files = [f for f in os.listdir('.') if f.startswith('scraped_articles_') and f.endswith('.json')]
        if not files:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y file bÃ i viáº¿t. HÃ£y cháº¡y google_news_serpapi.py trÆ°á»›c!")
            return
        input_file = max(files)  # Get most recent file
    
    print(f"ğŸ“‚ Äang Ä‘á»c dá»¯ liá»‡u tá»« file: {input_file}")
    
    # Load articles from file
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            articles = json.load(f)
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c file {input_file}: {str(e)}")
        return
    
    if not articles:
        print("âŒ KhÃ´ng cÃ³ bÃ i viáº¿t nÃ o trong file!")
        return
    
    print(f"ğŸ” ÄÃ£ tÃ¬m tháº¥y {len(articles)} bÃ i viáº¿t Ä‘á»ƒ xá»­ lÃ½")
    
    # Setup WebDriver
    driver = setup_driver()
    
    try:
        # Process each article to get full content
        enriched_articles = []
        
        for i, article in enumerate(articles):
            print(f"[{i+1}/{len(articles)}] Äang xá»­ lÃ½: {article.get('title', 'Unknown title')}")
            enriched = enrich_article(driver, article)
            if enriched:
                enriched_articles.append(enriched)
            
            # Add delay between requests to avoid overloading servers
            if i < len(articles) - 1:
                time.sleep(2)
        
        # Save enriched articles to file
        output_file = f"enriched_articles_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(enriched_articles, f, ensure_ascii=False, indent=4)
        
        print(f"âœ… ÄÃ£ lÆ°u {len(enriched_articles)} bÃ i viáº¿t Ä‘Ã£ lÃ m giÃ u vÃ o {output_file}")
        
        # Automatically send to backend if there are articles
        if enriched_articles:
            send_to_backend(enriched_articles)
    
    finally:
        # Clean up
        driver.quit()
        print("ğŸ”š ÄÃ£ hoÃ n thÃ nh quÃ¡ trÃ¬nh trÃ­ch xuáº¥t ná»™i dung")

if __name__ == "__main__":
    main()
