#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Module Ä‘á»ƒ trÃ­ch xuáº¥t ná»™i dung bÃ i viáº¿t tá»« URL Ä‘Ã£ thu tháº­p.
Má»—i URL Ä‘Æ°á»£c truy cáº­p báº±ng Selenium Ä‘á»ƒ láº¥y ná»™i dung Ä‘áº§y Ä‘á»§.
Cháº¡y sau khi google_news_serpapi.py Ä‘Ã£ thu tháº­p cÃ¡c URL bÃ i viáº¿t.
"""

import json
import time
import sys
import os
import requests
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import logging
from bs4 import BeautifulSoup
from urllib.parse import urlparse

# ğŸ”¹ Laravel Backend API URL - Update as needed
BACKEND_API_URL = "http://localhost:8000/api/articles/import"

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(f"content_scraper_{datetime.now().strftime('%Y%m%d')}.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

def setup_driver():
    """Setup and return a configured Chrome WebDriver instance"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    return driver

def get_site_specific_selectors(url):
    """
    Tráº£ vá» cÃ¡c bá»™ chá»n CSS tÃ¹y chá»‰nh dá»±a trÃªn tÃªn miá»n
    
    Args:
        url (str): URL cá»§a bÃ i viáº¿t
    
    Returns:
        dict: CÃ¡c bá»™ chá»n cho tiÃªu Ä‘á» vÃ  ná»™i dung
    """
    domain = urlparse(url).netloc
    
    selectors = {
        # VnExpress
        "vnexpress.net": {
            "title": ["h1.title-detail", "h1.title-post"],
            "content": ["article.fck_detail", "div.fck_detail", ".content_detail"],
            "paragraphs": ["p.Normal", "p"],
            "exclude": [".author", ".copyright", ".relatebox", ".box-tag"]
        },
        # Tuá»•i Tráº»
        "tuoitre.vn": {
            "title": ["h1.article-title", "h1.title-2"],
            "content": ["div.content.fck", "#main-detail-body"],
            "paragraphs": ["p"],
            "exclude": [".VCSortableInPreviewMode", ".relate-container"]
        },
        # DÃ¢n TrÃ­
        "dantri.com.vn": {
            "title": ["h1.title-page", "h1.e-title"],
            "content": ["div.dt-news__content", "div.e-content"],
            "paragraphs": ["p"],
            "exclude": [".dt-news__sapo", ".author-info"]
        },
        # Thanh NiÃªn
        "thanhnien.vn": {
            "title": ["h1.detail-title", "h1.cms-title"],
            "content": ["div.detail-content", "div.cms-body"],
            "paragraphs": ["p"],
            "exclude": [".author", ".source"]
        },
        # VietnamNet
        "vietnamnet.vn": {
            "title": ["h1.content-detail-title", "h1.title"],
            "content": ["div.content-detail", ".ArticleContent", "#article-body"],
            "paragraphs": ["p"],
            "exclude": [".author-info", ".article-relate"]
        },
        # NhÃ¢n DÃ¢n
        "nhandan.vn": {
            "title": ["div.box-title h1", ".nd-detail-title"],
            "content": ["div.box-content-detail", "#nd-article-content"],
            "paragraphs": ["p"],
            "exclude": [".box-author"]
        },
        # Tiá»n Phong
        "tienphong.vn": {
            "title": ["h1.article__title", "h1.cms-title"],
            "content": ["div.article__body", ".cms-body"],
            "paragraphs": ["p"],
            "exclude": [".article__author", ".article__tag", ".article__share"]
        },
        # BÃ¡o Má»›i
        "baomoi.com": {
            "title": ["h1.bm-title", "h1.title"],
            "content": ["div.content", ".bm-content"],
            "paragraphs": ["p"],
            "exclude": [".bm-source", ".bm-resource"]
        },
        # Zing News
        "zingnews.vn": {
            "title": ["h1.the-article-title", "h1.article-title"],
            "content": ["div.the-article-body", "article.the-article-content"],
            "paragraphs": ["p"],
            "exclude": [".author", ".source", ".article-tags", ".article-related"]
        },
        # 24h
        "24h.com.vn": {
            "title": ["h1.bld", "h1.clrTit"],
            "content": ["div.text-conent", "div.baiviet-bailienquan"],
            "paragraphs": ["p"],
            "exclude": [".nguontin", ".baiviet-tags"]
        }
    }
    
    # Máº·c Ä‘á»‹nh cho cÃ¡c trang khÃ´ng cÃ³ cáº¥u hÃ¬nh cá»¥ thá»ƒ
    default_selectors = {
        "title": ["h1", "h1.title", "h1.article-title", ".headline", ".article-headline", ".entry-title"],
        "content": ["article", "main", ".content", ".article-content", ".entry-content", ".post-content"],
        "paragraphs": ["p"],
        "exclude": [".comments", ".sidebar", ".related", ".footer", ".header", ".navigation", ".menu", ".ads"]
    }
    
    # Tráº£ vá» bá»™ chá»n tÃ¹y chá»‰nh hoáº·c máº·c Ä‘á»‹nh
    for key in selectors:
        if key in domain:
            return selectors[key]
            
    return default_selectors

def extract_content(driver, url, title="Unknown title"):
    """
    TrÃ­ch xuáº¥t ná»™i dung cÃ³ cáº¥u trÃºc tá»« URL bÃ i viáº¿t, vá»›i xá»­ lÃ½ tÃ¹y chá»‰nh cho cÃ¡c trang tin tá»©c phá»• biáº¿n
    
    Args:
        driver (WebDriver): Driver Selenium
        url (str): URL cá»§a bÃ i viáº¿t
        title (str): TiÃªu Ä‘á» bÃ i viáº¿t ban Ä‘áº§u (náº¿u cÃ³)
        
    Returns:
        tuple: (extracted_title, full_content)
    """
    try:
        logger.info(f"Äang truy cáº­p URL: {url}")
        driver.get(url)
        
        # Chá» ná»™i dung táº£i xong (tá»‘i Ä‘a 20 giÃ¢y)
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # Äáº£m báº£o trang Ä‘Ã£ táº£i Ä‘áº§y Ä‘á»§ báº±ng cÃ¡ch scroll xuá»‘ng
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.5);")
        time.sleep(1)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1)
        
        # Láº¥y ná»™i dung trang
        page_content = driver.page_source
        
        # Sá»­ dá»¥ng BeautifulSoup Ä‘á»ƒ phÃ¢n tÃ­ch HTML
        soup = BeautifulSoup(page_content, "html.parser")
        
        # Láº¥y cÃ¡c bá»™ chá»n tÃ¹y chá»‰nh theo trang web
        selectors = get_site_specific_selectors(url)
        
        # ---------- TrÃ­ch xuáº¥t tiÃªu Ä‘á» ----------
        extracted_title = ""
        
        # Thá»­ cÃ¡c bá»™ chá»n tiÃªu Ä‘á» tÃ¹y chá»‰nh
        for selector in selectors["title"]:
            title_element = soup.select_one(selector)
            if title_element and title_element.text.strip():
                extracted_title = title_element.text.strip()
                logger.info(f"ÄÃ£ tÃ¬m tháº¥y tiÃªu Ä‘á» tá»« bá»™ chá»n '{selector}': {extracted_title[:50]}...")
                break
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y, thá»­ tÃ¬m tá»« cÃ¡c tháº» h1/h2 phá»• biáº¿n
        if not extracted_title:
            heading_tags = soup.find_all(['h1', 'h2'], limit=3)
            for tag in heading_tags:
                text = tag.text.strip()
                if text and len(text) > 15:  # TiÃªu Ä‘á» thÆ°á»ng dÃ i hÆ¡n 15 kÃ½ tá»±
                    extracted_title = text
                    logger.info(f"ÄÃ£ tÃ¬m tháº¥y tiÃªu Ä‘á» tá»« tháº» '{tag.name}': {extracted_title[:50]}...")
                    break
        
        # Thá»­ láº¥y title tá»« tháº» title náº¿u váº«n khÃ´ng tÃ¬m tháº¥y
        if not extracted_title and soup.title:
            # Láº¥y tá»« tháº» title, loáº¡i bá» tÃªn trang web náº¿u cÃ³
            page_title = soup.title.text.strip()
            # Loáº¡i bá» pháº§n Ä‘uÃ´i nhÆ° "| VnExpress", "- DÃ¢n TrÃ­", v.v.
            extracted_title = re.sub(r'[-|]\s*[^-|]+$', '', page_title).strip()
            logger.info(f"ÄÃ£ láº¥y tiÃªu Ä‘á» tá»« tháº» title: {extracted_title[:50]}...")
        
        # Sá»­ dá»¥ng tiÃªu Ä‘á» Ä‘Ã£ cÃ³ náº¿u váº«n khÃ´ng tÃ¬m tháº¥y
        if not extracted_title and title and title != "Unknown title":
            extracted_title = title
            logger.info(f"Sá»­ dá»¥ng tiÃªu Ä‘á» Ä‘Ã£ cÃ³: {extracted_title[:50]}...")
        
        # ---------- TrÃ­ch xuáº¥t ná»™i dung ----------
        content_element = None
        
        # Thá»­ cÃ¡c bá»™ chá»n ná»™i dung tÃ¹y chá»‰nh
        for selector in selectors["content"]:
            content_element = soup.select_one(selector)
            if content_element:
                logger.info(f"ÄÃ£ tÃ¬m tháº¥y pháº§n tá»­ ná»™i dung tá»« bá»™ chá»n '{selector}'")
                break
        
        # Loáº¡i bá» cÃ¡c pháº§n tá»­ khÃ´ng mong muá»‘n tá»« ná»™i dung
        if content_element:
            for exclude_selector in selectors["exclude"]:
                for element in content_element.select(exclude_selector):
                    element.extract()
        
        # TrÃ­ch xuáº¥t ná»™i dung tá»« cÃ¡c Ä‘oáº¡n vÄƒn
        content_paragraphs = []
        
        if content_element:
            # TÃ¬m táº¥t cáº£ cÃ¡c Ä‘oáº¡n vÄƒn trong pháº§n tá»­ ná»™i dung
            paragraphs = content_element.select(", ".join(selectors["paragraphs"]))
            
            for p in paragraphs:
                text = p.text.strip()
                # Lá»c cÃ¡c Ä‘oáº¡n vÄƒn cÃ³ nghÄ©a (loáº¡i bá» cÃ¡c Ä‘oáº¡n quÃ¡ ngáº¯n hoáº·c lÃ  thÃ´ng tin phá»¥)
                if text and len(text) > 10:  # Láº¥y Ä‘oáº¡n vÄƒn dÃ i hÆ¡n 10 kÃ½ tá»±
                    content_paragraphs.append(text)
        else:
            # Náº¿u khÃ´ng tÃ¬m tháº¥y pháº§n tá»­ ná»™i dung, tÃ¬m táº¥t cáº£ cÃ¡c tháº» p
            logger.warning(f"KhÃ´ng tÃ¬m tháº¥y pháº§n tá»­ ná»™i dung cá»¥ thá»ƒ cho URL: {url}, dÃ¹ng phÆ°Æ¡ng phÃ¡p dá»± phÃ²ng")
            
            # Loáº¡i bá» cÃ¡c pháº§n tá»­ khÃ´ng mong muá»‘n
            for selector in selectors["exclude"]:
                for element in soup.select(selector):
                    element.extract()
            
            # Loáº¡i bá» cÃ¡c pháº§n tá»­ script, style, nav, header, footer, ads
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'iframe', 'aside']):
                element.extract()
            
            # TÃ¬m táº¥t cáº£ cÃ¡c tháº» p
            paragraphs = soup.find_all('p')
            for p in paragraphs:
                text = p.text.strip()
                if text and len(text) > 20:  # Äoáº¡n vÄƒn trong ná»™i dung chÃ­nh thÆ°á»ng dÃ i hÆ¡n
                    content_paragraphs.append(text)
        
        # Káº¿t há»£p ná»™i dung thÃ nh má»™t chuá»—i vÄƒn báº£n
        full_content = ""
        
        # ThÃªm ná»™i dung cá»§a cÃ¡c Ä‘oáº¡n vÄƒn
        if content_paragraphs:
            full_content = "\n\n".join(content_paragraphs)
        else:
            # PhÆ°Æ¡ng phÃ¡p cuá»‘i cÃ¹ng: láº¥y táº¥t cáº£ vÄƒn báº£n
            logger.warning(f"KhÃ´ng tÃ¬m tháº¥y Ä‘oáº¡n vÄƒn cho URL: {url}, dÃ¹ng phÆ°Æ¡ng phÃ¡p trÃ­ch xuáº¥t vÄƒn báº£n thÃ´")
            
            # Láº¥y vÄƒn báº£n
            text = soup.get_text()
            
            # Xá»­ lÃ½ vÄƒn báº£n
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text_chunks = [chunk for chunk in chunks if chunk and len(chunk) > 20]
            
            # Lá»c cÃ¡c Ä‘oáº¡n vÄƒn cÃ³ nghÄ©a
            filtered_chunks = []
            for chunk in text_chunks:
                # Loáº¡i bá» cÃ¡c Ä‘oáº¡n quÃ¡ ngáº¯n hoáº·c chá»‰ cÃ³ kÃ½ tá»± Ä‘áº·c biá»‡t/sá»‘
                if len(chunk) > 30 and not re.match(r'^[\d\W]+$', chunk):
                    filtered_chunks.append(chunk)
            
            if filtered_chunks:
                full_content = "\n\n".join(filtered_chunks)
        
        # LÃ m sáº¡ch ná»™i dung
        # Loáº¡i bá» khoáº£ng tráº¯ng dÆ° thá»«a
        full_content = re.sub(r'\s+', ' ', full_content).strip()
        # Loáº¡i bá» cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t vÃ  dáº¥u cháº¥m cÃ¢u dÆ° thá»«a
        full_content = re.sub(r'[.]{2,}', '.', full_content)
        
        # PhÃ¢n Ä‘oáº¡n láº¡i vÄƒn báº£n Ä‘á»ƒ dá»… Ä‘á»c
        paragraphs = [p.strip() for p in full_content.split('\n\n')]
        full_content = '\n\n'.join([p for p in paragraphs if p])
        
        # Kiá»ƒm tra káº¿t quáº£
        word_count = len(full_content.split())
        if word_count < 50:
            logger.warning(f"Ná»™i dung trÃ­ch xuáº¥t quÃ¡ ngáº¯n ({word_count} tá»«) tá»«: {url}")
        else:
            logger.info(f"TrÃ­ch xuáº¥t thÃ nh cÃ´ng ná»™i dung ({word_count} tá»«) tá»«: {url}")
        
        return extracted_title, full_content
    
    except Exception as e:
        logger.error(f"Lá»—i khi trÃ­ch xuáº¥t ná»™i dung tá»« {url}: {str(e)}")
        return title, ""

def filter_article(url):
    """
    Kiá»ƒm tra xem URL cÃ³ phÃ¹ há»£p cho viá»‡c trÃ­ch xuáº¥t ná»™i dung hay khÃ´ng
    
    Args:
        url (str): URL cá»§a bÃ i viáº¿t
        
    Returns:
        bool: True náº¿u URL phÃ¹ há»£p, False náº¿u cáº§n loáº¡i bá»
    """
    # Loáº¡i bá» cÃ¡c URL tá»« vtv.vn/video/ vÃ¬ Ä‘Ã¢y lÃ  ná»™i dung video
    if "vtv.vn/video/" in url:
        logger.warning(f"Bá» qua URL video khÃ´ng phÃ¹ há»£p: {url}")
        return False
    
    # Kiá»ƒm tra cÃ¡c Ä‘á»‹nh dáº¡ng URL Ä‘a phÆ°Æ¡ng tiá»‡n khÃ¡c
    media_patterns = ["/video/", "/clip/", "/podcast/", "/photo/", "/gallery/", "/infographic/"]
    for pattern in media_patterns:
        if pattern in url.lower():
            logger.warning(f"Bá» qua URL Ä‘a phÆ°Æ¡ng tiá»‡n khÃ´ng phÃ¹ há»£p: {url}")
            return False
    
    return True

def enrich_article(driver, article):
    """
    LÃ m phong phÃº thÃªm dá»¯ liá»‡u bÃ i viáº¿t báº±ng cÃ¡ch trÃ­ch xuáº¥t ná»™i dung Ä‘áº§y Ä‘á»§ tá»« URL
    
    Args:
        driver (WebDriver): Driver Selenium
        article (dict): ThÃ´ng tin bÃ i viáº¿t (vá»›i URL nhÆ°ng chÆ°a cÃ³ ná»™i dung)
        
    Returns:
        dict: BÃ i viáº¿t Ä‘Ã£ Ä‘Æ°á»£c lÃ m phong phÃº thÃªm vá»›i tiÃªu Ä‘á» vÃ  ná»™i dung Ä‘áº§y Ä‘á»§
    """
    url = article.get("source_url")
    # Bá» qua náº¿u khÃ´ng cÃ³ URL
    if not url:
        logger.warning(f"KhÃ´ng cÃ³ URL cho bÃ i viáº¿t: {article.get('title', 'Unknown title')}")
        return article
    
    # Kiá»ƒm tra URL cÃ³ phÃ¹ há»£p hay khÃ´ng
    if not filter_article(url):
        logger.info(f"Bá» qua URL khÃ´ng phÃ¹ há»£p: {url}")
        return None
    
    try:
        # TrÃ­ch xuáº¥t tiÃªu Ä‘á» vÃ  ná»™i dung tá»« URL
        extracted_title, full_content = extract_content(driver, url, article.get("title", "Unknown title"))
        
        # Cáº­p nháº­t tiÃªu Ä‘á» náº¿u Ä‘Ã£ trÃ­ch xuáº¥t Ä‘Æ°á»£c
        if extracted_title and (not article.get("title") or article.get("title") == "Unknown title" or article.get("title").startswith("Tin tá»©c má»›i nháº¥t vá»")):
            article["title"] = extracted_title
            logger.info(f"ÄÃ£ cáº­p nháº­t tiÃªu Ä‘á»: {extracted_title[:50]}...")
        
        # Cáº­p nháº­t ná»™i dung
        article["content"] = full_content
        
        # Cáº­p nháº­t tÃ³m táº¯t (summary) náº¿u khÃ´ng cÃ³ hoáº·c lÃ  máº·c Ä‘á»‹nh
        if not article.get("summary") or article.get("summary").startswith("BÃ i viáº¿t liÃªn quan Ä‘áº¿n"):
            # Táº¡o tÃ³m táº¯t tá»« 200 kÃ½ tá»± Ä‘áº§u tiÃªn cá»§a ná»™i dung
            if full_content:
                words = full_content.split()
                if len(words) > 30:  # Náº¿u cÃ³ Ã­t nháº¥t 30 tá»«
                    summary = " ".join(words[:30]) + "..."
                    article["summary"] = summary
                    logger.info(f"ÄÃ£ táº¡o tÃ³m táº¯t tá»± Ä‘á»™ng: {summary[:50]}...")
        
        # Xá»­ lÃ½ meta_data (cÃ³ thá»ƒ lÃ  chuá»—i JSON hoáº·c dict)
        if isinstance(article.get("meta_data"), str):
            try:
                meta_data = json.loads(article["meta_data"])
                meta_data["extracted_at"] = datetime.now().isoformat()
                meta_data["word_count"] = len(full_content.split())
                article["meta_data"] = json.dumps(meta_data)
            except json.JSONDecodeError:
                # Náº¿u khÃ´ng pháº£i JSON há»£p lá»‡, táº¡o má»›i
                article["meta_data"] = json.dumps({
                    "extracted_at": datetime.now().isoformat(),
                    "word_count": len(full_content.split())
                })
        else:
            # Xá»­ lÃ½ trÆ°á»ng há»£p lÃ  dict
            if not article.get("meta_data"):
                article["meta_data"] = {}
            article["meta_data"]["extracted_at"] = datetime.now().isoformat()
            article["meta_data"]["word_count"] = len(full_content.split())
            
        return article
    except Exception as e:
        logger.error(f"Lá»—i khi trÃ­ch xuáº¥t ná»™i dung cho {url}: {str(e)}")
        return article

def send_to_backend(articles):
    """
    Send articles to the Laravel backend API
    
    Args:
        articles (list): List of article data to send
        
    Returns:
        bool: Success status
    """
    try:
        payload = {"articles": articles}
        headers = {"Content-Type": "application/json"}
        
        response = requests.post(BACKEND_API_URL, json=payload, headers=headers)
        
        if response.status_code in (200, 201):
            result = response.json()
            print(f"âœ… ÄÃ£ gá»­i thÃ nh cÃ´ng {result.get('message', '')}")
            if 'errors' in result and result['errors']:
                print(f"âš ï¸ CÃ³ {len(result['errors'])} lá»—i trong quÃ¡ trÃ¬nh import:")
                for error in result['errors']:
                    print(f"  - {error}")
            return True
        else:
            print(f"âŒ Lá»—i khi gá»­i bÃ i viáº¿t tá»›i backend: {response.status_code} - {response.text}")
            return False
    except Exception as e:
        print(f"âŒ Lá»—i káº¿t ná»‘i tá»›i backend: {str(e)}")
        return False

def main():
    """
    HÃ m chÃ­nh Ä‘á»ƒ trÃ­ch xuáº¥t ná»™i dung tá»« cÃ¡c URL Ä‘Ã£ thu tháº­p
    Chá»©c nÄƒng: Äá»c file JSON lÆ°u URL tá»« google_news_serpapi.py, trÃ­ch xuáº¥t ná»™i dung vÃ  tiÃªu Ä‘á», 
    rá»“i lÆ°u cÃ¡c bÃ i viáº¿t Ä‘Ã£ lÃ m phong phÃº vÃ o file má»›i vÃ  cÃ³ thá»ƒ gá»­i tá»›i backend.
    """
    # Get input file from command line or use latest scraped file
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
    else:
        # Find latest scraped_articles file
        files = [f for f in os.listdir('.') if f.startswith('scraped_articles_') and f.endswith('.json')]
        if not files:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y file bÃ i viáº¿t. HÃ£y cháº¡y google_news_serpapi.py trÆ°á»›c!")
            return
        input_file = max(files)  # Get most recent file
    
    print(f"ğŸ“‚ Äang Ä‘á»c dá»¯ liá»‡u tá»« file: {input_file}")
    
    # Load articles from file
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            articles = json.load(f)
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c file {input_file}: {str(e)}")
        return
    
    if not articles:
        print("âŒ KhÃ´ng cÃ³ bÃ i viáº¿t nÃ o trong file!")
        return
    
    print(f"ğŸ” ÄÃ£ tÃ¬m tháº¥y {len(articles)} bÃ i viáº¿t Ä‘á»ƒ xá»­ lÃ½")
    
    # Setup WebDriver
    driver = setup_driver()
    
    try:
        # Process each article to get full content
        enriched_articles = []
        
        for i, article in enumerate(articles):
            print(f"[{i+1}/{len(articles)}] Äang xá»­ lÃ½: {article.get('title', 'Unknown title')}")
            enriched = enrich_article(driver, article)
            if enriched:
                enriched_articles.append(enriched)
            
            # Add delay between requests to avoid overloading servers
            if i < len(articles) - 1:
                time.sleep(2)
        
        # Save enriched articles to file
        output_file = f"enriched_articles_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(enriched_articles, f, ensure_ascii=False, indent=4)
        
        print(f"âœ… ÄÃ£ lÆ°u {len(enriched_articles)} bÃ i viáº¿t Ä‘Ã£ lÃ m giÃ u vÃ o {output_file}")
        
        # Ask to send to backend
        if enriched_articles:
            send_option = input("Báº¡n cÃ³ muá»‘n gá»­i bÃ i viáº¿t tá»›i backend? (y/n): ").lower()
            if send_option == 'y':
                send_to_backend(enriched_articles)
    
    finally:
        # Clean up
        driver.quit()
        print("ğŸ”š ÄÃ£ hoÃ n thÃ nh quÃ¡ trÃ¬nh trÃ­ch xuáº¥t ná»™i dung")

if __name__ == "__main__":
    main()
