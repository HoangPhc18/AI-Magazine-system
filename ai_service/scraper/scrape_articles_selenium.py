#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Module ƒë·ªÉ tr√≠ch xu·∫•t n·ªôi dung b√†i vi·∫øt t·ª´ URL ƒë√£ thu th·∫≠p.
M·ªói URL ƒë∆∞·ª£c truy c·∫≠p b·∫±ng Selenium ƒë·ªÉ l·∫•y n·ªôi dung ƒë·∫ßy ƒë·ªß.
Ch·∫°y sau khi google_news_serpapi.py ƒë√£ thu th·∫≠p c√°c URL b√†i vi·∫øt.
"""

import json
import time
import sys
import os
import requests
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import logging
from bs4 import BeautifulSoup
from urllib.parse import urlparse

# üîπ Laravel Backend API URL - Update as needed
BACKEND_API_URL = "http://localhost:8000/api/articles/import"

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(f"content_scraper_{datetime.now().strftime('%Y%m%d')}.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

def setup_driver():
    """Setup and return a configured Chrome WebDriver instance"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    return driver

def get_site_specific_selectors(url):
    """
    Tr·∫£ v·ªÅ c√°c b·ªô ch·ªçn CSS t√πy ch·ªânh d·ª±a tr√™n t√™n mi·ªÅn
    
    Args:
        url (str): URL c·ªßa b√†i vi·∫øt
    
    Returns:
        dict: C√°c b·ªô ch·ªçn cho ti√™u ƒë·ªÅ v√† n·ªôi dung
    """
    domain = urlparse(url).netloc
    
    selectors = {
        # VnExpress
        "vnexpress.net": {
            "title": ["h1.title-detail", "h1.title-post", ".title-news", ".title_news_detail"],
            "content": ["article.fck_detail", "div.fck_detail", ".content_detail", "article.content-detail"],
            "paragraphs": ["p.Normal", "p"],
            "exclude": [".author", ".copyright", ".relatebox", ".box-tag", ".social_pin", ".list-news", ".width_common", ".footer", ".header"]
        },
        # Tu·ªïi Tr·∫ª
        "tuoitre.vn": {
            "title": ["h1.article-title", "h1.title-2", "h1.detail-title", ".article-title"],
            "content": ["div.content.fck", "#main-detail-body", "div.detail-content", "#mainContent", ".content-news", ".detail-content-body"],
            "paragraphs": ["p"],
            "exclude": [".VCSortableInPreviewMode", ".relate-container", ".source", ".author", ".date-time"]
        },
        # D√¢n Tr√≠
        "dantri.com.vn": {
            "title": ["h1.title-page", "h1.e-title", "h1.title", ".e-magazine", ".title-news"],
            "content": ["div.dt-news__content", "div.e-content", ".singular-content", ".article-body", ".dt-news__body"],
            "paragraphs": ["p"],
            "exclude": [".dt-news__sapo", ".author-info", ".article-topic", ".e-magazineplus", ".dt-news__meta"]
        },
        # Thanh Ni√™n
        "thanhnien.vn": {
            "title": ["h1.detail-title", "h1.cms-title", ".details__headline", "h1.title-news"],
            "content": ["div.detail-content", "div.cms-body", ".l-content", ".details__content", "#abody", "#content-id"],
            "paragraphs": ["p"],
            "exclude": [".author", ".source", ".details__meta", ".details__author", ".related-container"]
        },
        # VietnamNet
        "vietnamnet.vn": {
            "title": ["h1.content-detail-title", "h1.title", ".content-title", "h1.title-item-news", ".title-content"],
            "content": ["div.content-detail", ".ArticleContent", "#article-body", ".articleContent", ".boxPostDetail", ".detail-content"],
            "paragraphs": ["p"],
            "exclude": [".author-info", ".article-relate", ".box-taitro", ".box-title", ".article-tags"]
        },
        # Nh√¢n D√¢n
        "nhandan.vn": {
            "title": ["div.box-title h1", ".nd-detail-title", ".title-detail", ".article-title", "h1.article-title"],
            "content": ["div.box-content-detail", "#nd-article-content", ".article-body", ".detail-content-wrap"],
            "paragraphs": ["p"],
            "exclude": [".box-author", ".article-meta", ".box-share"]
        },
        # Ti·ªÅn Phong
        "tienphong.vn": {
            "title": ["h1.article__title", "h1.cms-title", ".headline", ".main-article-title", "h1.article-title"],
            "content": ["div.article__body", ".cms-body", ".article-body", ".article-content", ".main-article-body"],
            "paragraphs": ["p"],
            "exclude": [".article__author", ".article__tag", ".article__share", ".article__meta"]
        },
        # B√°o M·ªõi
        "baomoi.com": {
            "title": ["h1.bm-title", "h1.title", ".article-title", ".article-header", ".title", ".headline"],
            "content": ["div.content", ".bm-content", ".article-body", ".story__content", ".article__content"],
            "paragraphs": ["p"],
            "exclude": [".bm-source", ".bm-resource", ".relate-container", ".bm-avatar", ".top-comments"]
        },
        # Zing News
        "zingnews.vn": {
            "title": ["h1.the-article-title", "h1.article-title", ".the-article-header", ".page-title", ".article-header h1"],
            "content": ["div.the-article-body", "article.the-article-content", ".the-article-content", ".article-content"],
            "paragraphs": ["p"],
            "exclude": [".author", ".source", ".article-tags", ".article-related", ".the-article-tags", ".the-article-meta"]
        },
        # 24h
        "24h.com.vn": {
            "title": ["h1.bld", "h1.clrTit", ".titCM", ".tuht-dts", "article h1"],
            "content": ["div.text-conent", "div.baiviet-bailienquan", ".colCenter-in", ".boxDtlBody", "article .ctTp"],
            "paragraphs": ["p"],
            "exclude": [".nguontin", ".baiviet-tags", ".bv-cp", ".fb-like", ".imgCation"]
        },
        # CafeF
        "cafef.vn": {
            "title": ["h1.title", ".articledetail_title", ".title-detail", "h1.title_detail"],
            "content": ["div#mainContent", ".contentdetail", ".article-body", "#CMS_Detail", "#content_detail_news"],
            "paragraphs": ["p"],
            "exclude": [".author", ".source", ".relationnews", ".tindlienquan"]
        },
        # VOV
        "vov.vn": {
            "title": ["h1.article-title", ".main-article h1", ".cms-title", ".vovtitle"],
            "content": ["div.article-body", ".main-article-body", ".article-content", "#article_content", ".vov-content"],
            "paragraphs": ["p"],
            "exclude": [".article-author", ".article-tools", ".article-related"]
        },
        # Lao ƒê·ªông
        "laodong.vn": {
            "title": ["h1.title", ".article-title", ".headline", "h1.headline_detail"],
            "content": ["div.article-content", ".cms-body", ".contentbody", ".detail-content-body", "#box_details"],
            "paragraphs": ["p"],
            "exclude": [".author", ".source", ".article-meta", ".article-tools", ".boxrelation"]
        },
        # B√°o Thanh tra
        "thanhtra.com.vn": {
            "title": ["h1.title", ".article-title", ".news-title"],
            "content": [".article-body", ".news-content", ".content-body"],
            "paragraphs": ["p"],
            "exclude": [".author", ".article-info", ".tags"]
        },
        # Ph√°p Lu·∫≠t TP.HCM
        "plo.vn": {
            "title": ["h1.title", ".article__title", ".article-title"],
            "content": [".article__body", ".article-content", ".cms-body"],
            "paragraphs": ["p"],
            "exclude": [".author", ".source", ".tags-container"]
        },
        # S√†i G√≤n Gi·∫£i Ph√≥ng
        "sggp.org.vn": {
            "title": ["h1.title", ".cms-title", ".article-title"],
            "content": [".article-content", ".cms-body", "#content_detail_news"],
            "paragraphs": ["p"],
            "exclude": [".author", ".nguon", ".source"]
        }
    }
    
    # M·∫∑c ƒë·ªãnh cho c√°c trang kh√¥ng c√≥ c·∫•u h√¨nh c·ª• th·ªÉ
    default_selectors = {
        "title": ["h1", "h1.title", "h1.article-title", ".headline", ".article-headline", ".entry-title", ".post-title", ".main-title"],
        "content": ["article", "main", ".content", ".article-content", ".entry-content", ".post-content", ".news-content", ".main-content", "#content", "#main"],
        "paragraphs": ["p"],
        "exclude": [".comments", ".sidebar", ".related", ".footer", ".header", ".navigation", ".menu", ".ads", ".social", ".sharing", ".tags", ".author", ".meta", ".date"]
    }
    
    # Tr·∫£ v·ªÅ b·ªô ch·ªçn t√πy ch·ªânh ho·∫∑c m·∫∑c ƒë·ªãnh
    for key in selectors:
        if key in domain:
            return selectors[key]
            
    return default_selectors

def extract_content(driver, url, title="Unknown title"):
    """
    Tr√≠ch xu·∫•t n·ªôi dung d·∫°ng vƒÉn b·∫£n thu·∫ßn t√∫y t·ª´ URL b√†i vi·∫øt, v·ªõi x·ª≠ l√Ω t√πy ch·ªânh cho c√°c trang tin t·ª©c ph·ªï bi·∫øn.
    Gi·ªØ nguy√™n n·ªôi dung ti√™u ƒë·ªÅ v√† b√†i vi·∫øt kh√¥ng s·ª≠a ƒë·ªïi ƒë·ªÉ l∆∞u v√†o database.
    
    Args:
        driver (WebDriver): Driver Selenium
        url (str): URL c·ªßa b√†i vi·∫øt
        title (str): Ti√™u ƒë·ªÅ b√†i vi·∫øt ban ƒë·∫ßu (n·∫øu c√≥)
        
    Returns:
        tuple: (extracted_title, full_content)
    """
    try:
        logger.info(f"ƒêang truy c·∫≠p URL: {url}")
        driver.get(url)
        
        # Ch·ªù n·ªôi dung t·∫£i xong (t·ªëi ƒëa 20 gi√¢y)
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # ƒê·∫£m b·∫£o trang ƒë√£ t·∫£i ƒë·∫ßy ƒë·ªß b·∫±ng c√°ch scroll xu·ªëng
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.3);")
        time.sleep(1.5)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.7);")
        time.sleep(1.5)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.5)
        
        # L·∫•y n·ªôi dung trang
        page_content = driver.page_source
        
        # S·ª≠ d·ª•ng BeautifulSoup ƒë·ªÉ ph√¢n t√≠ch HTML
        soup = BeautifulSoup(page_content, "html.parser")
        
        # X√°c ƒë·ªãnh t√™n mi·ªÅn ƒë·ªÉ √°p d·ª•ng selector ph√π h·ª£p
        domain = urlparse(url).netloc
        logger.info(f"ƒêang x·ª≠ l√Ω trang web: {domain}")
        
        # L·∫•y c√°c b·ªô ch·ªçn t√πy ch·ªânh theo trang web
        selectors = get_site_specific_selectors(url)
        
        # ---------- Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ ----------
        extracted_title = ""
        
        # Th·ª≠ c√°c b·ªô ch·ªçn ti√™u ƒë·ªÅ t√πy ch·ªânh
        for selector in selectors["title"]:
            title_elements = soup.select(selector)
            if title_elements:
                for title_element in title_elements:
                    if title_element and title_element.text.strip():
                        extracted_title = title_element.text.strip()
                        logger.info(f"ƒê√£ t√¨m th·∫•y ti√™u ƒë·ªÅ t·ª´ b·ªô ch·ªçn '{selector}': {extracted_title[:50]}...")
                        break
                if extracted_title:
                    break
        
        # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ t√¨m t·ª´ c√°c th·∫ª h1/h2 ph·ªï bi·∫øn
        if not extracted_title:
            heading_tags = soup.find_all(['h1', 'h2'], limit=5)
            for tag in heading_tags:
                text = tag.text.strip()
                if text and len(text) > 15:  # Ti√™u ƒë·ªÅ th∆∞·ªùng d√†i h∆°n 15 k√Ω t·ª±
                    extracted_title = text
                    logger.info(f"ƒê√£ t√¨m th·∫•y ti√™u ƒë·ªÅ t·ª´ th·∫ª '{tag.name}': {extracted_title[:50]}...")
                    break
        
        # Th·ª≠ l·∫•y title t·ª´ th·∫ª title n·∫øu v·∫´n kh√¥ng t√¨m th·∫•y
        if not extracted_title and soup.title:
            # L·∫•y t·ª´ th·∫ª title, lo·∫°i b·ªè t√™n trang web n·∫øu c√≥
            page_title = soup.title.text.strip()
            # Lo·∫°i b·ªè ph·∫ßn ƒëu√¥i nh∆∞ "| VnExpress", "- D√¢n Tr√≠", v.v.
            extracted_title = re.sub(r'[-|]\s*[^-|]+$', '', page_title).strip()
            logger.info(f"ƒê√£ l·∫•y ti√™u ƒë·ªÅ t·ª´ th·∫ª title: {extracted_title[:50]}...")
        
        # S·ª≠ d·ª•ng ti√™u ƒë·ªÅ ƒë√£ c√≥ n·∫øu v·∫´n kh√¥ng t√¨m th·∫•y
        if not extracted_title and title and title != "Unknown title":
            extracted_title = title
            logger.info(f"S·ª≠ d·ª•ng ti√™u ƒë·ªÅ ƒë√£ c√≥: {extracted_title[:50]}...")
            
        # ---------- Tr√≠ch xu·∫•t n·ªôi dung ----------
        content_element = None
        
        # Th·ª≠ c√°c b·ªô ch·ªçn n·ªôi dung t√πy ch·ªânh
        for selector in selectors["content"]:
            content_elements = soup.select(selector)
            if content_elements:
                # Ch·ªçn ph·∫ßn t·ª≠ n·ªôi dung l·ªõn nh·∫•t (c√≥ nhi·ªÅu text nh·∫•t)
                max_length = 0
                for element in content_elements:
                    text_length = len(element.get_text())
                    if text_length > max_length:
                        max_length = text_length
                        content_element = element
                
                if content_element:
                    logger.info(f"ƒê√£ t√¨m th·∫•y ph·∫ßn t·ª≠ n·ªôi dung t·ª´ b·ªô ch·ªçn '{selector}' ({max_length} k√Ω t·ª±)")
                    break
        
        # Tr√≠ch xu·∫•t n·ªôi dung vƒÉn b·∫£n thu·∫ßn t√∫y
        if content_element:
            # Tr∆∞·ªõc khi tr√≠ch xu·∫•t, lo·∫°i b·ªè c√°c ph·∫ßn t·ª≠ kh√¥ng mong mu·ªën t·ª´ n·ªôi dung
            for exclude_selector in selectors["exclude"]:
                for element in content_element.select(exclude_selector):
                    element.extract()
            
            # L·∫•y vƒÉn b·∫£n thu·∫ßn t√∫y, gi·ªØ nguy√™n c√°ch ƒëo·∫°n
            paragraphs = []
            
            # T√¨m t·∫•t c·∫£ c√°c ƒëo·∫°n vƒÉn trong ph·∫ßn t·ª≠ n·ªôi dung
            if "," in ", ".join(selectors["paragraphs"]):
                p_elements = content_element.select(", ".join(selectors["paragraphs"]))
            else:
                p_elements = content_element.find_all(selectors["paragraphs"][0])
                
            # N·∫øu kh√¥ng t√¨m th·∫•y ƒëo·∫°n vƒÉn, l·∫•y t·∫•t c·∫£ vƒÉn b·∫£n trong ph·∫ßn t·ª≠ n·ªôi dung
            if not p_elements or len(p_elements) < 3:
                # Th·ª≠ t√¨m t·∫•t c·∫£ c√°c th·∫ª text n√≥i chung
                all_text_elements = [el for el in content_element.find_all(text=True) if el.parent.name not in ['script', 'style']]
                clean_text = []
                for el in all_text_elements:
                    text = el.strip()
                    if text and len(text) > 10:
                        clean_text.append(text)
                
                if clean_text:
                    full_content = "\n\n".join(clean_text)
                else:
                    full_content = content_element.get_text(separator="\n\n", strip=True)
                
                logger.info(f"ƒê√£ tr√≠ch xu·∫•t n·ªôi dung vƒÉn b·∫£n ({len(full_content.split())} t·ª´) t·ª´: {url}")
            else:
                # L·∫•y n·ªôi dung t·ª´ t·ª´ng ƒëo·∫°n vƒÉn
                for p in p_elements:
                    text = p.text.strip()
                    if text:  # L·∫•y t·∫•t c·∫£ ƒëo·∫°n vƒÉn c√≥ n·ªôi dung
                        paragraphs.append(text)
                
                full_content = "\n\n".join(paragraphs)
                logger.info(f"ƒê√£ tr√≠ch xu·∫•t n·ªôi dung t·ª´ {len(paragraphs)} ƒëo·∫°n vƒÉn, t·ªïng c·ªông {len(full_content.split())} t·ª´")
        else:
            # N·∫øu kh√¥ng t√¨m th·∫•y ph·∫ßn t·ª≠ n·ªôi dung, th·ª≠ ph∆∞∆°ng ph√°p ch·ªß ƒë·ªông h∆°n
            logger.warning(f"Kh√¥ng t√¨m th·∫•y ph·∫ßn t·ª≠ n·ªôi dung c·ª• th·ªÉ cho URL: {url}, d√πng ph∆∞∆°ng ph√°p d·ª± ph√≤ng")
            
            # Ph∆∞∆°ng ph√°p 1: T√¨m ph·∫ßn t·ª≠ c√≥ nhi·ªÅu th·∫ª <p> nh·∫•t
            article_candidates = []
            
            for tag in ['article', 'main', 'div.content', 'div.article', '.detail', '#detail', '#content']:
                elements = soup.select(tag)
                for element in elements:
                    p_count = len(element.find_all('p'))
                    if p_count >= 3:  # √çt nh·∫•t 3 ƒëo·∫°n vƒÉn
                        article_candidates.append((element, p_count))
            
            if article_candidates:
                # S·∫Øp x·∫øp theo s·ªë l∆∞·ª£ng th·∫ª p, l·∫•y ph·∫ßn t·ª≠ c√≥ nhi·ªÅu th·∫ª p nh·∫•t
                article_candidates.sort(key=lambda x: x[1], reverse=True)
                content_element = article_candidates[0][0]
                logger.info(f"ƒê√£ t√¨m th·∫•y ph·∫ßn t·ª≠ n·ªôi dung b·∫±ng ph∆∞∆°ng ph√°p ph√¢n t√≠ch c·∫•u tr√∫c ({article_candidates[0][1]} ƒëo·∫°n vƒÉn)")
                
                # L·∫•y t·∫•t c·∫£ c√°c ƒëo·∫°n vƒÉn t·ª´ ph·∫ßn t·ª≠ n√†y
                paragraphs = []
                for p in content_element.find_all('p'):
                    text = p.text.strip()
                    if text and len(text) > 15:  # Ch·ªâ l·∫•y ƒëo·∫°n vƒÉn c√≥ ƒë·ªß n·ªôi dung
                        paragraphs.append(text)
                
                if paragraphs:
                    full_content = "\n\n".join(paragraphs)
                    logger.info(f"ƒê√£ tr√≠ch xu·∫•t n·ªôi dung d·ª± ph√≤ng t·ª´ {len(paragraphs)} ƒëo·∫°n vƒÉn, t·ªïng c·ªông {len(full_content.split())} t·ª´")
                else:
                    # Kh√¥ng c√≥ ƒëo·∫°n vƒÉn, l·∫•y t·∫•t c·∫£ vƒÉn b·∫£n
                    full_content = content_element.get_text(separator="\n\n", strip=True)
                    logger.info(f"ƒê√£ tr√≠ch xu·∫•t to√†n b·ªô vƒÉn b·∫£n t·ª´ ph·∫ßn t·ª≠ n·ªôi dung ({len(full_content.split())} t·ª´)")
            else:
                # Ph∆∞∆°ng ph√°p 2: L·∫•y t·∫•t c·∫£ c√°c ƒëo·∫°n vƒÉn t·ª´ trang
                paragraphs = []
                for p in soup.find_all('p'):
                    text = p.text.strip()
                    if text and len(text) > 20:  # Ch·ªâ l·∫•y ƒëo·∫°n vƒÉn c√≥ ƒë·ªß n·ªôi dung
                        paragraphs.append(text)
                
                if paragraphs:
                    full_content = "\n\n".join(paragraphs)
                    logger.info(f"ƒê√£ tr√≠ch xu·∫•t n·ªôi dung t·ª´ t·∫•t c·∫£ c√°c ƒëo·∫°n vƒÉn tr√™n trang ({len(paragraphs)} ƒëo·∫°n)")
                else:
                    # Ph∆∞∆°ng ph√°p cu·ªëi c√πng: l·∫•y to√†n b·ªô vƒÉn b·∫£n t·ª´ trang
                    main_content = soup.find('main') or soup.find('article') or soup.find('body')
                    full_content = main_content.get_text(separator="\n\n", strip=True)
                    logger.info(f"ƒê√£ tr√≠ch xu·∫•t vƒÉn b·∫£n t·ªïng th·ªÉ ({len(full_content.split())} t·ª´)")
        
        # Lo·∫°i b·ªè c√°c d√≤ng tr·ªëng v√† l√†m s·∫°ch n·ªôi dung
        if full_content:
            # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng d∆∞ th·ª´a
            full_content = re.sub(r'\n{3,}', '\n\n', full_content)
            
            # N·∫øu n·ªôi dung qu√° ng·∫Øn, th√¥ng b√°o c·∫£nh b√°o
            if len(full_content.split()) < 50:
                logger.warning(f"N·ªôi dung tr√≠ch xu·∫•t qu√° ng·∫Øn ({len(full_content.split())} t·ª´), c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c")
        
        return extracted_title, full_content
    
    except Exception as e:
        logger.error(f"L·ªói khi tr√≠ch xu·∫•t n·ªôi dung t·ª´ {url}: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return title, ""

def filter_article(url):
    """
    Ki·ªÉm tra xem URL c√≥ ph√π h·ª£p cho vi·ªác tr√≠ch xu·∫•t n·ªôi dung hay kh√¥ng
    
    Args:
        url (str): URL c·ªßa b√†i vi·∫øt
        
    Returns:
        bool: True n·∫øu URL ph√π h·ª£p, False n·∫øu c·∫ßn lo·∫°i b·ªè
    """
    # Lo·∫°i b·ªè c√°c URL t·ª´ vtv.vn/video/ v√¨ ƒë√¢y l√† n·ªôi dung video
    if "vtv.vn/video/" in url:
        logger.warning(f"B·ªè qua URL video kh√¥ng ph√π h·ª£p: {url}")
        return False
    
    # Ki·ªÉm tra c√°c ƒë·ªãnh d·∫°ng URL ƒëa ph∆∞∆°ng ti·ªán kh√°c
    media_patterns = ["/video/", "/clip/", "/podcast/", "/photo/", "/gallery/", "/infographic/"]
    for pattern in media_patterns:
        if pattern in url.lower():
            logger.warning(f"B·ªè qua URL ƒëa ph∆∞∆°ng ti·ªán kh√¥ng ph√π h·ª£p: {url}")
            return False
    
    return True

def enrich_article(driver, article):
    """
    L√†m phong ph√∫ th√™m d·ªØ li·ªáu b√†i vi·∫øt b·∫±ng c√°ch tr√≠ch xu·∫•t n·ªôi dung ƒë·∫ßy ƒë·ªß t·ª´ URL
    
    Args:
        driver (WebDriver): Driver Selenium
        article (dict): Th√¥ng tin b√†i vi·∫øt (v·ªõi URL nh∆∞ng ch∆∞a c√≥ n·ªôi dung)
        
    Returns:
        dict: B√†i vi·∫øt ƒë√£ ƒë∆∞·ª£c l√†m phong ph√∫ th√™m v·ªõi ti√™u ƒë·ªÅ v√† n·ªôi dung ƒë·∫ßy ƒë·ªß
    """
    url = article.get("source_url")
    # B·ªè qua n·∫øu kh√¥ng c√≥ URL
    if not url:
        logger.warning(f"Kh√¥ng c√≥ URL cho b√†i vi·∫øt: {article.get('title', 'Unknown title')}")
        return article
    
    # Ki·ªÉm tra URL c√≥ ph√π h·ª£p hay kh√¥ng
    if not filter_article(url):
        logger.info(f"B·ªè qua URL kh√¥ng ph√π h·ª£p: {url}")
        return None
    
    try:
        # Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ v√† n·ªôi dung t·ª´ URL
        extracted_title, full_content = extract_content(driver, url, article.get("title", "Unknown title"))
        
        # C·∫≠p nh·∫≠t ti√™u ƒë·ªÅ n·∫øu ƒë√£ tr√≠ch xu·∫•t ƒë∆∞·ª£c
        if extracted_title and (not article.get("title") or article.get("title") == "Unknown title" or article.get("title").startswith("Tin t·ª©c m·ªõi nh·∫•t v·ªÅ")):
            article["title"] = extracted_title
            logger.info(f"ƒê√£ c·∫≠p nh·∫≠t ti√™u ƒë·ªÅ: {extracted_title[:50]}...")
        
        # C·∫≠p nh·∫≠t n·ªôi dung
        if full_content:
            article["content"] = full_content
            logger.info(f"ƒê√£ c·∫≠p nh·∫≠t n·ªôi dung vƒÉn b·∫£n cho b√†i vi·∫øt ({len(full_content.split())} t·ª´)")
        
        # C·∫≠p nh·∫≠t t√≥m t·∫Øt (summary) n·∫øu kh√¥ng c√≥ ho·∫∑c l√† m·∫∑c ƒë·ªãnh
        if not article.get("summary") or article.get("summary").startswith("B√†i vi·∫øt li√™n quan ƒë·∫øn"):
            # T·∫°o t√≥m t·∫Øt t·ª´ n·ªôi dung vƒÉn b·∫£n
            if full_content:
                words = full_content.split()
                if len(words) > 30:
                    summary = " ".join(words[:30]) + "..."
                    article["summary"] = summary
                    logger.info(f"ƒê√£ t·∫°o t√≥m t·∫Øt t·ª± ƒë·ªông: {summary[:50]}...")
        
        # X·ª≠ l√Ω meta_data (c√≥ th·ªÉ l√† chu·ªói JSON ho·∫∑c dict)
        if isinstance(article.get("meta_data"), str):
            try:
                meta_data = json.loads(article["meta_data"])
                meta_data["extracted_at"] = datetime.now().isoformat()
                meta_data["word_count"] = len(full_content.split())
                article["meta_data"] = json.dumps(meta_data)
            except json.JSONDecodeError:
                # N·∫øu kh√¥ng ph·∫£i JSON h·ª£p l·ªá, t·∫°o m·ªõi
                article["meta_data"] = json.dumps({
                    "extracted_at": datetime.now().isoformat(),
                    "word_count": len(full_content.split())
                })
        else:
            # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p l√† dict
            if not article.get("meta_data"):
                article["meta_data"] = {}
            article["meta_data"]["extracted_at"] = datetime.now().isoformat()
            article["meta_data"]["word_count"] = len(full_content.split())
            
        return article
    except Exception as e:
        logger.error(f"L·ªói khi tr√≠ch xu·∫•t n·ªôi dung cho {url}: {str(e)}")
        return article

def send_to_backend(articles):
    """
    Send articles to the Laravel backend API
    
    Args:
        articles (list): List of article data to send
        
    Returns:
        bool: Success status
    """
    try:
        payload = {"articles": articles}
        headers = {"Content-Type": "application/json"}
        
        print(f"üöÄ ƒêang g·ª≠i {len(articles)} b√†i vi·∫øt t·ªõi backend...")
        response = requests.post(BACKEND_API_URL, json=payload, headers=headers)
        
        if response.status_code in (200, 201):
            result = response.json()
            print(f"‚úÖ ƒê√£ g·ª≠i th√†nh c√¥ng {result.get('message', '')}")
            if 'errors' in result and result['errors']:
                print(f"‚ö†Ô∏è C√≥ {len(result['errors'])} l·ªói trong qu√° tr√¨nh import:")
                for error in result['errors']:
                    print(f"  - {error}")
            return True
        else:
            print(f"‚ùå L·ªói khi g·ª≠i b√†i vi·∫øt t·ªõi backend: {response.status_code} - {response.text}")
            return False
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi t·ªõi backend: {str(e)}")
        return False

def main():
    """
    H√†m ch√≠nh ƒë·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ c√°c URL ƒë√£ thu th·∫≠p
    Ch·ª©c nƒÉng: ƒê·ªçc file JSON l∆∞u URL t·ª´ google_news_serpapi.py, tr√≠ch xu·∫•t n·ªôi dung v√† ti√™u ƒë·ªÅ, 
    r·ªìi l∆∞u c√°c b√†i vi·∫øt ƒë√£ l√†m phong ph√∫ v√†o file m·ªõi v√† c√≥ th·ªÉ g·ª≠i t·ªõi backend.
    """
    # Check for auto_send argument
    auto_send = "--auto-send" in sys.argv
    
    # Get input file from command line or use latest scraped file
    input_file = None
    for arg in sys.argv[1:]:
        if not arg.startswith("--") and arg.endswith(".json"):
            input_file = arg
            break
            
    if not input_file:
        # Find latest scraped_articles file
        files = [f for f in os.listdir('.') if f.startswith('scraped_articles_') and f.endswith('.json')]
        if not files:
            print("‚ùå Kh√¥ng t√¨m th·∫•y file b√†i vi·∫øt. H√£y ch·∫°y google_news_serpapi.py tr∆∞·ªõc!")
            return
        input_file = max(files)  # Get most recent file
    
    print(f"üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ file: {input_file}")
    
    # Load articles from file
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            articles = json.load(f)
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc file {input_file}: {str(e)}")
        return
    
    if not articles:
        print("‚ùå Kh√¥ng c√≥ b√†i vi·∫øt n√†o trong file!")
        return
    
    print(f"üîç ƒê√£ t√¨m th·∫•y {len(articles)} b√†i vi·∫øt ƒë·ªÉ x·ª≠ l√Ω")
    
    # Setup WebDriver
    driver = setup_driver()
    
    try:
        # Process each article to get full content
        enriched_articles = []
        
        for i, article in enumerate(articles):
            print(f"[{i+1}/{len(articles)}] ƒêang x·ª≠ l√Ω: {article.get('title', 'Unknown title')}")
            enriched = enrich_article(driver, article)
            if enriched:
                enriched_articles.append(enriched)
            
            # Add delay between requests to avoid overloading servers
            if i < len(articles) - 1:
                time.sleep(2)
        
        # Save enriched articles to file
        output_file = f"enriched_articles_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(enriched_articles, f, ensure_ascii=False, indent=4)
        
        print(f"‚úÖ ƒê√£ l∆∞u {len(enriched_articles)} b√†i vi·∫øt ƒë√£ l√†m gi√†u v√†o {output_file}")
        
        # Automatically send to backend if there are articles
        if enriched_articles:
            send_to_backend(enriched_articles)
    
    finally:
        # Clean up
        driver.quit()
        print("üîö ƒê√£ ho√†n th√†nh qu√° tr√¨nh tr√≠ch xu·∫•t n·ªôi dung")

if __name__ == "__main__":
    main()
