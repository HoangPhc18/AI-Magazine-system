#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Module for searching Google News for a specific keyword using Selenium.
"""

import os
import sys
import re
import time
import random
import logging
import requests
import unicodedata
import json
from urllib.parse import urljoin, urlparse, parse_qs, quote_plus
from bs4 import BeautifulSoup
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from unidecode import unidecode

# Import c·∫•u h√¨nh t·ª´ module config
from config import get_config

# Import the content extractor from scrape_articles_selenium.py
from scrape_articles_selenium import extract_article_content

# T·∫£i c·∫•u h√¨nh
config = get_config()

# L·∫•y th√¥ng tin c·∫•u h√¨nh API URLs
BACKEND_URL = config["BACKEND_URL"]
BACKEND_PORT = config["BACKEND_PORT"]
BASE_API_URL = config["BASE_API_URL"]
CATEGORIES_API_URL = config["CATEGORIES_API_URL"]
SUBCATEGORIES_API_URL = config["SUBCATEGORIES_API_URL"]
BACKEND_API_URL = config["ARTICLES_API_URL"]
ARTICLES_IMPORT_API_URL = config["ARTICLES_IMPORT_API_URL"]
ARTICLES_CHECK_API_URL = config["ARTICLES_CHECK_API_URL"]

# üîπ S·ªë b√†i vi·∫øt t·ªëi ƒëa cho m·ªói danh m·ª•c
MAX_ARTICLES_PER_CATEGORY = config.get("MAX_ARTICLES_PER_CATEGORY", 3)
MAX_ARTICLES_PER_SUBCATEGORY = config.get("MAX_ARTICLES_PER_SUBCATEGORY", 2)
USE_SUBCATEGORIES = config.get("USE_SUBCATEGORIES", True)

# Th√¥ng tin c·∫•u h√¨nh service
PORT = config["PORT_SCRAPER"]
HOST = config["HOST"]
DEBUG = config["DEBUG"]

# Th∆∞ m·ª•c ƒë·∫ßu ra JSON
OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(f"keyword_search_{datetime.now().strftime('%Y%m%d')}.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

# List of User-Agents to rotate
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 Edg/113.0.1774.42",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/113.0",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 16_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Mobile/15E148 Safari/604.1"
]

def get_api_headers(content_type=None):
    """
    T·∫°o headers chu·∫©n cho API requests v·ªõi x·ª≠ l√Ω ƒë·∫∑c bi·ªát cho magazine.test tr√™n Linux
    
    Args:
        content_type (str, optional): Lo·∫°i n·ªôi dung, v√≠ d·ª• 'application/json'
        
    Returns:
        dict: Headers ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a
    """
    # T·∫£i l·∫°i c·∫•u h√¨nh ƒë·ªÉ c√≥ th√¥ng tin m·ªõi nh·∫•t
    current_config = get_config()
    
    # T·∫°o headers c∆° b·∫£n
    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'application/json',
    }
    
    # Th√™m Content-Type n·∫øu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if content_type:
        headers['Content-Type'] = content_type
    
    # Th√™m Host header n·∫øu c·∫•u h√¨nh y√™u c·∫ßu
    if current_config.get("USE_HOST_HEADER", False):
        headers['Host'] = 'magazine.test'
        logger.debug("S·ª≠ d·ª•ng Host header: magazine.test")
    
    return headers

def fetch_categories_from_backend():
    """
    L·∫•y danh s√°ch c√°c danh m·ª•c t·ª´ backend.
    
    Returns:
        list: Danh s√°ch c√°c danh m·ª•c ho·∫∑c None n·∫øu c√≥ l·ªói
    """
    try:
        # T·∫£i l·∫°i c·∫•u h√¨nh ƒë·ªÉ c√≥ th√¥ng tin m·ªõi nh·∫•t
        config = get_config()
        categories_url = config["CATEGORIES_API_URL"]
        
        # G·ªçi API l·∫•y danh s√°ch danh m·ª•c
        logger.info(f"Fetching categories from backend: {categories_url}")
        
        # S·ª≠ d·ª•ng h√†m get_api_headers
        headers = get_api_headers()
        
        response = requests.get(categories_url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            categories = response.json()
            
            # Ki·ªÉm tra c·∫•u tr√∫c d·ªØ li·ªáu tr·∫£ v·ªÅ
            if not isinstance(categories, list):
                # N·∫øu kh√¥ng ph·∫£i list, ki·ªÉm tra xem c√≥ ph·∫£i l√† object v·ªõi data key kh√¥ng
                if isinstance(categories, dict) and 'data' in categories:
                    categories = categories['data']
                else:
                    logger.error(f"Unexpected categories data structure: {categories}")
                    return None
            
            # Ki·ªÉm tra v√† log th√¥ng tin v·ªÅ c√°c danh m·ª•c
            valid_categories = []
            for category in categories:
                if not isinstance(category, dict):
                    logger.warning(f"Invalid category data type: {type(category)}")
                    continue
                    
                if 'id' not in category or 'name' not in category:
                    logger.warning(f"Category missing required fields: {category}")
                    continue
                    
                # Ghi log th√¥ng tin danh m·ª•c
                logger.info(f"Found category: ID: {category['id']}, Name: {category['name']}, " +
                           f"Slug: {category.get('slug', 'N/A')}")
                           
                valid_categories.append(category)
            
            logger.info(f"Successfully fetched {len(valid_categories)} categories from backend")
            return valid_categories
        else:
            logger.error(f"Failed to fetch categories from backend. Status code: {response.status_code}")
            return None
            
    except Exception as e:
        logger.error(f"Error fetching categories from backend: {str(e)}")
        return None

def fetch_subcategories_by_category(category_id):
    """
    L·∫•y danh s√°ch c√°c danh m·ª•c con cho m·ªôt danh m·ª•c c·ª• th·ªÉ t·ª´ backend.
    
    Args:
        category_id (int): ID c·ªßa danh m·ª•c cha
        
    Returns:
        list: Danh s√°ch c√°c danh m·ª•c con
    """
    try:
        # T·∫°o URL API ƒë·ªÉ l·∫•y danh m·ª•c con
        api_url = f"{CATEGORIES_API_URL}/{category_id}/subcategories"
        
        # Log th√¥ng tin request
        logger.info(f"Fetching subcategories for category ID {category_id} from: {api_url}")
        
        # G·ª≠i request ƒë·∫øn backend API
        headers = get_api_headers()
        
        response = requests.get(api_url, headers=headers, timeout=30)
        
        # Ki·ªÉm tra response
        if response.status_code == 200:
            subcategories = response.json()
            
            if isinstance(subcategories, list) and len(subcategories) > 0:
                logger.info(f"Found {len(subcategories)} subcategories for category ID {category_id}")
                for subcategory in subcategories[:3]:  # Hi·ªÉn th·ªã 3 danh m·ª•c con ƒë·∫ßu ti√™n
                    logger.info(f"Subcategory: ID: {subcategory.get('id')}, Name: {subcategory.get('name')}")
                return subcategories
            else:
                logger.info(f"No subcategories found for category ID {category_id}")
                return []
        else:
            logger.error(f"Failed to fetch subcategories. Status code: {response.status_code}")
            return []
            
    except Exception as e:
        logger.error(f"Error fetching subcategories: {str(e)}")
        return []

def get_category_by_id(category_id):
    """
    L·∫•y th√¥ng tin danh m·ª•c t·ª´ backend d·ª±a tr√™n ID
    
    Args:
        category_id: ID c·ªßa danh m·ª•c c·∫ßn l·∫•y
        
    Returns:
        dict: Th√¥ng tin danh m·ª•c ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    try:
        logger.info(f"Fetching category with ID {category_id} from backend")
        headers = get_api_headers()
        response = requests.get(f"{CATEGORIES_API_URL}/{category_id}", headers=headers, timeout=15)
        
        if response.status_code == 200:
            category = response.json()
            logger.info(f"Successfully fetched category: {category['name']}")
            return category
        else:
            logger.error(f"Failed to fetch category with ID {category_id}. Status code: {response.status_code}")
            logger.error(f"Response: {response.text}")
            return None
            
    except Exception as e:
        logger.error(f"Error while fetching category with ID {category_id}: {str(e)}")
        return None

def get_subcategory_by_id(subcategory_id):
    """
    L·∫•y th√¥ng tin danh m·ª•c con t·ª´ backend d·ª±a tr√™n ID
    
    Args:
        subcategory_id: ID c·ªßa danh m·ª•c con c·∫ßn l·∫•y
        
    Returns:
        dict: Th√¥ng tin danh m·ª•c con ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    try:
        logger.info(f"Fetching subcategory with ID {subcategory_id} from backend")
        headers = get_api_headers()
        response = requests.get(f"{SUBCATEGORIES_API_URL}/{subcategory_id}", headers=headers, timeout=15)
        
        if response.status_code == 200:
            subcategory = response.json()
            logger.info(f"Successfully fetched subcategory: {subcategory['name']}")
            return subcategory
        else:
            logger.error(f"Failed to fetch subcategory with ID {subcategory_id}. Status code: {response.status_code}")
            logger.error(f"Response: {response.text}")
            return None
            
    except Exception as e:
        logger.error(f"Error while fetching subcategory with ID {subcategory_id}: {str(e)}")
        return None

def import_article_to_backend(category_id, article_url, title, content, subcategory_id=None):
    """
    G·ª≠i b√†i vi·∫øt ƒë√£ t√¨m ƒë∆∞·ª£c v√†o backend.
    
    Args:
        category_id (int): ID c·ªßa danh m·ª•c
        article_url (str): URL b√†i vi·∫øt
        title (str): Ti√™u ƒë·ªÅ b√†i vi·∫øt
        content (str): N·ªôi dung b√†i vi·∫øt
        subcategory_id (int, optional): ID c·ªßa danh m·ª•c con (n·∫øu c√≥)
        
    Returns:
        bool: True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    try:
        headers = get_api_headers('application/json')
        
        # Ph√¢n t√≠ch URL ƒë·ªÉ l·∫•y domain
        source_name = ""
        try:
            parsed_url = urlparse(article_url)
            source_name = parsed_url.netloc
        except:
            source_name = "unknown-source"
        
        # T·∫°o slug t·ª´ ti√™u ƒë·ªÅ
        slug = generate_slug(title, add_uuid=True)
        
        # T·∫°o summary t·ª´ content
        summary = ""
        if content:
            sentences = re.split(r'[.!?]+', content)
            if len(sentences) >= 2:
                summary = '. '.join(s.strip() for s in sentences[:2] if s.strip()) + '.'
            else:
                summary = sentences[0].strip() if sentences else ""
        
        # ƒê·∫£m b·∫£o category_id l√† s·ªë nguy√™n
        try:
            category_id = int(category_id)
        except (ValueError, TypeError):
            logger.error(f"Invalid category_id: {category_id}")
            return False
        
        # T·∫°o article object
        article = {
            'category_id': category_id,
            'url': article_url,
            'source_url': article_url,
            'source_name': source_name,
            'source_icon': f"https://www.google.com/s2/favicons?domain={source_name}",
            'title': title,
            'slug': slug,
            'summary': summary,
            'content': content,
            'published_at': datetime.now().isoformat(),
            'is_published': 1,
            'is_imported': 1,
            'category': category_id  # Th√™m field category ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch
        }
        
        # Th√™m subcategory_id v√†o request n·∫øu c√≥
        if subcategory_id:
            # ƒê·∫£m b·∫£o subcategory_id l√† s·ªë nguy√™n
            try:
                subcategory_id = int(subcategory_id)
                article['subcategory_id'] = subcategory_id
            except (ValueError, TypeError):
                logger.error(f"Invalid subcategory_id: {subcategory_id}")
                # Kh√¥ng return False ·ªü ƒë√¢y, ch·ªâ kh√¥ng th√™m subcategory_id v√†o request
            
            # Th√™m th√¥ng tin danh m·ª•c con
            subcategory = get_subcategory_by_id(subcategory_id)
            if subcategory and 'name' in subcategory:
                article['subcategory_name'] = subcategory['name']
        
        # ƒê√≥ng g√≥i article trong m·∫£ng "articles" nh∆∞ API y√™u c·∫ßu
        data = {
            'articles': [article]
        }
        
        # Log chi ti·∫øt request ƒë·ªÉ debug
        logger.info(f"Article request: title='{title}', category_id={category_id}, subcategory_id={subcategory_id if subcategory_id else 'None'}")
        
        # S·ª≠ d·ª•ng endpoint import thay v√¨ API articles tr·ª±c ti·∫øp
        import_endpoint = f"{ARTICLES_IMPORT_API_URL}"
        logger.info(f"Importing article to backend: {import_endpoint}")
        
        # G·ª≠i request v·ªõi timeout d√†i h∆°n ƒë·ªÉ x·ª≠ l√Ω b√†i vi·∫øt l·ªõn
        response = requests.post(import_endpoint, headers=headers, json=data, timeout=60)
        
        if response.status_code == 200 or response.status_code == 201:
            # Ph√¢n t√≠ch k·∫øt qu·∫£ tr·∫£ v·ªÅ
            result = response.json()
            if result.get('status') == 'success':
                logger.info(f"Successfully imported article for category ID {category_id}{' and subcategory ID ' + str(subcategory_id) if subcategory_id else ''}")
                return True
            elif result.get('status') == 'warning':
                # Tr∆∞·ªùng h·ª£p c√≥ c·∫£nh b√°o nh∆∞ng kh√¥ng l·ªói
                logger.warning(f"Warning when importing article: {result.get('message')}")
                if result.get('skipped', 0) > 0:
                    # B·ªã b·ªè qua nh∆∞ng kh√¥ng ph·∫£i l·ªói
                    logger.warning(f"Article was skipped. Reason: {result.get('errors', ['Unknown reason'])[0]}")
                    # V·∫´n tr·∫£ v·ªÅ True v√¨ ƒë√¢y kh√¥ng ph·∫£i l·ªói k·ªπ thu·∫≠t
                    return True
                return True
            else:
                logger.error(f"Failed to import article. Status: {result.get('status')}, Message: {result.get('message')}")
                return False
        else:
            logger.error(f"Failed to import article. Status code: {response.status_code}, Response: {response.text}")
            return False
            
    except Exception as e:
        logger.error(f"Error importing article: {str(e)}")
        return False

def remove_vietnamese_accents(text):
    """
    Lo·∫°i b·ªè d·∫•u trong ti·∫øng Vi·ªát.
    
    Args:
        text (str): VƒÉn b·∫£n ti·∫øng Vi·ªát c√≥ d·∫•u
        
    Returns:
        str: VƒÉn b·∫£n kh√¥ng d·∫•u
    """
    # B·∫£ng chuy·ªÉn ƒë·ªïi ch·ªØ c√≥ d·∫•u sang kh√¥ng d·∫•u
    vietnamese_map = {
        '√†': 'a', '√°': 'a', '·∫£': 'a', '√£': 'a', '·∫°': 'a',
        'ƒÉ': 'a', '·∫±': 'a', '·∫Ø': 'a', '·∫≥': 'a', '·∫µ': 'a', '·∫∑': 'a',
        '√¢': 'a', '·∫ß': 'a', '·∫•': 'a', '·∫©': 'a', '·∫´': 'a', '·∫≠': 'a',
        'ƒë': 'd',
        '√®': 'e', '√©': 'e', '·∫ª': 'e', '·∫Ω': 'e', '·∫π': 'e',
        '√™': 'e', '·ªÅ': 'e', '·∫ø': 'e', '·ªÉ': 'e', '·ªÖ': 'e', '·ªá': 'e',
        '√¨': 'i', '√≠': 'i', '·ªâ': 'i', 'ƒ©': 'i', '·ªã': 'i',
        '√≤': 'o', '√≥': 'o', '·ªè': 'o', '√µ': 'o', '·ªç': 'o',
        '√¥': 'o', '·ªì': 'o', '·ªë': 'o', '·ªï': 'o', '·ªó': 'o', '·ªô': 'o',
        '∆°': 'o', '·ªù': 'o', '·ªõ': 'o', '·ªü': 'o', '·ª°': 'o', '·ª£': 'o',
        '√π': 'u', '√∫': 'u', '·ªß': 'u', '≈©': 'u', '·ª•': 'u',
        '∆∞': 'u', '·ª´': 'u', '·ª©': 'u', '·ª≠': 'u', '·ªØ': 'u', '·ª±': 'u',
        '·ª≥': 'y', '√Ω': 'y', '·ª∑': 'y', '·ªπ': 'y', '·ªµ': 'y',
    }
    
    # Ph∆∞∆°ng ph√°p 1: S·ª≠ d·ª•ng b·∫£ng chuy·ªÉn ƒë·ªïi
    result1 = ''.join(vietnamese_map.get(c.lower(), c) for c in text)
    
    # Ph∆∞∆°ng ph√°p 2: S·ª≠ d·ª•ng unicodedata
    result2 = unicodedata.normalize('NFKD', text)
    result2 = ''.join([c for c in result2 if not unicodedata.combining(c)])
    result2 = result2.replace('ƒë', 'd').replace('ƒê', 'D')
    
    # S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p 1 v√¨ n√≥ x·ª≠ l√Ω t·ªët h∆°n v·ªõi ch·ªØ 'ƒë'
    return result1

def setup_selenium_driver():
    """
    Kh·ªüi t·∫°o tr√¨nh ƒëi·ªÅu khi·ªÉn Selenium v·ªõi c√°c t√πy ch·ªçn ph√π h·ª£p.
    
    Returns:
        webdriver: Tr√¨nh ƒëi·ªÅu khi·ªÉn Selenium ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh
    """
    try:
        # Thi·∫øt l·∫≠p c√°c t√πy ch·ªçn cho Chrome
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # Ch·∫°y trong ch·∫ø ƒë·ªô headless (kh√¥ng hi·ªÉn th·ªã giao di·ªán)
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        
        # Th√™m User-Agent ng·∫´u nhi√™n
        user_agent = random.choice(USER_AGENTS)
        chrome_options.add_argument(f"--user-agent={user_agent}")
        
        # Tr√°nh ph√°t hi·ªán t·ª± ƒë·ªông
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # Thi·∫øt l·∫≠p ng√¥n ng·ªØ
        chrome_options.add_argument("--lang=vi-VN")
        
        # Kh·ªüi t·∫°o tr√¨nh ƒëi·ªÅu khi·ªÉn
        logger.info("Initializing Chrome WebDriver...")
        driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()),
            options=chrome_options
        )
        
        # Thi·∫øt l·∫≠p c√°c thu·ªôc t√≠nh ƒë·ªÉ tr√°nh ph√°t hi·ªán
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        return driver
    
    except Exception as e:
        logger.error(f"Error setting up Selenium driver: {str(e)}")
        raise

def search_with_selenium(keyword):
    """
    T√¨m ki·∫øm tr√™n Google News b·∫±ng Selenium v√† tr·∫£ v·ªÅ URL b√†i vi·∫øt ƒë·∫ßu ti√™n.
    
    Args:
        keyword (str): T·ª´ kh√≥a t√¨m ki·∫øm
        
    Returns:
        str: URL c·ªßa b√†i vi·∫øt ƒë·∫ßu ti√™n, ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    driver = None
    try:
        # Kh·ªüi t·∫°o tr√¨nh ƒëi·ªÅu khi·ªÉn
        driver = setup_selenium_driver()
        
        # Truy c·∫≠p Google News
        logger.info(f"Accessing Google News homepage with Selenium")
        driver.get("https://news.google.com/?hl=vi&gl=VN&ceid=VN:vi")
        
        # Ch·ªù trang t·∫£i xong
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # T√¨m ki·∫øm √¥ t√¨m ki·∫øm v√† nh·∫≠p t·ª´ kh√≥a
        logger.info(f"Looking for search box on Google News")
        search_box = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='text']"))
        )
        
        # Nh·∫≠p t·ª´ kh√≥a v·ªõi "when:1d" ƒë·ªÉ gi·ªõi h·∫°n trong 1 ng√†y
        search_query = f"{keyword} when:1d"
        logger.info(f"Entering search query: '{search_query}'")
        
        search_box.clear()
        search_box.send_keys(search_query)
        search_box.send_keys(Keys.RETURN)
        
        # Ch·ªù k·∫øt qu·∫£ t√¨m ki·∫øm xu·∫•t hi·ªán
        logger.info("Waiting for search results...")
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "article"))
        )
        
        # Th√™m th·ªùi gian ch·ªù ƒë·ªÉ trang t·∫£i ho√†n to√†n
        time.sleep(3)
        
        # L·∫•y HTML c·ªßa trang k·∫øt qu·∫£
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # T√¨m c√°c b√†i vi·∫øt trong k·∫øt qu·∫£
        articles = soup.select('article')
        
        if articles:
            logger.info(f"Found {len(articles)} articles in Google News")
            
            # T√¨m b√†i vi·∫øt ƒë·∫ßu ti√™n c√≥ li√™n k·∫øt
            for article in articles:
                # T√¨m ti√™u ƒë·ªÅ b√†i vi·∫øt
                title_element = article.select_one('h3 a, h4 a')
                
                if title_element:
                    logger.info(f"Found article with title: {title_element.text.strip()}")
                    
                    # L·∫•y li√™n k·∫øt t·ª´ b√†i vi·∫øt
                    link = article.select_one('a[href^="./articles/"]')
                    
                    if link:
                        relative_url = link['href']
                        
                        # Truy c·∫≠p v√†o li√™n k·∫øt ƒë·ªÉ l·∫•y URL th·ª±c
                        if relative_url.startswith('./'):
                            absolute_news_url = f"https://news.google.com{relative_url[1:]}"
                            logger.info(f"Navigating to article link: {absolute_news_url}")
                            
                            # M·ªü li√™n k·∫øt trong c·ª≠a s·ªï m·ªõi
                            driver.execute_script(f"window.open('{absolute_news_url}', '_blank');")
                            
                            # Chuy·ªÉn sang c·ª≠a s·ªï m·ªõi
                            driver.switch_to.window(driver.window_handles[1])
                            
                            # Ch·ªù chuy·ªÉn h∆∞·ªõng
                            time.sleep(5)
                            
                            # L·∫•y URL cu·ªëi c√πng sau khi chuy·ªÉn h∆∞·ªõng
                            final_url = driver.current_url
                            
                            # Ki·ªÉm tra URL c√≥ ph·∫£i l√† t·ª´ Google News kh√¥ng
                            if 'news.google.com' not in final_url:
                                logger.info(f"Successfully obtained article URL: {final_url}")
                                return final_url
            
            logger.warning("Could not extract actual URL from any article")
            
        else:
            logger.warning(f"No articles found for keyword: {keyword}")
        
        return None
        
    except TimeoutException:
        logger.error("Timeout waiting for page elements")
        return None
    
    except WebDriverException as e:
        logger.error(f"Selenium WebDriver error: {str(e)}")
        return None
    
    except Exception as e:
        logger.error(f"Error in Selenium search: {str(e)}")
        return None
    
    finally:
        # ƒê√≥ng tr√¨nh duy·ªát
        if driver:
            try:
                driver.quit()
                logger.info("WebDriver closed successfully")
            except Exception as e:
                logger.error(f"Error closing WebDriver: {str(e)}")

def search_with_requests(keyword):
    """
    T√¨m ki·∫øm tr√™n Google News b·∫±ng requests HTTP th√¥ng th∆∞·ªùng.
    ƒê∆∞·ª£c s·ª≠ d·ª•ng nh∆∞ m·ªôt ph∆∞∆°ng √°n d·ª± ph√≤ng n·∫øu Selenium th·∫•t b·∫°i.
    
    Args:
        keyword (str): T·ª´ kh√≥a t√¨m ki·∫øm
        
    Returns:
        str: URL c·ªßa b√†i vi·∫øt ƒë·∫ßu ti√™n, ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    try:
        # Chu·∫©n b·ªã headers v·ªõi User-Agent ng·∫´u nhi√™n
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        
        # T√¨m ki·∫øm tr√™n Google News
        search_query = f"{keyword} when:1d"
        search_url = f"https://news.google.com/search?q={quote_plus(search_query)}&hl=vi&gl=VN&ceid=VN:vi"
        
        logger.info(f"Searching with HTTP request: {search_url}")
        response = requests.get(search_url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m c√°c b√†i vi·∫øt
            articles = soup.select('article')
            
            if articles:
                logger.info(f"Found {len(articles)} articles via HTTP request")
                
                # G·ª° l·ªói: Hi·ªÉn th·ªã ti√™u ƒë·ªÅ c·ªßa c√°c b√†i vi·∫øt
                for idx, article in enumerate(articles[:5]):
                    title_elem = article.select_one('h3, h4')
                    title = title_elem.text.strip() if title_elem else "No title"
                    logger.info(f"Article {idx+1}: {title}")
                
                # Th·ª≠ tr√≠ch xu·∫•t URL tr·ª±c ti·∫øp t·ª´ Google Search
                logger.info("Trying direct Google Search as it's more reliable")
                google_search_url = f"https://www.google.com/search?q={quote_plus(keyword)}&tbm=nws"
                logger.info(f"Searching with Google Search: {google_search_url}")
                
                try:
                    search_headers = {
                        'User-Agent': random.choice(USER_AGENTS),
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
                    }
                    
                    search_response = requests.get(google_search_url, headers=search_headers, timeout=15)
                    
                    if search_response.status_code == 200:
                        search_soup = BeautifulSoup(search_response.text, 'html.parser')
                        
                        # Log HTML ƒë·ªÉ g·ª° l·ªói
                        with open('google_search_response.html', 'w', encoding='utf-8') as f:
                            f.write(search_response.text)
                        logger.info("Saved Google Search response to google_search_response.html")
                        
                        # T√¨m c√°c k·∫øt qu·∫£ t·ª´ Google Search - th·ª≠ nhi·ªÅu selector kh√°c nhau
                        for selector in ['div.g a', 'a[href^="https://"]', '.WlydOe', '.DhN8Cf a']:
                            search_results = search_soup.select(selector)
                            logger.info(f"Found {len(search_results)} results with selector '{selector}'")
                            
                            for result in search_results:
                                href = result.get('href')
                                if href and href.startswith('http') and 'google.com' not in href:
                                    logger.info(f"Found article URL from Google Search: {href}")
                                    return href
                    
                    logger.warning("No results found from Google Search")
                except Exception as e:
                    logger.error(f"Error with Google Search: {str(e)}")
                
                # Th·ª≠ truy c·∫≠p tr·ª±c ti·∫øp m·ªôt trang tin t·ª©c Vi·ªát Nam c√≥ b√†i v·ªÅ t·ª´ kh√≥a n√†y
                news_sites = [
                    f"https://vnexpress.net/tim-kiem?q={quote_plus(keyword)}",
                    f"https://tuoitre.vn/tim-kiem.htm?keywords={quote_plus(keyword)}",
                    f"https://thanhnien.vn/tim-kiem/?q={quote_plus(keyword)}",
                    f"https://dantri.com.vn/tim-kiem?q={quote_plus(keyword)}"
                ]
                
                for site_url in news_sites:
                    try:
                        logger.info(f"Trying direct news site search: {site_url}")
                        site_response = requests.get(site_url, headers=headers, timeout=15)
                        
                        if site_response.status_code == 200:
                            site_soup = BeautifulSoup(site_response.text, 'html.parser')
                            
                            # T√¨m ki·∫øm c√°c li√™n k·∫øt b√†i vi·∫øt - th·ª≠ nhi·ªÅu selector kh√°c nhau cho t·ª´ng trang
                            for article_selector in ['article a', '.title-news a', '.story', '.article-title a', '.title a']:
                                article_links = site_soup.select(article_selector)
                                
                                for link in article_links[:3]:  # Ch·ªâ xem 3 k·∫øt qu·∫£ ƒë·∫ßu ti√™n
                                    href = link.get('href')
                                    if href:
                                        # Chuy·ªÉn ƒë·ªïi URL t∆∞∆°ng ƒë·ªëi th√†nh tuy·ªát ƒë·ªëi n·∫øu c·∫ßn
                                        if not href.startswith('http'):
                                            base_url = urlparse(site_url)
                                            href = f"{base_url.scheme}://{base_url.netloc}{href if href.startswith('/') else '/' + href}"
                                        
                                        logger.info(f"Found article from direct news site: {href}")
                                        return href
                    except Exception as e:
                        logger.error(f"Error with direct news site {site_url}: {str(e)}")
            
            logger.warning("No suitable articles found via HTTP request")
        else:
            logger.error(f"HTTP request failed with status code: {response.status_code}")
        
        # N·∫øu t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p ƒë·ªÅu th·∫•t b·∫°i, th·ª≠ t√¨m ki·∫øm v·ªõi Bing
        try:
            bing_url = f"https://www.bing.com/news/search?q={quote_plus(keyword)}&qft=sortbydate%3d"
            logger.info(f"Trying Bing News as last resort: {bing_url}")
            
            bing_headers = {
                'User-Agent': random.choice(USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
            }
            
            bing_response = requests.get(bing_url, headers=bing_headers, timeout=15)
            
            if bing_response.status_code == 200:
                bing_soup = BeautifulSoup(bing_response.text, 'html.parser')
                
                # T√¨m k·∫øt qu·∫£ tin t·ª©c t·ª´ Bing
                bing_results = bing_soup.select('.news-card a')
                
                for result in bing_results:
                    href = result.get('href')
                    if href and href.startswith('http') and 'bing.com' not in href and 'msn.com' not in href:
                        logger.info(f"Found article from Bing News: {href}")
                        return href
        except Exception as e:
            logger.error(f"Error with Bing search: {str(e)}")
        
        return None
    
    except Exception as e:
        logger.error(f"Error in HTTP request method: {str(e)}")
        return None

def direct_news_search(keyword):
    """
    T√¨m ki·∫øm tr·ª±c ti·∫øp tr√™n c√°c trang tin t·ª©c Vi·ªát Nam ph·ªï bi·∫øn.
    Ph∆∞∆°ng ph√°p n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng khi Google News ho·∫∑c Google Search kh√¥ng ho·∫°t ƒë·ªông.
    
    Args:
        keyword (str): T·ª´ kh√≥a t√¨m ki·∫øm
        
    Returns:
        str: URL b√†i vi·∫øt n·∫øu t√¨m th·∫•y, None n·∫øu kh√¥ng
    """
    logger.info(f"Performing direct news search for: {keyword}")
    
    # Danh s√°ch c√°c trang tin t·ª©c Vi·ªát Nam ph·ªï bi·∫øn
    news_sites = [
        {
            'name': 'VnExpress',
            'search_url': f"https://vnexpress.net/tim-kiem?q={quote_plus(keyword)}",
            'selectors': ['.title-news a', '.item-news h3 a', '.title_news a']
        },
        {
            'name': 'Tu·ªïi Tr·∫ª',
            'search_url': f"https://tuoitre.vn/tim-kiem.htm?keywords={quote_plus(keyword)}",
            'selectors': ['.news-item', '.title-news a', 'h3.title-news a']
        },
        {
            'name': 'Thanh Ni√™n',
            'search_url': f"https://thanhnien.vn/tim-kiem/?q={quote_plus(keyword)}",
            'selectors': ['.story', '.story__title a', '.highlights__item-title a']
        },
        {
            'name': 'D√¢n Tr√≠',
            'search_url': f"https://dantri.com.vn/tim-kiem?q={quote_plus(keyword)}",
            'selectors': ['.article-title a', '.article-item a', '.news-item__title a']
        },
        {
            'name': 'Zing News',
            'search_url': f"https://zingnews.vn/tim-kiem.html?q={quote_plus(keyword)}",
            'selectors': ['.article-title a', '.article-item a', '.news-item__title a']
        },
        {
            'name': 'B√≥ng ƒê√° 24h',
            'search_url': f"https://bongda24h.vn/tim-kiem/{quote_plus(keyword)}/1.html",
            'selectors': ['.news-title a', '.title a', 'h3 a']
        },
        {
            'name': 'Ng∆∞·ªùi ƒê∆∞a Tin',
            'search_url': f"https://www.nguoiduatin.vn/tim-kiem?q={quote_plus(keyword)}",
            'selectors': ['article h3 a', '.article-title a']
        }
    ]
    
    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
    }
    
    for site in news_sites:
        try:
            logger.info(f"Searching on {site['name']}: {site['search_url']}")
            response = requests.get(site['search_url'], headers=headers, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # L∆∞u HTML ƒë·ªÉ g·ª° l·ªói n·∫øu c·∫ßn
                with open(f"{site['name'].lower()}_search.html", 'w', encoding='utf-8') as f:
                    f.write(response.text)
                
                # Th·ª≠ c√°c selector
                for selector in site['selectors']:
                    links = soup.select(selector)
                    logger.info(f"Found {len(links)} links with selector '{selector}' on {site['name']}")
                    
                    if links:
                        for link in links[:3]:  # Ch·ªâ l·∫•y 3 k·∫øt qu·∫£ ƒë·∫ßu ti√™n
                            href = link.get('href')
                            if href and ('http' in href or href.startswith('/')):
                                # Chuy·ªÉn ƒë·ªïi URL t∆∞∆°ng ƒë·ªëi th√†nh tuy·ªát ƒë·ªëi n·∫øu c·∫ßn
                                if not href.startswith('http'):
                                    base_url = urlparse(site['search_url'])
                                    href = f"{base_url.scheme}://{base_url.netloc}{href if href.startswith('/') else '/' + href}"
                                
                                logger.info(f"Found article on {site['name']}: {href}")
                                return href
        except Exception as e:
            logger.error(f"Error searching on {site['name']}: {str(e)}")
    
    logger.warning("No articles found in direct news search")
    return None

def search_google_news(keyword):
    """
    T√¨m ki·∫øm t·ª´ kh√≥a tr√™n Google News v√† tr·∫£ v·ªÅ URL b√†i vi·∫øt ƒë·∫ßu ti√™n.
    Th·ª≠ c·∫£ t·ª´ kh√≥a g·ªëc v√† t·ª´ kh√≥a kh√¥ng d·∫•u.
    
    Args:
        keyword (str): T·ª´ kh√≥a t√¨m ki·∫øm
        
    Returns:
        str: URL c·ªßa b√†i vi·∫øt ƒë·∫ßu ti√™n, ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    # Th·ª≠ c·∫£ t·ª´ kh√≥a g·ªëc v√† t·ª´ kh√≥a kh√¥ng d·∫•u
    original_keyword = keyword
    keyword_no_accent = remove_vietnamese_accents(keyword)
    
    # Log th√¥ng tin t·ª´ kh√≥a
    logger.info(f"Searching for keyword: '{original_keyword}' and non-accented version: '{keyword_no_accent}'")
    
    # Danh s√°ch t·ª´ kh√≥a ƒë·ªÉ th·ª≠
    keywords_to_try = [original_keyword]
    
    # N·∫øu t·ª´ kh√≥a kh√¥ng d·∫•u kh√°c v·ªõi t·ª´ kh√≥a g·ªëc, th√™m v√†o danh s√°ch
    if keyword_no_accent != original_keyword:
        keywords_to_try.append(keyword_no_accent)
    
    # L∆∞u l·ªói cho t·ª´ng t·ª´ kh√≥a
    errors = {}
    
    # Th·ª≠ t·ª´ng t·ª´ kh√≥a
    for current_keyword in keywords_to_try:
        logger.info(f"Trying to search with keyword: '{current_keyword}'")
        
        try:
            # V√¥ hi·ªáu h√≥a Selenium v√¨ l·ªói WebDriver tr√™n Windows
            # logger.info(f"Attempting to search with Selenium for: '{current_keyword}'")
            # url = search_with_selenium(current_keyword)
            # 
            # if url:
            #     logger.info(f"Successfully found URL with Selenium: {url}")
            #     return url
            
            # Ch·ªâ s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p requests HTTP
            logger.info(f"Using HTTP request method for: '{current_keyword}'")
            url = search_with_requests(current_keyword)
            
            if url:
                logger.info(f"Successfully found URL with HTTP request: {url}")
                return url
            
            # N·∫øu kh√¥ng t√¨m th·∫•y, ghi l·∫°i l·ªói
            error_msg = f"No articles found for keyword '{current_keyword}' using HTTP request method"
            logger.error(error_msg)
            errors[current_keyword] = error_msg
            
        except Exception as e:
            error_msg = f"Exception searching for '{current_keyword}': {str(e)}"
            logger.error(error_msg)
            errors[current_keyword] = error_msg
    
    # N·∫øu t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p th√¥ng th∆∞·ªùng th·∫•t b·∫°i, th·ª≠ t√¨m ki·∫øm tr·ª±c ti·∫øp
    logger.info("All standard methods failed, trying direct news search")
    url = direct_news_search(original_keyword)
    if url:
        logger.info(f"Found article through direct news search: {url}")
        return url
    
    # T·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p ƒë·ªÅu th·∫•t b·∫°i
    logger.error(f"All keyword searches failed. Errors: {errors}")
    return None

def search_with_category(category_id, subcategory_id=None):
    """
    T√¨m ki·∫øm b√†i vi·∫øt d·ª±a tr√™n ID danh m·ª•c ho·∫∑c danh m·ª•c con t·ª´ backend.
    
    Args:
        category_id (int): ID c·ªßa danh m·ª•c
        subcategory_id (int, optional): ID c·ªßa danh m·ª•c con (n·∫øu c√≥)
        
    Returns:
        dict: Th√¥ng tin b√†i vi·∫øt ƒë√£ t√¨m th·∫•y v√† tr√≠ch xu·∫•t, ho·∫∑c None n·∫øu th·∫•t b·∫°i
    """
    try:
        # ∆Øu ti√™n t√¨m ki·∫øm v·ªõi subcategory n·∫øu c√≥
        if subcategory_id:
            subcategory = get_subcategory_by_id(subcategory_id)
            
            if not subcategory:
                logger.error(f"Could not find subcategory with ID: {subcategory_id}")
                return None
            
            # S·ª≠ d·ª•ng t√™n danh m·ª•c con l√†m t·ª´ kh√≥a t√¨m ki·∫øm
            if 'name' not in subcategory:
                logger.error(f"Subcategory data does not contain 'name' field: {subcategory}")
                return None
                
            keyword = subcategory['name']
            logger.info(f"Using subcategory name '{keyword}' as search keyword")
            
            # L·∫•y th√¥ng tin danh m·ª•c ch√≠nh
            category = get_category_by_id(category_id)
            
            if not category:
                logger.error(f"Could not find category with ID: {category_id}")
                return None
                
            category_name = category['name']
        else:
            # L·∫•y th√¥ng tin danh m·ª•c t·ª´ backend
            category = get_category_by_id(category_id)
            
            if not category:
                logger.error(f"Could not find category with ID: {category_id}")
                return None
            
            # S·ª≠ d·ª•ng t√™n danh m·ª•c (c·ªôt name) l√†m t·ª´ kh√≥a t√¨m ki·∫øm
            if 'name' not in category:
                logger.error(f"Category data does not contain 'name' field: {category}")
                return None
                
            keyword = category['name']
            logger.info(f"Using category name '{keyword}' as search keyword")
            category_name = keyword
        
        # T√¨m ki·∫øm b√†i vi·∫øt v·ªõi t·ª´ kh√≥a
        article_url = search_google_news(keyword)
        
        if not article_url:
            logger.error(f"No article URL found for keyword: {keyword}")
            return None
        
        # Ki·ªÉm tra URL
        if not article_url.startswith('http'):
            logger.error(f"Invalid URL format: {article_url}")
            return None
            
        # Ki·ªÉm tra b√†i vi·∫øt ƒë√£ t·ªìn t·∫°i trong database ch∆∞a
        if check_article_exists(article_url):
            logger.warning(f"B√†i vi·∫øt ƒë√£ t·ªìn t·∫°i trong database, t√¨m b√†i vi·∫øt kh√°c: {article_url}")
            # Th·ª≠ t√¨m URL kh√°c n·∫øu URL n√†y ƒë√£ t·ªìn t·∫°i
            for attempt in range(3):
                logger.info(f"Attempting to find a different article (attempt {attempt+1}/3)")
                new_url = search_google_news(keyword + f" -{article_url.split('/')[2]}")
                if new_url and new_url != article_url and not check_article_exists(new_url):
                    article_url = new_url
                    logger.info(f"Found alternative URL: {article_url}")
                    break
                time.sleep(1)
        
        logger.info(f"Found article URL: {article_url}, extracting content...")
        
        # Tr√≠ch xu·∫•t n·ªôi dung b√†i vi·∫øt
        article_data = extract_article_content(article_url)
        
        if not article_data:
            logger.error(f"Failed to extract any content from URL: {article_url}")
            return None
            
        # Ki·ªÉm tra d·ªØ li·ªáu c√≥ ƒë·ªß c√°c tr∆∞·ªùng c·∫ßn thi·∫øt kh√¥ng
        if not article_data.get("title"):
            logger.error(f"Extracted article has no title: {article_url}")
            return None
            
        if not article_data.get("content"):
            logger.error(f"Extracted article has no content: {article_url}")
            return None
        
        # Ki·ªÉm tra ƒë·ªô d√†i n·ªôi dung
        content_length = len(article_data.get("content", ""))
        if content_length < 100:
            logger.error(f"Article content too short ({content_length} chars): {article_url}")
            return None
            
        logger.info(f"Successfully extracted content from URL: {article_url}")
        logger.info(f"Title: {article_data.get('title')}")
        logger.info(f"Content length: {content_length} chars")
        
        # L∆∞u d·ªØ li·ªáu b√†i vi·∫øt v√†o file JSON
        json_filepath = save_article_to_json(
            category_id=category_id,
            category_name=category_name,
            article_url=article_url,
            article_data=article_data,
            subcategory_id=subcategory_id,
            subcategory_name=keyword if subcategory_id else None
        )
        
        if not json_filepath:
            logger.error(f"Failed to save article to JSON for keyword: {keyword}")
            return None
        
        # L∆∞u th√¥ng tin v√†o backend n·∫øu b√†i vi·∫øt ch∆∞a t·ªìn t·∫°i
        import_success = False
        if not check_article_exists(article_url):
            import_success = import_article_to_backend(
                category_id, 
                article_url, 
                article_data["title"], 
                article_data["content"],
                subcategory_id
            )
            if import_success:
                logger.info(f"Successfully imported article to backend")
            else:
                logger.warning(f"Failed to import article to backend, but continuing with local save")
        else:
            logger.info(f"B·ªè qua import v√¨ b√†i vi·∫øt ƒë√£ t·ªìn t·∫°i trong database")
            import_success = True  # ƒê√°nh d·∫•u l√† th√†nh c√¥ng v√¨ b√†i vi·∫øt ƒë√£ t·ªìn t·∫°i
        
        result = {
            "category_id": category_id,
            "category_name": category_name,
            "url": article_url,
            "title": article_data.get("title", ""),
            "content_length": len(article_data.get("content", "")),
            "json_filepath": json_filepath,
            "import_success": import_success
        }
        
        if subcategory_id:
            result["subcategory_id"] = subcategory_id
            result["subcategory_name"] = keyword
        
        return result
        
    except Exception as e:
        logger.error(f"Error in search_with_category: {str(e)}")
        return None

def process_all_categories():
    """
    X·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c t·ª´ backend, ch·ªâ t√¨m ki·∫øm v√† l∆∞u tr·ªØ b√†i vi·∫øt cho c√°c danh m·ª•c con.
    
    Returns:
        dict: K·∫øt qu·∫£ x·ª≠ l√Ω c√°c danh m·ª•c
    """
    result = {
        'success': 0,
        'failed': 0,
        'categories': []
    }
    
    try:
        # L·∫•y danh s√°ch danh m·ª•c t·ª´ backend
        categories = fetch_categories_from_backend()
        
        if not categories:
            logger.error("Failed to fetch categories from backend")
            return result
        
        logger.info(f"Processing {len(categories)} categories")
        
        # X·ª≠ l√Ω t·ª´ng danh m·ª•c
        for category in categories:
            # Ki·ªÉm tra c·∫•u tr√∫c danh m·ª•c h·ª£p l·ªá
            category_id = category.get('id')
            category_name = category.get('name')
            
            if not category_id or not category_name:
                logger.warning(f"Invalid category data: {category}")
                result['failed'] += 1
                result['categories'].append({
                    'id': category_id,
                    'name': category_name,
                    'status': 'failed',
                    'error': 'Invalid category data - missing id or name field'
                })
                continue
                
            # Log th√¥ng tin chi ti·∫øt v·ªÅ danh m·ª•c
            logger.info(f"Processing category: ID: {category_id}, Name: {category_name}, Slug: {category.get('slug', 'N/A')}")
            
            # L·∫•y danh s√°ch danh m·ª•c con
            subcategories = fetch_subcategories_by_category(category_id)
            
            if subcategories and len(subcategories) > 0:
                logger.info(f"Found {len(subcategories)} subcategories for category {category_name}")
                
                # X·ª≠ l√Ω t·ª´ng danh m·ª•c con
                for subcategory in subcategories:
                    subcategory_id = subcategory.get('id')
                    subcategory_name = subcategory.get('name')
                    
                    logger.info(f"Processing subcategory: ID: {subcategory_id}, Name: {subcategory_name}")
                    
                    # Gi·ªõi h·∫°n s·ªë b√†i vi·∫øt theo c·∫•u h√¨nh
                    articles_per_subcategory = MAX_ARTICLES_PER_SUBCATEGORY
                    
                    # T√¨m ki·∫øm v√† tr√≠ch xu·∫•t n·ªôi dung b√†i vi·∫øt cho danh m·ª•c con n√†y
                    for i in range(articles_per_subcategory):
                        if result['success'] >= MAX_ARTICLES_PER_CATEGORY and MAX_ARTICLES_PER_CATEGORY > 0:
                            logger.info(f"Reached maximum articles limit ({MAX_ARTICLES_PER_CATEGORY}), stopping")
                            break
                            
                        logger.info(f"Finding article {i+1}/{articles_per_subcategory} for subcategory {subcategory_name}")
                        
                        # T√¨m ki·∫øm b√†i vi·∫øt v·ªõi subcategory
                        article_result = search_with_category(category_id, subcategory_id)
                        
                        if article_result:
                            # ƒê√°nh d·∫•u th√†nh c√¥ng v√† l∆∞u k·∫øt qu·∫£
                            result['success'] += 1
                            result['categories'].append({
                                'id': category_id,
                                'name': category_name,
                                'subcategory_id': subcategory_id,
                                'subcategory_name': subcategory_name,
                                'status': 'success',
                                'url': article_result['url'],
                                'title': article_result['title'],
                                'content_length': article_result['content_length'],
                                'json_filepath': article_result['json_filepath']
                            })
                        else:
                            logger.warning(f"Failed to find article for subcategory: {subcategory_name}")
                            break
                            
                        # Th√™m th·ªùi gian ngh·ªâ ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
                        time.sleep(2)
            else:
                logger.info(f"No subcategories found for category {category_name}. Skipping.")
                # Kh√¥ng c√≤n t√¨m ki·∫øm cho category khi kh√¥ng c√≥ subcategory
                continue
        
        logger.info(f"Processed all categories. Success: {result['success']}, Failed: {result['failed']}")
        return result
            
    except Exception as e:
        logger.error(f"Error processing categories: {str(e)}")
        return result

def save_article_to_json(category_id, category_name, article_url, article_data, subcategory_id=None, subcategory_name=None):
    """
    L∆∞u th√¥ng tin b√†i vi·∫øt v√†o file JSON.
    
    Args:
        category_id (int): ID c·ªßa danh m·ª•c
        category_name (str): T√™n danh m·ª•c
        article_url (str): URL c·ªßa b√†i vi·∫øt
        article_data (dict): D·ªØ li·ªáu b√†i vi·∫øt g·ªìm title v√† content
        subcategory_id (int, optional): ID c·ªßa danh m·ª•c con (n·∫øu c√≥)
        subcategory_name (str, optional): T√™n danh m·ª•c con (n·∫øu c√≥)
        
    Returns:
        str: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON ƒë√£ l∆∞u
    """
    try:
        # Ki·ªÉm tra v√† log chi ti·∫øt v·ªÅ d·ªØ li·ªáu ƒë·∫ßu v√†o
        logger.info(f"Saving article to JSON: category_id={category_id}, subcategory_id={subcategory_id}")
        
        # ƒê·∫£m b·∫£o c√≥ title
        title = article_data.get("title", "").strip()
        if not title:
            logger.error("Cannot save article without a title")
            return None
        
        # ƒê·∫£m b·∫£o c√≥ content
        content = article_data.get("content", "").strip()
        if not content:
            logger.error("Cannot save article without content")
            return None
            
        # ƒê·∫£m b·∫£o category_id l√† s·ªë nguy√™n
        try:
            category_id = int(category_id)
        except (ValueError, TypeError):
            logger.error(f"Invalid category_id: {category_id}")
            return None
            
        # ƒê·∫£m b·∫£o subcategory_id l√† s·ªë nguy√™n n·∫øu c√≥
        if subcategory_id:
            try:
                subcategory_id = int(subcategory_id)
            except (ValueError, TypeError):
                logger.error(f"Invalid subcategory_id: {subcategory_id}")
                subcategory_id = None
        
        # T·∫°o t√™n file t·ª´ ti√™u ƒë·ªÅ b√†i vi·∫øt
        # Lo·∫°i b·ªè k√Ω t·ª± kh√¥ng h·ª£p l·ªá cho t√™n file
        title_slug = generate_slug(title, add_uuid=True)
        
        # T·∫°o t√™n file v·ªõi format: category_id-subcategory_id-title_slug.json (n·∫øu c√≥ subcategory)
        # ho·∫∑c category_id-title_slug.json (n·∫øu kh√¥ng c√≥ subcategory)
        if subcategory_id:
            filename = os.path.join(OUTPUT_DIR, f"{category_id}-{subcategory_id}-{title_slug}.json")
        else:
            filename = os.path.join(OUTPUT_DIR, f"{category_id}-{title_slug}.json")
        
        # Ph√¢n t√≠ch URL ƒë·ªÉ l·∫•y domain
        source_name = ""
        try:
            parsed_url = urlparse(article_url)
            source_name = parsed_url.netloc
        except:
            source_name = "unknown-source"

        # X√°c ƒë·ªãnh summary t·ª´ n·ªôi dung n·∫øu kh√¥ng c√≥
        content = article_data.get("content", "")
        summary = article_data.get("summary", "")
        if not summary and content:
            sentences = re.split(r'[.!?]+', content)
            if len(sentences) >= 2:
                summary = '. '.join(s.strip() for s in sentences[:2] if s.strip()) + '.'
            else:
                summary = sentences[0].strip() if sentences else ""
                
        # ƒê·∫£m b·∫£o summary kh√¥ng qu√° d√†i
        if len(summary) > 300:
            summary = summary[:297] + "..."
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu JSON
        article_json = {
            "category_id": category_id,
            "category_name": category_name,
            "category": category_id,  # Th√™m tr∆∞·ªùng n√†y cho t∆∞∆°ng th√≠ch v·ªõi API
            "url": article_url,
            "source_url": article_url,
            "source_name": source_name,
            "source_icon": f"https://www.google.com/s2/favicons?domain={source_name}",
            "title": title,
            "slug": title_slug,
            "summary": summary,
            "content": content,
            "published_at": datetime.now().isoformat(),
            "extracted_at": datetime.now().isoformat(),
            "is_published": 1,
            "is_imported": 1
        }
        
        # Th√™m th√¥ng tin subcategory n·∫øu c√≥
        if subcategory_id:
            article_json["subcategory_id"] = subcategory_id
            article_json["subcategory_name"] = subcategory_name
        
        # Ghi file JSON
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(article_json, f, ensure_ascii=False, indent=4)
            
        logger.info(f"Saved article to file: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"Error saving article to JSON: {str(e)}")
        return None

def check_article_exists(url):
    """
    Ki·ªÉm tra m·ªôt b√†i vi·∫øt ƒë√£ t·ªìn t·∫°i trong database hay ch∆∞a d·ª±a tr√™n URL
    
    Args:
        url (str): URL c·ªßa b√†i vi·∫øt c·∫ßn ki·ªÉm tra
    
    Returns:
        bool: True n·∫øu b√†i vi·∫øt ƒë√£ t·ªìn t·∫°i, False n·∫øu ch∆∞a t·ªìn t·∫°i ho·∫∑c c√≥ l·ªói
    """
    try:
        # URL ƒë·ªÉ ki·ªÉm tra b√†i vi·∫øt
        check_url = f"{ARTICLES_CHECK_API_URL}"
        
        # S·ª≠ d·ª•ng h√†m get_api_headers ƒë·ªÉ l·∫•y headers nh·∫•t qu√°n
        headers = get_api_headers('application/json')
        
        data = {
            'url': url
        }
        
        logger.info(f"Ki·ªÉm tra b√†i vi·∫øt ƒë√£ t·ªìn t·∫°i: {url}")
        logger.info(f"G·ª≠i request ƒë·∫øn: {check_url} v·ªõi headers: {headers}")
        
        response = requests.post(check_url, headers=headers, json=data, timeout=10)
        
        # Log ph·∫£n h·ªìi
        logger.info(f"Response status code: {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            exists = result.get('exists', False)
            
            if exists:
                logger.warning(f"B√†i vi·∫øt ƒë√£ t·ªìn t·∫°i trong database: {url}")
                return True
            else:
                logger.info(f"B√†i vi·∫øt ch∆∞a t·ªìn t·∫°i trong database: {url}")
                return False
        else:
            # N·∫øu API th·∫•t b·∫°i, th·ª≠ s·ª≠ d·ª•ng domain tr·ª±c ti·∫øp n·∫øu ƒëang s·ª≠ d·ª•ng Host header
            current_config = get_config()
            if current_config.get("USE_HOST_HEADER", False):
                try:
                    direct_url = "http://magazine.test/api/articles/check"
                    logger.info(f"Th·ª≠ ki·ªÉm tra b√†i vi·∫øt qua domain tr·ª±c ti·∫øp: {direct_url}")
                    direct_response = requests.post(direct_url, headers={'Content-Type': 'application/json'}, json=data, timeout=10)
                    
                    if direct_response.status_code == 200:
                        direct_result = direct_response.json()
                        direct_exists = direct_result.get('exists', False)
                        logger.info(f"K·∫øt qu·∫£ ki·ªÉm tra qua domain: {direct_exists}")
                        return direct_exists
                except Exception as domain_err:
                    logger.error(f"Kh√¥ng th·ªÉ ki·ªÉm tra qua domain: {str(domain_err)}")
            
            # N·∫øu API th·∫•t b·∫°i, gi·∫£ ƒë·ªãnh b√†i vi·∫øt ch∆∞a t·ªìn t·∫°i ƒë·ªÉ ti·∫øp t·ª•c x·ª≠ l√Ω
            logger.warning(f"Kh√¥ng th·ªÉ ki·ªÉm tra b√†i vi·∫øt, gi·∫£ ƒë·ªãnh ch∆∞a t·ªìn t·∫°i: {url}")
            return False
            
    except Exception as e:
        logger.error(f"L·ªói khi ki·ªÉm tra b√†i vi·∫øt: {str(e)}")
        # Trong tr∆∞·ªùng h·ª£p l·ªói, gi·∫£ ƒë·ªãnh b√†i vi·∫øt ch∆∞a t·ªìn t·∫°i ƒë·ªÉ ti·∫øp t·ª•c x·ª≠ l√Ω
        return False

def generate_slug(text, max_length=50, add_uuid=False):
    """
    T·∫°o slug t·ª´ chu·ªói vƒÉn b·∫£n, ph√π h·ª£p cho t√™n file v√† URL.
    
    Args:
        text (str): Chu·ªói vƒÉn b·∫£n c·∫ßn t·∫°o slug
        max_length (int): ƒê·ªô d√†i t·ªëi ƒëa c·ªßa slug
        add_uuid (bool): Th√™m UUID ƒë·ªÉ ƒë·∫£m b·∫£o slug l√† duy nh·∫•t
        
    Returns:
        str: Slug ƒë∆∞·ª£c t·∫°o t·ª´ chu·ªói vƒÉn b·∫£n
    """
    try:
        if not text:
            logger.warning("Empty text provided for slug generation")
            text = "article"
            
        # Chuy·ªÉn sang ch·ªØ th∆∞·ªùng v√† lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát
        text = text.lower().strip()
        text = remove_vietnamese_accents(text)
        
        # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ c√°i, s·ªë, d·∫•u c√°ch
        text = re.sub(r'[^\w\s-]', '', text)
        
        # Thay th·∫ø d·∫•u c√°ch b·∫±ng d·∫•u g·∫°ch ngang
        text = re.sub(r'\s+', '-', text)
        
        # Lo·∫°i b·ªè nhi·ªÅu d·∫•u g·∫°ch ngang li√™n ti·∫øp
        text = re.sub(r'-+', '-', text)
        
        # Lo·∫°i b·ªè d·∫•u g·∫°ch ngang ·ªü ƒë·∫ßu v√† cu·ªëi
        text = text.strip('-')
        
        # Gi·ªõi h·∫°n ƒë·ªô d√†i
        if len(text) > max_length:
            text = text[:max_length].rstrip('-')
        
        # Ki·ªÉm tra n·∫øu slug qu√° ng·∫Øn
        if len(text) < 3:
            text = f"article-{text}"
        
        # Th√™m UUID n·∫øu c·∫ßn
        if add_uuid:
            import uuid
            uuid_str = str(uuid.uuid4())[:8]
            text = f"{text}-{uuid_str}"
            
        return text
    except Exception as e:
        logger.error(f"Error generating slug: {str(e)}")
        # Tr·∫£ v·ªÅ m·ªôt gi√° tr·ªã m·∫∑c ƒë·ªãnh n·∫øu c√≥ l·ªói
        import uuid
        return f"article-{str(uuid.uuid4())[:8]}"

if __name__ == '__main__':
    # Ki·ªÉm tra xem c√≥ ch·∫°y ch·∫ø ƒë·ªô x·ª≠ l√Ω t·∫•t c·∫£ danh m·ª•c hay kh√¥ng
    if len(sys.argv) > 1 and sys.argv[1] == '--all-categories':
        logger.info("Processing all categories from backend")
        results = process_all_categories()
        print(f"Processed {len(results['categories'])} categories. Success: {results['success']}, Failed: {results['failed']}")
        
        # Hi·ªÉn th·ªã th√™m th√¥ng tin v·ªÅ c√°c b√†i vi·∫øt ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng
        for category in results['categories']:
            if category['status'] == 'success':
                print(f"\nSuccessful category: {category['name']} (ID: {category['id']})")
                print(f"  URL: {category['url']}")
                print(f"  Title: {category['title']}")
                print(f"  Content length: {category['content_length']} characters")
                print(f"  Saved to: {category['json_filepath']}")
    elif len(sys.argv) > 1:
        # Ki·ªÉm tra xem tham s·ªë l√† category_id hay keyword
        param = sys.argv[1]
        
        # N·∫øu tham s·ªë l√† s·ªë, xem nh∆∞ category_id
        if param.isdigit():
            category_id = int(param)
            logger.info(f"Searching with category ID: {category_id}")
            result = search_with_category(category_id)
            
            if result:
                print(f"Successfully processed article for category ID: {category_id}")
                print(f"URL: {result['url']}")
                print(f"Title: {result['title']}")
                print(f"Content length: {result['content_length']} characters")
                print(f"Saved to: {result['json_filepath']}")
            else:
                print(f"No article found or failed to process for category ID: {category_id}")
        else:
            # Ng∆∞·ª£c l·∫°i, xem nh∆∞ keyword
            keyword = param
            logger.info(f"Searching with keyword: {keyword}")
            url = search_google_news(keyword)
            
            if url:
                print(f"Found URL: {url}")
                print("Extracting content...")
                
                # Tr√≠ch xu·∫•t n·ªôi dung
                article_data = extract_article_content(url)
                
                if article_data and article_data.get("title") and article_data.get("content"):
                    print(f"Successfully extracted content:")
                    print(f"Title: {article_data['title']}")
                    print(f"Content length: {len(article_data['content'])} characters")
                    
                    # L∆∞u v√†o file JSON
                    json_filepath = save_article_to_json(
                        category_id=0,  # 0 v√¨ kh√¥ng c√≥ category_id
                        category_name=keyword,
                        article_url=url,
                        article_data=article_data
                    )
                    
                    if json_filepath:
                        print(f"Saved to: {json_filepath}")
                else:
                    print(f"Failed to extract content from: {url}")
            else:
                print(f"No URL found for keyword: {keyword}")
    else:
        print("Usage: python google_news.py <keyword or category_id>")
        print("       python google_news.py --all-categories") 