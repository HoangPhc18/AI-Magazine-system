#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Module for searching Google News for a specific keyword using Selenium.
"""

import os
import sys
import re
import time
import random
import logging
import requests
import unicodedata
import json
from urllib.parse import urljoin, urlparse, parse_qs, quote_plus
from bs4 import BeautifulSoup
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException

# Import the content extractor from scrape_articles_selenium.py
from scrape_articles_selenium import extract_article_content

# üîπ S·ªë b√†i vi·∫øt t·ªëi ƒëa cho m·ªói danh m·ª•c
MAX_ARTICLES_PER_CATEGORY = 3

# Set environment variables for Flask
os.environ["PORT"] = "5001"
os.environ["HOST"] = "0.0.0.0"
os.environ["DEBUG"] = "False"

# API URLs
BACKEND_API_URL = "http://host.docker.internal:8000/api/articles/import"
CATEGORIES_API_URL = "http://host.docker.internal:8000/api/categories"

# Th∆∞ m·ª•c ƒë·∫ßu ra JSON
OUTPUT_DIR = "output"

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(f"keyword_search_{datetime.now().strftime('%Y%m%d')}.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

# List of User-Agents to rotate
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 Edg/113.0.1774.42",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/113.0",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 16_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Mobile/15E148 Safari/604.1"
]

def fetch_categories_from_backend():
    """
    L·∫•y danh s√°ch c√°c danh m·ª•c t·ª´ backend.
    
    Returns:
        list: Danh s√°ch c√°c danh m·ª•c ho·∫∑c None n·∫øu c√≥ l·ªói
    """
    try:
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'application/json',
        }
        
        # G·ªçi API l·∫•y danh s√°ch danh m·ª•c
        logger.info(f"Fetching categories from backend: {CATEGORIES_API_URL}")
        
        response = requests.get(CATEGORIES_API_URL, headers=headers, timeout=15)
        
        if response.status_code == 200:
            categories = response.json()
            
            # Ki·ªÉm tra c·∫•u tr√∫c d·ªØ li·ªáu tr·∫£ v·ªÅ
            if not isinstance(categories, list):
                # N·∫øu kh√¥ng ph·∫£i list, ki·ªÉm tra xem c√≥ ph·∫£i l√† object v·ªõi data key kh√¥ng
                if isinstance(categories, dict) and 'data' in categories:
                    categories = categories['data']
                else:
                    logger.error(f"Unexpected categories data structure: {categories}")
                    return None
            
            # Ki·ªÉm tra v√† log th√¥ng tin v·ªÅ c√°c danh m·ª•c
            valid_categories = []
            for category in categories:
                if not isinstance(category, dict):
                    logger.warning(f"Invalid category data type: {type(category)}")
                    continue
                    
                if 'id' not in category or 'name' not in category:
                    logger.warning(f"Category missing required fields: {category}")
                    continue
                    
                # Ghi log th√¥ng tin danh m·ª•c
                logger.info(f"Found category: ID: {category['id']}, Name: {category['name']}, " +
                           f"Slug: {category.get('slug', 'N/A')}")
                           
                valid_categories.append(category)
            
            logger.info(f"Successfully fetched {len(valid_categories)} categories from backend")
            return valid_categories
        else:
            logger.error(f"Failed to fetch categories from backend. Status code: {response.status_code}")
            return None
            
    except Exception as e:
        logger.error(f"Error fetching categories from backend: {str(e)}")
        return None

def get_category_by_id(category_id):
    """
    L·∫•y th√¥ng tin danh m·ª•c t·ª´ backend d·ª±a tr√™n ID
    
    Args:
        category_id: ID c·ªßa danh m·ª•c c·∫ßn l·∫•y
        
    Returns:
        dict: Th√¥ng tin danh m·ª•c ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    try:
        logger.info(f"Fetching category with ID {category_id} from backend")
        response = requests.get(f"{CATEGORIES_API_URL}/{category_id}")
        
        if response.status_code == 200:
            category = response.json()
            logger.info(f"Successfully fetched category: {category['name']}")
            return category
        else:
            logger.error(f"Failed to fetch category with ID {category_id}. Status code: {response.status_code}")
            logger.error(f"Response: {response.text}")
            return None
            
    except Exception as e:
        logger.error(f"Error while fetching category with ID {category_id}: {str(e)}")
        return None

def import_article_to_backend(category_id, article_url, title, content):
    """
    G·ª≠i b√†i vi·∫øt ƒë√£ t√¨m ƒë∆∞·ª£c v√†o backend.
    
    Args:
        category_id (int): ID c·ªßa danh m·ª•c
        article_url (str): URL b√†i vi·∫øt
        title (str): Ti√™u ƒë·ªÅ b√†i vi·∫øt
        content (str): N·ªôi dung b√†i vi·∫øt
        
    Returns:
        bool: True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    try:
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'application/json',
            'Content-Type': 'application/json',
        }
        
        data = {
            'category_id': category_id,
            'url': article_url,
            'title': title,
            'content': content
        }
        
        logger.info(f"Importing article to backend: {BACKEND_API_URL}")
        response = requests.post(BACKEND_API_URL, headers=headers, json=data, timeout=30)
        
        if response.status_code == 200 or response.status_code == 201:
            logger.info(f"Successfully imported article for category ID {category_id}")
            return True
        else:
            logger.error(f"Failed to import article. Status code: {response.status_code}, Response: {response.text}")
            return False
            
    except Exception as e:
        logger.error(f"Error importing article: {str(e)}")
        return False

def remove_vietnamese_accents(text):
    """
    Lo·∫°i b·ªè d·∫•u trong ti·∫øng Vi·ªát.
    
    Args:
        text (str): VƒÉn b·∫£n ti·∫øng Vi·ªát c√≥ d·∫•u
        
    Returns:
        str: VƒÉn b·∫£n kh√¥ng d·∫•u
    """
    # B·∫£ng chuy·ªÉn ƒë·ªïi ch·ªØ c√≥ d·∫•u sang kh√¥ng d·∫•u
    vietnamese_map = {
        '√†': 'a', '√°': 'a', '·∫£': 'a', '√£': 'a', '·∫°': 'a',
        'ƒÉ': 'a', '·∫±': 'a', '·∫Ø': 'a', '·∫≥': 'a', '·∫µ': 'a', '·∫∑': 'a',
        '√¢': 'a', '·∫ß': 'a', '·∫•': 'a', '·∫©': 'a', '·∫´': 'a', '·∫≠': 'a',
        'ƒë': 'd',
        '√®': 'e', '√©': 'e', '·∫ª': 'e', '·∫Ω': 'e', '·∫π': 'e',
        '√™': 'e', '·ªÅ': 'e', '·∫ø': 'e', '·ªÉ': 'e', '·ªÖ': 'e', '·ªá': 'e',
        '√¨': 'i', '√≠': 'i', '·ªâ': 'i', 'ƒ©': 'i', '·ªã': 'i',
        '√≤': 'o', '√≥': 'o', '·ªè': 'o', '√µ': 'o', '·ªç': 'o',
        '√¥': 'o', '·ªì': 'o', '·ªë': 'o', '·ªï': 'o', '·ªó': 'o', '·ªô': 'o',
        '∆°': 'o', '·ªù': 'o', '·ªõ': 'o', '·ªü': 'o', '·ª°': 'o', '·ª£': 'o',
        '√π': 'u', '√∫': 'u', '·ªß': 'u', '≈©': 'u', '·ª•': 'u',
        '∆∞': 'u', '·ª´': 'u', '·ª©': 'u', '·ª≠': 'u', '·ªØ': 'u', '·ª±': 'u',
        '·ª≥': 'y', '√Ω': 'y', '·ª∑': 'y', '·ªπ': 'y', '·ªµ': 'y',
    }
    
    # Ph∆∞∆°ng ph√°p 1: S·ª≠ d·ª•ng b·∫£ng chuy·ªÉn ƒë·ªïi
    result1 = ''.join(vietnamese_map.get(c.lower(), c) for c in text)
    
    # Ph∆∞∆°ng ph√°p 2: S·ª≠ d·ª•ng unicodedata
    result2 = unicodedata.normalize('NFKD', text)
    result2 = ''.join([c for c in result2 if not unicodedata.combining(c)])
    result2 = result2.replace('ƒë', 'd').replace('ƒê', 'D')
    
    # S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p 1 v√¨ n√≥ x·ª≠ l√Ω t·ªët h∆°n v·ªõi ch·ªØ 'ƒë'
    return result1

def setup_selenium_driver():
    """
    Kh·ªüi t·∫°o tr√¨nh ƒëi·ªÅu khi·ªÉn Selenium v·ªõi c√°c t√πy ch·ªçn ph√π h·ª£p.
    
    Returns:
        webdriver: Tr√¨nh ƒëi·ªÅu khi·ªÉn Selenium ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh
    """
    try:
        # Thi·∫øt l·∫≠p c√°c t√πy ch·ªçn cho Chrome
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # Ch·∫°y trong ch·∫ø ƒë·ªô headless (kh√¥ng hi·ªÉn th·ªã giao di·ªán)
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        
        # Th√™m User-Agent ng·∫´u nhi√™n
        user_agent = random.choice(USER_AGENTS)
        chrome_options.add_argument(f"--user-agent={user_agent}")
        
        # Tr√°nh ph√°t hi·ªán t·ª± ƒë·ªông
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # Thi·∫øt l·∫≠p ng√¥n ng·ªØ
        chrome_options.add_argument("--lang=vi-VN")
        
        # Kh·ªüi t·∫°o tr√¨nh ƒëi·ªÅu khi·ªÉn
        logger.info("Initializing Chrome WebDriver...")
        driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()),
            options=chrome_options
        )
        
        # Thi·∫øt l·∫≠p c√°c thu·ªôc t√≠nh ƒë·ªÉ tr√°nh ph√°t hi·ªán
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        return driver
    
    except Exception as e:
        logger.error(f"Error setting up Selenium driver: {str(e)}")
        raise

def search_with_selenium(keyword):
    """
    T√¨m ki·∫øm tr√™n Google News b·∫±ng Selenium v√† tr·∫£ v·ªÅ URL b√†i vi·∫øt ƒë·∫ßu ti√™n.
    
    Args:
        keyword (str): T·ª´ kh√≥a t√¨m ki·∫øm
        
    Returns:
        str: URL c·ªßa b√†i vi·∫øt ƒë·∫ßu ti√™n, ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    driver = None
    try:
        # Kh·ªüi t·∫°o tr√¨nh ƒëi·ªÅu khi·ªÉn
        driver = setup_selenium_driver()
        
        # Truy c·∫≠p Google News
        logger.info(f"Accessing Google News homepage with Selenium")
        driver.get("https://news.google.com/?hl=vi&gl=VN&ceid=VN:vi")
        
        # Ch·ªù trang t·∫£i xong
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # T√¨m ki·∫øm √¥ t√¨m ki·∫øm v√† nh·∫≠p t·ª´ kh√≥a
        logger.info(f"Looking for search box on Google News")
        search_box = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='text']"))
        )
        
        # Nh·∫≠p t·ª´ kh√≥a v·ªõi "when:1d" ƒë·ªÉ gi·ªõi h·∫°n trong 1 ng√†y
        search_query = f"{keyword} when:1d"
        logger.info(f"Entering search query: '{search_query}'")
        
        search_box.clear()
        search_box.send_keys(search_query)
        search_box.send_keys(Keys.RETURN)
        
        # Ch·ªù k·∫øt qu·∫£ t√¨m ki·∫øm xu·∫•t hi·ªán
        logger.info("Waiting for search results...")
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "article"))
        )
        
        # Th√™m th·ªùi gian ch·ªù ƒë·ªÉ trang t·∫£i ho√†n to√†n
        time.sleep(3)
        
        # L·∫•y HTML c·ªßa trang k·∫øt qu·∫£
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # T√¨m c√°c b√†i vi·∫øt trong k·∫øt qu·∫£
        articles = soup.select('article')
        
        if articles:
            logger.info(f"Found {len(articles)} articles in Google News")
            
            # T√¨m b√†i vi·∫øt ƒë·∫ßu ti√™n c√≥ li√™n k·∫øt
            for article in articles:
                # T√¨m ti√™u ƒë·ªÅ b√†i vi·∫øt
                title_element = article.select_one('h3 a, h4 a')
                
                if title_element:
                    logger.info(f"Found article with title: {title_element.text.strip()}")
                    
                    # L·∫•y li√™n k·∫øt t·ª´ b√†i vi·∫øt
                    link = article.select_one('a[href^="./articles/"]')
                    
                    if link:
                        relative_url = link['href']
                        
                        # Truy c·∫≠p v√†o li√™n k·∫øt ƒë·ªÉ l·∫•y URL th·ª±c
                        if relative_url.startswith('./'):
                            absolute_news_url = f"https://news.google.com{relative_url[1:]}"
                            logger.info(f"Navigating to article link: {absolute_news_url}")
                            
                            # M·ªü li√™n k·∫øt trong c·ª≠a s·ªï m·ªõi
                            driver.execute_script(f"window.open('{absolute_news_url}', '_blank');")
                            
                            # Chuy·ªÉn sang c·ª≠a s·ªï m·ªõi
                            driver.switch_to.window(driver.window_handles[1])
                            
                            # Ch·ªù chuy·ªÉn h∆∞·ªõng
                            time.sleep(5)
                            
                            # L·∫•y URL cu·ªëi c√πng sau khi chuy·ªÉn h∆∞·ªõng
                            final_url = driver.current_url
                            
                            # Ki·ªÉm tra URL c√≥ ph·∫£i l√† t·ª´ Google News kh√¥ng
                            if 'news.google.com' not in final_url:
                                logger.info(f"Successfully obtained article URL: {final_url}")
                                return final_url
            
            logger.warning("Could not extract actual URL from any article")
            
        else:
            logger.warning(f"No articles found for keyword: {keyword}")
        
        return None
        
    except TimeoutException:
        logger.error("Timeout waiting for page elements")
        return None
    
    except WebDriverException as e:
        logger.error(f"Selenium WebDriver error: {str(e)}")
        return None
    
    except Exception as e:
        logger.error(f"Error in Selenium search: {str(e)}")
        return None
    
    finally:
        # ƒê√≥ng tr√¨nh duy·ªát
        if driver:
            try:
                driver.quit()
                logger.info("WebDriver closed successfully")
            except Exception as e:
                logger.error(f"Error closing WebDriver: {str(e)}")

def search_with_requests(keyword):
    """
    T√¨m ki·∫øm tr√™n Google News b·∫±ng requests HTTP th√¥ng th∆∞·ªùng.
    ƒê∆∞·ª£c s·ª≠ d·ª•ng nh∆∞ m·ªôt ph∆∞∆°ng √°n d·ª± ph√≤ng n·∫øu Selenium th·∫•t b·∫°i.
    
    Args:
        keyword (str): T·ª´ kh√≥a t√¨m ki·∫øm
        
    Returns:
        str: URL c·ªßa b√†i vi·∫øt ƒë·∫ßu ti√™n, ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    try:
        # Chu·∫©n b·ªã headers v·ªõi User-Agent ng·∫´u nhi√™n
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        
        # T√¨m ki·∫øm tr√™n Google News
        search_query = f"{keyword} when:1d"
        search_url = f"https://news.google.com/search?q={quote_plus(search_query)}&hl=vi&gl=VN&ceid=VN:vi"
        
        logger.info(f"Searching with HTTP request: {search_url}")
        response = requests.get(search_url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m c√°c b√†i vi·∫øt
            articles = soup.select('article')
            
            if articles:
                logger.info(f"Found {len(articles)} articles via HTTP request")
                
                # G·ª° l·ªói: Hi·ªÉn th·ªã ti√™u ƒë·ªÅ c·ªßa c√°c b√†i vi·∫øt
                for idx, article in enumerate(articles[:5]):
                    title_elem = article.select_one('h3, h4')
                    title = title_elem.text.strip() if title_elem else "No title"
                    logger.info(f"Article {idx+1}: {title}")
                
                # Th·ª≠ tr√≠ch xu·∫•t URL tr·ª±c ti·∫øp t·ª´ Google Search
                logger.info("Trying direct Google Search as it's more reliable")
                google_search_url = f"https://www.google.com/search?q={quote_plus(keyword)}&tbm=nws"
                logger.info(f"Searching with Google Search: {google_search_url}")
                
                try:
                    search_headers = {
                        'User-Agent': random.choice(USER_AGENTS),
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
                    }
                    
                    search_response = requests.get(google_search_url, headers=search_headers, timeout=15)
                    
                    if search_response.status_code == 200:
                        search_soup = BeautifulSoup(search_response.text, 'html.parser')
                        
                        # Log HTML ƒë·ªÉ g·ª° l·ªói
                        with open('google_search_response.html', 'w', encoding='utf-8') as f:
                            f.write(search_response.text)
                        logger.info("Saved Google Search response to google_search_response.html")
                        
                        # T√¨m c√°c k·∫øt qu·∫£ t·ª´ Google Search - th·ª≠ nhi·ªÅu selector kh√°c nhau
                        for selector in ['div.g a', 'a[href^="https://"]', '.WlydOe', '.DhN8Cf a']:
                            search_results = search_soup.select(selector)
                            logger.info(f"Found {len(search_results)} results with selector '{selector}'")
                            
                            for result in search_results:
                                href = result.get('href')
                                if href and href.startswith('http') and 'google.com' not in href:
                                    logger.info(f"Found article URL from Google Search: {href}")
                                    return href
                    
                    logger.warning("No results found from Google Search")
                except Exception as e:
                    logger.error(f"Error with Google Search: {str(e)}")
                
                # Th·ª≠ truy c·∫≠p tr·ª±c ti·∫øp m·ªôt trang tin t·ª©c Vi·ªát Nam c√≥ b√†i v·ªÅ t·ª´ kh√≥a n√†y
                news_sites = [
                    f"https://vnexpress.net/tim-kiem?q={quote_plus(keyword)}",
                    f"https://tuoitre.vn/tim-kiem.htm?keywords={quote_plus(keyword)}",
                    f"https://thanhnien.vn/tim-kiem/?q={quote_plus(keyword)}",
                    f"https://dantri.com.vn/tim-kiem?q={quote_plus(keyword)}"
                ]
                
                for site_url in news_sites:
                    try:
                        logger.info(f"Trying direct news site search: {site_url}")
                        site_response = requests.get(site_url, headers=headers, timeout=15)
                        
                        if site_response.status_code == 200:
                            site_soup = BeautifulSoup(site_response.text, 'html.parser')
                            
                            # T√¨m ki·∫øm c√°c li√™n k·∫øt b√†i vi·∫øt - th·ª≠ nhi·ªÅu selector kh√°c nhau cho t·ª´ng trang
                            for article_selector in ['article a', '.title-news a', '.story', '.article-title a', '.title a']:
                                article_links = site_soup.select(article_selector)
                                
                                for link in article_links[:3]:  # Ch·ªâ xem 3 k·∫øt qu·∫£ ƒë·∫ßu ti√™n
                                    href = link.get('href')
                                    if href:
                                        # Chuy·ªÉn ƒë·ªïi URL t∆∞∆°ng ƒë·ªëi th√†nh tuy·ªát ƒë·ªëi n·∫øu c·∫ßn
                                        if not href.startswith('http'):
                                            base_url = urlparse(site_url)
                                            href = f"{base_url.scheme}://{base_url.netloc}{href if href.startswith('/') else '/' + href}"
                                        
                                        logger.info(f"Found article from direct news site: {href}")
                                        return href
                    except Exception as e:
                        logger.error(f"Error with direct news site {site_url}: {str(e)}")
            
            logger.warning("No suitable articles found via HTTP request")
        else:
            logger.error(f"HTTP request failed with status code: {response.status_code}")
        
        # N·∫øu t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p ƒë·ªÅu th·∫•t b·∫°i, th·ª≠ t√¨m ki·∫øm v·ªõi Bing
        try:
            bing_url = f"https://www.bing.com/news/search?q={quote_plus(keyword)}&qft=sortbydate%3d"
            logger.info(f"Trying Bing News as last resort: {bing_url}")
            
            bing_headers = {
                'User-Agent': random.choice(USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
            }
            
            bing_response = requests.get(bing_url, headers=bing_headers, timeout=15)
            
            if bing_response.status_code == 200:
                bing_soup = BeautifulSoup(bing_response.text, 'html.parser')
                
                # T√¨m k·∫øt qu·∫£ tin t·ª©c t·ª´ Bing
                bing_results = bing_soup.select('.news-card a')
                
                for result in bing_results:
                    href = result.get('href')
                    if href and href.startswith('http') and 'bing.com' not in href and 'msn.com' not in href:
                        logger.info(f"Found article from Bing News: {href}")
                        return href
        except Exception as e:
            logger.error(f"Error with Bing search: {str(e)}")
        
        return None
    
    except Exception as e:
        logger.error(f"Error in HTTP request method: {str(e)}")
        return None

def direct_news_search(keyword):
    """
    T√¨m ki·∫øm tr·ª±c ti·∫øp tr√™n c√°c trang tin t·ª©c Vi·ªát Nam ph·ªï bi·∫øn.
    Ph∆∞∆°ng ph√°p n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng khi Google News ho·∫∑c Google Search kh√¥ng ho·∫°t ƒë·ªông.
    
    Args:
        keyword (str): T·ª´ kh√≥a t√¨m ki·∫øm
        
    Returns:
        str: URL b√†i vi·∫øt n·∫øu t√¨m th·∫•y, None n·∫øu kh√¥ng
    """
    logger.info(f"Performing direct news search for: {keyword}")
    
    # Danh s√°ch c√°c trang tin t·ª©c Vi·ªát Nam ph·ªï bi·∫øn
    news_sites = [
        {
            'name': 'VnExpress',
            'search_url': f"https://vnexpress.net/tim-kiem?q={quote_plus(keyword)}",
            'selectors': ['.title-news a', '.item-news h3 a', '.title_news a']
        },
        {
            'name': 'Tu·ªïi Tr·∫ª',
            'search_url': f"https://tuoitre.vn/tim-kiem.htm?keywords={quote_plus(keyword)}",
            'selectors': ['.news-item', '.title-news a', 'h3.title-news a']
        },
        {
            'name': 'Thanh Ni√™n',
            'search_url': f"https://thanhnien.vn/tim-kiem/?q={quote_plus(keyword)}",
            'selectors': ['.story', '.story__title a', '.highlights__item-title a']
        },
        {
            'name': 'D√¢n Tr√≠',
            'search_url': f"https://dantri.com.vn/tim-kiem?q={quote_plus(keyword)}",
            'selectors': ['.article-title a', '.article-item a', '.news-item__title a']
        },
        {
            'name': 'Zing News',
            'search_url': f"https://zingnews.vn/tim-kiem.html?q={quote_plus(keyword)}",
            'selectors': ['.article-title a', '.article-item a', '.news-item__title a']
        },
        {
            'name': 'B√≥ng ƒê√° 24h',
            'search_url': f"https://bongda24h.vn/tim-kiem/{quote_plus(keyword)}/1.html",
            'selectors': ['.news-title a', '.title a', 'h3 a']
        },
        {
            'name': 'Ng∆∞·ªùi ƒê∆∞a Tin',
            'search_url': f"https://www.nguoiduatin.vn/tim-kiem?q={quote_plus(keyword)}",
            'selectors': ['article h3 a', '.article-title a']
        }
    ]
    
    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
    }
    
    for site in news_sites:
        try:
            logger.info(f"Searching on {site['name']}: {site['search_url']}")
            response = requests.get(site['search_url'], headers=headers, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # L∆∞u HTML ƒë·ªÉ g·ª° l·ªói n·∫øu c·∫ßn
                with open(f"{site['name'].lower()}_search.html", 'w', encoding='utf-8') as f:
                    f.write(response.text)
                
                # Th·ª≠ c√°c selector
                for selector in site['selectors']:
                    links = soup.select(selector)
                    logger.info(f"Found {len(links)} links with selector '{selector}' on {site['name']}")
                    
                    if links:
                        for link in links[:3]:  # Ch·ªâ l·∫•y 3 k·∫øt qu·∫£ ƒë·∫ßu ti√™n
                            href = link.get('href')
                            if href and ('http' in href or href.startswith('/')):
                                # Chuy·ªÉn ƒë·ªïi URL t∆∞∆°ng ƒë·ªëi th√†nh tuy·ªát ƒë·ªëi n·∫øu c·∫ßn
                                if not href.startswith('http'):
                                    base_url = urlparse(site['search_url'])
                                    href = f"{base_url.scheme}://{base_url.netloc}{href if href.startswith('/') else '/' + href}"
                                
                                logger.info(f"Found article on {site['name']}: {href}")
                                return href
        except Exception as e:
            logger.error(f"Error searching on {site['name']}: {str(e)}")
    
    logger.warning("No articles found in direct news search")
    return None

def search_google_news(keyword):
    """
    T√¨m ki·∫øm t·ª´ kh√≥a tr√™n Google News v√† tr·∫£ v·ªÅ URL b√†i vi·∫øt ƒë·∫ßu ti√™n.
    Th·ª≠ c·∫£ t·ª´ kh√≥a g·ªëc v√† t·ª´ kh√≥a kh√¥ng d·∫•u.
    
    Args:
        keyword (str): T·ª´ kh√≥a t√¨m ki·∫øm
        
    Returns:
        str: URL c·ªßa b√†i vi·∫øt ƒë·∫ßu ti√™n, ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    # Th·ª≠ c·∫£ t·ª´ kh√≥a g·ªëc v√† t·ª´ kh√≥a kh√¥ng d·∫•u
    original_keyword = keyword
    keyword_no_accent = remove_vietnamese_accents(keyword)
    
    # Log th√¥ng tin t·ª´ kh√≥a
    logger.info(f"Searching for keyword: '{original_keyword}' and non-accented version: '{keyword_no_accent}'")
    
    # Danh s√°ch t·ª´ kh√≥a ƒë·ªÉ th·ª≠
    keywords_to_try = [original_keyword]
    
    # N·∫øu t·ª´ kh√≥a kh√¥ng d·∫•u kh√°c v·ªõi t·ª´ kh√≥a g·ªëc, th√™m v√†o danh s√°ch
    if keyword_no_accent != original_keyword:
        keywords_to_try.append(keyword_no_accent)
    
    # L∆∞u l·ªói cho t·ª´ng t·ª´ kh√≥a
    errors = {}
    
    # Th·ª≠ t·ª´ng t·ª´ kh√≥a
    for current_keyword in keywords_to_try:
        logger.info(f"Trying to search with keyword: '{current_keyword}'")
        
        try:
            # V√¥ hi·ªáu h√≥a Selenium v√¨ l·ªói WebDriver tr√™n Windows
            # logger.info(f"Attempting to search with Selenium for: '{current_keyword}'")
            # url = search_with_selenium(current_keyword)
            # 
            # if url:
            #     logger.info(f"Successfully found URL with Selenium: {url}")
            #     return url
            
            # Ch·ªâ s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p requests HTTP
            logger.info(f"Using HTTP request method for: '{current_keyword}'")
            url = search_with_requests(current_keyword)
            
            if url:
                logger.info(f"Successfully found URL with HTTP request: {url}")
                return url
            
            # N·∫øu kh√¥ng t√¨m th·∫•y, ghi l·∫°i l·ªói
            error_msg = f"No articles found for keyword '{current_keyword}' using HTTP request method"
            logger.error(error_msg)
            errors[current_keyword] = error_msg
            
        except Exception as e:
            error_msg = f"Exception searching for '{current_keyword}': {str(e)}"
            logger.error(error_msg)
            errors[current_keyword] = error_msg
    
    # N·∫øu t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p th√¥ng th∆∞·ªùng th·∫•t b·∫°i, th·ª≠ t√¨m ki·∫øm tr·ª±c ti·∫øp
    logger.info("All standard methods failed, trying direct news search")
    url = direct_news_search(original_keyword)
    if url:
        logger.info(f"Found article through direct news search: {url}")
        return url
    
    # T·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p ƒë·ªÅu th·∫•t b·∫°i
    logger.error(f"All keyword searches failed. Errors: {errors}")
    return None

def search_with_category(category_id):
    """
    T√¨m ki·∫øm b√†i vi·∫øt d·ª±a tr√™n ID danh m·ª•c t·ª´ backend.
    
    Args:
        category_id (int): ID c·ªßa danh m·ª•c
        
    Returns:
        dict: Th√¥ng tin b√†i vi·∫øt ƒë√£ t√¨m th·∫•y v√† tr√≠ch xu·∫•t, ho·∫∑c None n·∫øu th·∫•t b·∫°i
    """
    try:
        # L·∫•y th√¥ng tin danh m·ª•c t·ª´ backend
        category = get_category_by_id(category_id)
        
        if not category:
            logger.error(f"Could not find category with ID: {category_id}")
            return None
        
        # S·ª≠ d·ª•ng t√™n danh m·ª•c (c·ªôt name) l√†m t·ª´ kh√≥a t√¨m ki·∫øm
        if 'name' not in category:
            logger.error(f"Category data does not contain 'name' field: {category}")
            return None
            
        category_name = category['name']
        logger.info(f"Using category name '{category_name}' as search keyword")
        
        # T√¨m ki·∫øm b√†i vi·∫øt v·ªõi t·ª´ kh√≥a l√† t√™n danh m·ª•c
        article_url = search_google_news(category_name)
        
        if not article_url:
            logger.error(f"No article URL found for category: {category_name}")
            return None
            
        logger.info(f"Found article URL: {article_url}, extracting content...")
        
        # Tr√≠ch xu·∫•t n·ªôi dung b√†i vi·∫øt
        article_data = extract_article_content(article_url)
        
        if not article_data or not article_data.get("title") or not article_data.get("content"):
            logger.error(f"Failed to extract content from URL: {article_url}")
            return None
            
        logger.info(f"Successfully extracted content from URL: {article_url}")
        logger.info(f"Title: {article_data.get('title')}")
        logger.info(f"Content length: {len(article_data.get('content', ''))}")
        
        # L∆∞u d·ªØ li·ªáu b√†i vi·∫øt v√†o file JSON
        json_filepath = save_article_to_json(
            category_id=category_id,
            category_name=category_name,
            article_url=article_url,
            article_data=article_data
        )
        
        if not json_filepath:
            logger.error(f"Failed to save article to JSON for category: {category_name}")
        
        # L∆∞u th√¥ng tin v√†o backend n·∫øu c·∫ßn
        # import_article_to_backend(category_id, article_url, article_data["title"], article_data["content"])
        
        return {
            "category_id": category_id,
            "category_name": category_name,
            "url": article_url,
            "title": article_data.get("title", ""),
            "content_length": len(article_data.get("content", "")),
            "json_filepath": json_filepath
        }
        
    except Exception as e:
        logger.error(f"Error in search_with_category: {str(e)}")
        return None

def process_all_categories():
    """
    X·ª≠ l√Ω t·∫•t c·∫£ c√°c danh m·ª•c t·ª´ backend, t√¨m ki·∫øm v√† l∆∞u tr·ªØ b√†i vi·∫øt cho m·ªói danh m·ª•c.
    
    Returns:
        dict: K·∫øt qu·∫£ x·ª≠ l√Ω c√°c danh m·ª•c
    """
    result = {
        'success': 0,
        'failed': 0,
        'categories': []
    }
    
    try:
        # L·∫•y danh s√°ch danh m·ª•c t·ª´ backend
        categories = fetch_categories_from_backend()
        
        if not categories:
            logger.error("Failed to fetch categories from backend")
            return result
        
        logger.info(f"Processing {len(categories)} categories")
        
        # X·ª≠ l√Ω t·ª´ng danh m·ª•c
        for category in categories:
            # Ki·ªÉm tra c·∫•u tr√∫c danh m·ª•c h·ª£p l·ªá (id, name, slug, description, created_at, updated_at, deleted_at)
            category_id = category.get('id')
            category_name = category.get('name')
            
            if not category_id or not category_name:
                logger.warning(f"Invalid category data: {category}")
                result['failed'] += 1
                result['categories'].append({
                    'id': category_id,
                    'name': category_name,
                    'status': 'failed',
                    'error': 'Invalid category data - missing id or name field'
                })
                continue
                
            # Log th√¥ng tin chi ti·∫øt v·ªÅ danh m·ª•c
            logger.info(f"Processing category: ID: {category_id}, Name: {category_name}, Slug: {category.get('slug', 'N/A')}")
            
            # Gi·ªõi h·∫°n s·ªë b√†i vi·∫øt theo c·∫•u h√¨nh
            if result['success'] >= MAX_ARTICLES_PER_CATEGORY and MAX_ARTICLES_PER_CATEGORY > 0:
                logger.info(f"Reached maximum number of articles per category: {MAX_ARTICLES_PER_CATEGORY}")
                break
            
            # T√¨m ki·∫øm v√† tr√≠ch xu·∫•t n·ªôi dung b√†i vi·∫øt cho danh m·ª•c n√†y
            article_result = search_with_category(category_id)
            
            if not article_result:
                logger.error(f"No article found or failed to process for category: {category_name}")
                result['failed'] += 1
                result['categories'].append({
                    'id': category_id,
                    'name': category_name,
                    'status': 'failed',
                    'error': 'No article found or processing failed'
                })
                continue
            
            # Th√™m th√¥ng tin v√†o k·∫øt qu·∫£
            logger.info(f"Successfully processed article for category {category_name}: {article_result['url']}")
            result['success'] += 1
            result['categories'].append({
                'id': category_id,
                'name': category_name,
                'slug': category.get('slug'),
                'status': 'success',
                'url': article_result['url'],
                'title': article_result['title'],
                'content_length': article_result['content_length'],
                'json_filepath': article_result['json_filepath']
            })
            
        logger.info(f"Processed all categories. Success: {result['success']}, Failed: {result['failed']}")
        return result
            
    except Exception as e:
        logger.error(f"Error processing categories: {str(e)}")
        return result

def save_article_to_json(category_id, category_name, article_url, article_data):
    """
    L∆∞u th√¥ng tin b√†i vi·∫øt v√†o file JSON.
    
    Args:
        category_id (int): ID c·ªßa danh m·ª•c
        category_name (str): T√™n danh m·ª•c
        article_url (str): URL c·ªßa b√†i vi·∫øt
        article_data (dict): D·ªØ li·ªáu b√†i vi·∫øt g·ªìm title v√† content
        
    Returns:
        str: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON ƒë√£ l∆∞u
    """
    try:
        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
        if not os.path.exists(OUTPUT_DIR):
            os.makedirs(OUTPUT_DIR)
            logger.info(f"Created output directory: {OUTPUT_DIR}")
        
        # T·∫°o t√™n file d·ª±a tr√™n th·ªùi gian v√† danh m·ª•c
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        sanitized_name = re.sub(r'[^\w\-_]', '_', category_name)
        filename = f"{OUTPUT_DIR}/{sanitized_name}_{category_id}_{timestamp}.json"
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu JSON
        article_json = {
            "category_id": category_id,
            "category_name": category_name,
            "url": article_url,
            "title": article_data.get("title", ""),
            "content": article_data.get("content", ""),
            "scraped_at": datetime.now().isoformat()
        }
        
        # Ghi file JSON
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(article_json, f, ensure_ascii=False, indent=4)
            
        logger.info(f"Saved article to JSON file: {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"Error saving article to JSON: {str(e)}")
        return None

if __name__ == '__main__':
    # Ki·ªÉm tra xem c√≥ ch·∫°y ch·∫ø ƒë·ªô x·ª≠ l√Ω t·∫•t c·∫£ danh m·ª•c hay kh√¥ng
    if len(sys.argv) > 1 and sys.argv[1] == '--all-categories':
        logger.info("Processing all categories from backend")
        results = process_all_categories()
        print(f"Processed {len(results['categories'])} categories. Success: {results['success']}, Failed: {results['failed']}")
        
        # Hi·ªÉn th·ªã th√™m th√¥ng tin v·ªÅ c√°c b√†i vi·∫øt ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng
        for category in results['categories']:
            if category['status'] == 'success':
                print(f"\nSuccessful category: {category['name']} (ID: {category['id']})")
                print(f"  URL: {category['url']}")
                print(f"  Title: {category['title']}")
                print(f"  Content length: {category['content_length']} characters")
                print(f"  Saved to: {category['json_filepath']}")
    elif len(sys.argv) > 1:
        # Ki·ªÉm tra xem tham s·ªë l√† category_id hay keyword
        param = sys.argv[1]
        
        # N·∫øu tham s·ªë l√† s·ªë, xem nh∆∞ category_id
        if param.isdigit():
            category_id = int(param)
            logger.info(f"Searching with category ID: {category_id}")
            result = search_with_category(category_id)
            
            if result:
                print(f"Successfully processed article for category ID: {category_id}")
                print(f"URL: {result['url']}")
                print(f"Title: {result['title']}")
                print(f"Content length: {result['content_length']} characters")
                print(f"Saved to: {result['json_filepath']}")
            else:
                print(f"No article found or failed to process for category ID: {category_id}")
        else:
            # Ng∆∞·ª£c l·∫°i, xem nh∆∞ keyword
            keyword = param
            logger.info(f"Searching with keyword: {keyword}")
            url = search_google_news(keyword)
            
            if url:
                print(f"Found URL: {url}")
                print("Extracting content...")
                
                # Tr√≠ch xu·∫•t n·ªôi dung
                article_data = extract_article_content(url)
                
                if article_data and article_data.get("title") and article_data.get("content"):
                    print(f"Successfully extracted content:")
                    print(f"Title: {article_data['title']}")
                    print(f"Content length: {len(article_data['content'])} characters")
                    
                    # L∆∞u v√†o file JSON
                    json_filepath = save_article_to_json(
                        category_id=0,  # 0 v√¨ kh√¥ng c√≥ category_id
                        category_name=keyword,
                        article_url=url,
                        article_data=article_data
                    )
                    
                    if json_filepath:
                        print(f"Saved to: {json_filepath}")
                else:
                    print(f"Failed to extract content from: {url}")
            else:
                print(f"No URL found for keyword: {keyword}")
    else:
        print("Usage: python google_news.py <keyword or category_id>")
        print("       python google_news.py --all-categories") 